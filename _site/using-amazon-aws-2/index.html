<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8" />
  <title>［touch spark］3. 使用Spark分析wikipedia流量数据 | Taotao's Zone</title>
  <meta name="renderer" content="webkit">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="mobile-web-app-capable" content="yes">
  <link rel="stylesheet" href="/css/font-awesome/css/font-awesome.min.css" type="text/css" />
  <!-- <link rel="stylesheet" href="/css/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="/css/default.css" type="text/css" />
  <link rel="stylesheet" href="/css/desktop.css" type="text/css" />
  <link rel="stylesheet" href="/css/mobile.css" type="text/css" />
  <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
  <link rel="alternate" type="application/atom+xml" title="Recent Entries" href="/atom.xml" />
  <script src="/js/jquery-1.11.0.min.js" type="text/javascript"></script>
  <script src="/js/jquery-migrate-1.2.1.js" type="text/javascript"></script>
  <script src="/js/jquery.transit.min.js" type="text/javascript"></script>
  <script src="/js/common.js" type="text/javascript"></script>
</head>
<body>
  <link rel="stylesheet" href="/js/prettify/prettify.css" />
<style type="text/css">
  html {
    /*background: url() no-repeat center center fixed;*/
    background: #333333;
    -webkit-background-size: cover;
    -moz-background-size: cover;
    -o-background-size: cover;
    background-size: cover;
  }
  body { background:transparent; }
  @media screen and (max-width: 750px){
    body { background: rgba(255, 255, 255, 0.9); }
  }
</style>

<div id="content" class="post" style="margin-top: 20px;">
  <div id="avatar" class="avatar circle" data-in-right="false" style="width: 150px; height: 150px; position: fixed; top: 40px; z-index: 99; opacity: 0;">
    <div class="center" style="margin-top: 4px; height: 142px; width: 142px; border-radius: 71px; background-image: url('../images/2.jpg');"></div>
  </div>

  <div class="entry" style="position: relative;">
    <h1 class="entry-title"><a href="/using-amazon-aws-2" title="［touch spark］3. 使用Spark分析wikipedia流量数据">［touch spark］3. 使用Spark分析wikipedia流量数据</a></h1>

    <p class="entry-date">2014-12-02 <span class="lastModified" style="display: none;" data-source="_posts/spark/2014-12-02-using-amazon-aws-2.md">最后更新时间: </span></p>

    <p>　　本文是接上一篇的，所以序号就延续下来了。上一篇主要记录一些EC2配置和启动的问题，有兴趣请移步<a href="../using-amazon-aws-1">Amazon AWS EC2 入门</a></p>

<h2>4. 利用spark来分析wikipedia流量数据</h2>

<p>　　启动spark shell。路径在/root/spark/spark-shell。</p>

<h3>4.1  热身</h3>

<p>　　创建一个RDD，在spark-shell中，可以用sc代替SparkContext来创建RDD。这里需要注意一点，在Scala中有两种变量类型var和val，其中var是variable的缩写，val是value的缩写。顾名思义，var是可变的，val是不可变的。简单的可以把val理解成C/C++里的常量，或者Erlang里的变量【Erlang里的变量具有单次赋值的特征】。  </p>
<div class="highlight"><pre><code class="language-text" data-lang="text">scala&gt; var a =&quot;aaa&quot;
a: java.lang.String = aaa

scala&gt; a = &quot;a&quot;
a: java.lang.String = a

scala&gt; val b = &quot;aaa&quot;
b: java.lang.String = aaa

scala&gt; b = &quot;a&quot;
&lt;console&gt;:12: error: reassignment to val
       b = &quot;a&quot;
         ^
</code></pre></div>
<p>　　这里我们需要用val来指定一个新建的RDD，原因有2：第一，我们不需要对RDD做in place的改变，所以可以采用val来指定；其次，我们不应该对RDD做in place的改变，所以必须采用val来指定。下面，我们新建一个val型pagecounts变量，读取wikipedia 20GB的流量数据，并以两种方式打印前3条数据。  </p>
<div class="highlight"><pre><code class="language-text" data-lang="text">scala&gt; val pagecounts = sc.textFile(&quot;/wiki/pagecounts&quot;)
14/12/04 05:58:35 INFO mapred.FileInputFormat: Total input paths to process : 74
pagecounts: spark.RDD[String] = spark.MappedRDD@2fddef87

scala&gt; pagecounts.take(3)
14/12/04 05:58:49 INFO spark.SparkContext: Starting job...
14/12/04 05:58:49 INFO spark.CacheTracker: Registering RDD ID 1 with cache
14/12/04 05:58:49 INFO spark.CacheTrackerActor: Registering RDD 1 with 177 partitions
14/12/04 05:58:49 INFO spark.CacheTracker: Registering RDD ID 0 with cache
14/12/04 05:58:49 INFO spark.CacheTrackerActor: Registering RDD 0 with 177 partitions
14/12/04 05:58:49 INFO spark.CacheTrackerActor: Asked for current cache locations
14/12/04 05:58:49 INFO spark.MesosScheduler: Final stage: Stage 0
14/12/04 05:58:49 INFO spark.MesosScheduler: Parents of final stage: List()
14/12/04 05:58:49 INFO spark.MesosScheduler: Missing parents: List()
14/12/04 05:58:49 INFO spark.MesosScheduler: Computing the requested partition locally
14/12/04 05:58:49 INFO spark.SparkContext: Job finished in 0.098193078 s
14/12/04 05:58:49 INFO spark.SparkContext: Starting job...
14/12/04 05:58:49 INFO spark.CacheTrackerActor: Asked for current cache locations
14/12/04 05:58:49 INFO spark.MesosScheduler: Final stage: Stage 1
14/12/04 05:58:49 INFO spark.MesosScheduler: Parents of final stage: List()
14/12/04 05:58:49 INFO spark.MesosScheduler: Missing parents: List()
14/12/04 05:58:49 INFO spark.MesosScheduler: Computing the requested partition locally
14/12/04 05:58:49 INFO spark.SparkContext: Job finished in 0.026119526 s
res1: Array[String] = Array(20090505-000000 aa.b ?71G4Bo1cAdWyg 1 14463, 20090505-000000 aa.b Special:Statistics 1 840, 20090505-000000 aa.b Special:Whatlinkshere/MediaWiki:Returnto 1 1019)

scala&gt; pagecounts.take(3).foreach(println)
14/12/04 05:59:16 INFO spark.SparkContext: Starting job...
14/12/04 05:59:16 INFO spark.CacheTrackerActor: Asked for current cache locations
14/12/04 05:59:16 INFO spark.MesosScheduler: Final stage: Stage 2
14/12/04 05:59:16 INFO spark.MesosScheduler: Parents of final stage: List()
14/12/04 05:59:16 INFO spark.MesosScheduler: Missing parents: List()
14/12/04 05:59:16 INFO spark.MesosScheduler: Computing the requested partition locally
14/12/04 05:59:16 INFO spark.SparkContext: Job finished in 0.004355182 s
14/12/04 05:59:16 INFO spark.SparkContext: Starting job...
14/12/04 05:59:16 INFO spark.CacheTrackerActor: Asked for current cache locations
14/12/04 05:59:16 INFO spark.MesosScheduler: Final stage: Stage 3
14/12/04 05:59:16 INFO spark.MesosScheduler: Parents of final stage: List()
14/12/04 05:59:16 INFO spark.MesosScheduler: Missing parents: List()
14/12/04 05:59:16 INFO spark.MesosScheduler: Computing the requested partition locally
14/12/04 05:59:16 INFO spark.SparkContext: Job finished in 0.016392708 s
20090505-000000 aa.b ?71G4Bo1cAdWyg 1 14463
20090505-000000 aa.b Special:Statistics 1 840
20090505-000000 aa.b Special:Whatlinkshere/MediaWiki:Returnto 1 1019
</code></pre></div>
<h3>4.2 初试RDD Transfomation 和 RDD Action</h3>

<p>　　下面，我们来演示一个RDD Transformation的例子。关于RDD Transformation，这篇有详细介绍和示例<a href="../spark-transformers/">spark RDD transformation 学习</a>。首先，我们先看看这20GB的文件里有多少条数据，然后查询一下看所有流量数据中，有多少条是浏览的英文wiki。<br>
　　首先，执行pagecounts.count来查看有多少条数据。这个动作会产生177个spark任务，这里是从HDFS读书数据，所以这个任务的瓶颈实在I/O这块，整个任务执行下来大概2~3分钟。这里我执行了几次，大概花了2分钟左右的时间，执行结果如下：  </p>
<div class="highlight"><pre><code class="language-text" data-lang="text">scala&gt; pagecounts.count
.
.
.
14/12/05 01:19:58 INFO spark.SimpleJob: Finished TID 173 (progress: 177/177)
14/12/05 01:19:58 INFO spark.MesosScheduler: Completed ResultTask(0, 174)
14/12/05 01:19:58 INFO spark.SparkContext: Job finished in 95.251659404 s
res0: Long = 329641466
</code></pre></div>
<p>　　在任务运行的时候，可以打开web窗口访问：http://<master_node_hostname>:8080 来实时观察执行进度。下面是我的一个截图示例：<br>
<img src="../../images/mesos-cluster.png" alt="mesos-cluster">  </p>

<p>　　现在，我们来利用现在这个RDD来trasform出另外一个RDD，用于记录英文wiki的数据。也通过把英文wiki的流量数据写到内存里，来比较一下数据在内存中和不在内存中两种情况下一些操作的耗时。这个测试需要下面4步：<br>
　　1. 通过trasformation生成一个RDD[enPages]，记录英文wiki流量数据，因为这个步骤也需要遍历一边所有数据，所以这个步骤耗时也应该和上一个 pagecounts.count 耗时相当。</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">scala&gt; val enPages = pagecounts.filter(_.split(&quot; &quot;)(1) == &quot;en&quot;)
enPages: spark.RDD[String] = spark.FilteredRDD@1b8f2e35

scala&gt; enPages.count
.
.
.
14/12/05 01:51:01 INFO spark.SparkContext: Job finished in 114.035390332 s
res1: Long = 122352588
</code></pre></div>
<p>　　2. 把enPages缓存到内存中  </p>
<div class="highlight"><pre><code class="language-text" data-lang="text">scala&gt; enPages.cache
res0: spark.RDD[String] = spark.FilteredRDD@78bf34f4
</code></pre></div>
<p>　　3. 执行enPages.count，看看执行速度有神马区别，what happened? 按照原计划，现在不应该是神速吗？仔细看看下面的执行log，是不是有一种恍然大悟的赶脚啊？  </p>
<div class="highlight"><pre><code class="language-text" data-lang="text">scala&gt; enPages.count
.
.
.
14/12/05 01:59:33 INFO spark.SimpleJob: Size of task 0:176 is 10680 bytes and took 4 ms to serialize by spark.JavaSerializerInstance
14/12/05 01:59:33 INFO spark.CacheTrackerActor: Cache entry added: (2, 176) on ip-172-31-25-137.ec2.internal (size added: 16.0B, available: 6.0GB)
14/12/05 01:59:33 INFO spark.SimpleJob: Finished TID 176 (progress: 172/177)
14/12/05 01:59:33 INFO spark.MesosScheduler: Completed ResultTask(0, 176)
14/12/05 01:59:34 INFO spark.CacheTrackerActor: Cache entry added: (2, 170) on ip-172-31-25-139.ec2.internal (size added: 10.3MB, available: 5.0GB)
14/12/05 01:59:34 INFO spark.SimpleJob: Finished TID 169 (progress: 173/177)
14/12/05 01:59:34 INFO spark.MesosScheduler: Completed ResultTask(0, 170)
14/12/05 01:59:35 INFO spark.CacheTrackerActor: Cache entry added: (2, 172) on ip-172-31-25-137.ec2.internal (size added: 183.3MB, available: 5.8GB)
14/12/05 01:59:35 INFO spark.SimpleJob: Finished TID 171 (progress: 174/177)
14/12/05 01:59:35 INFO spark.MesosScheduler: Completed ResultTask(0, 172)
14/12/05 01:59:35 INFO spark.CacheTrackerActor: Cache entry added: (2, 175) on ip-172-31-25-138.ec2.internal (size added: 16.0B, available: 6.1GB)
14/12/05 01:59:35 INFO spark.SimpleJob: Finished TID 174 (progress: 175/177)
14/12/05 01:59:35 INFO spark.MesosScheduler: Completed ResultTask(0, 175)
14/12/05 01:59:36 INFO spark.CacheTrackerActor: Cache entry added: (2, 173) on ip-172-31-25-138.ec2.internal (size added: 16.0B, available: 6.1GB)
14/12/05 01:59:36 INFO spark.SimpleJob: Finished TID 172 (progress: 176/177)
14/12/05 01:59:36 INFO spark.MesosScheduler: Completed ResultTask(0, 173)
14/12/05 01:59:36 INFO spark.CacheTrackerActor: Cache entry added: (2, 174) on ip-172-31-25-139.ec2.internal (size added: 178.3MB, available: 4.8GB)
14/12/05 01:59:36 INFO spark.SimpleJob: Finished TID 173 (progress: 177/177)
14/12/05 01:59:36 INFO spark.MesosScheduler: Completed ResultTask(0, 174)
14/12/05 01:59:36 INFO spark.SparkContext: Job finished in 130.727017576 s
res0: Long = 122352588
</code></pre></div>
<p>　　4. 好，现在我们再次执行enPages.count，看看是不是有神马神奇的事情发生了。  </p>
<div class="highlight"><pre><code class="language-text" data-lang="text">scala&gt; enPages.count
.
.
.
14/12/05 02:12:01 INFO spark.SparkContext: Job finished in 2.492567199 s
res2: Long = 122352588
</code></pre></div>
<p>　　
　　哇，130秒和2.5秒的对决，心算一下，52倍啊，如果visualize一下这个数据，估计会更让人吃惊吧。擅于YY的我不禁用echarts画了个图，感受一下内存计算的神速。画图代码如下，直接把代码粘贴到<a href="http://echarts.baidu.com/doc/example/bar1.html#macarons">echarts bar</a>，在再点击刷新就可以看到图了。 </p>
<div class="highlight"><pre><code class="language-text" data-lang="text">option = {
    title : {
        text: &#39;enPages.count&#39;,
        subtext: &#39;by taotao.li&#39;
    },
    tooltip : {
        trigger: &#39;axis&#39;
    },
    legend: {
        data:[&#39;no cache&#39;,&#39;cache&#39;]
    },
    toolbox: {
        show : true,
        feature : {
            mark : {show: true},
            dataView : {show: true, readOnly: false},
            magicType : {show: true, type: [&#39;line&#39;, &#39;bar&#39;]},
            restore : {show: true},
            saveAsImage : {show: true}
        }
    },
    calculable : true,
    xAxis : [
        {
            type : &#39;category&#39;,
            data : [&#39;one time&#39;]
        }
    ],
    yAxis : [
        {
            type : &#39;value&#39;
        }
    ],
    series : [
        {
            name:&#39;no cache&#39;,
            type:&#39;bar&#39;,
            data:[130.727017576]
        },
        {
            name:&#39;cache&#39;,
            type:&#39;bar&#39;,
            data:[2.492567199]
        }
    ]
};
</code></pre></div>
<p><img src="../../images/enPages-pic.png" alt="enPages-pic">  </p>

<h3>4.3 分析wikipedia的每日PV</h3>

<p>　　重新温习一下<a href="../using-amazon-aws-1">上一篇末尾</a>分析的wiki流量数据的格式如下：  </p>

<ul>
<li>date_time: 以YYYYMMDD-HHMMSS格式表示的访问时间，且以小时为单位；<br></li>
<li>project_code：表示对应的页面所使用的语言；<br></li>
<li>page_title：表示访问的wiki标题；<br></li>
<li>num<em>hits：表示从date</em>time起一小时内的浏览量；<br></li>
<li>page_size： 表示以字节为单位，这个页面的大小；<br></li>
</ul>

<p>　　OK，现在我们如果想要分析wiki流量的日PV，在上面5个字段中应该最关注的是date<em>time和num</em>hits吧。所以这里我们针对每一行数据创建一个key-value对，其中key是date<em>time，value是num</em>hits，在相加上相同的key对应的value就可以了。 下面我们把这些步骤拆开，一步一步分析，其中有些输出我就省略了。 </p>
<div class="highlight"><pre><code class="language-text" data-lang="text">scala&gt; val enTuples = enPages.map(line =&gt; line.split(&quot; &quot;))
enTuples: spark.RDD[Array[java.lang.String]] = spark.MappedRDD@34ba89c5

scala&gt; enTuples.take(5)
.
.
.
14/12/08 03:05:26 INFO spark.SparkContext: Job finished in 7.956625757 s
res3: Array[Array[java.lang.String]] = Array(Array(20090505-000000, en, !, 4, 170494), Array(20090505-000000, en, !!!, 21, 306957), Array(20090505-000000, en, !!!Fuck_You!!!, 9, 87025), Array(20090505-000000, en, !!!Fuck_You!!!_And_Then_Some, 2, 18249), Array(20090505-000000, en, !!!Fuck_You!!!_and_Then_Some, 2, 17960))


scala&gt; val enKeyValuePairs = enTuples.map(line =&gt; (line(0).substring(0, 8), line(3).toInt))
enKeyValuePairs: spark.RDD[(java.lang.String, Int)] = spark.MappedRDD@5e62a8d2

scala&gt; enKeyValuePairs.take(5).foreach(println)
.
.
.
14/12/08 03:07:58 INFO spark.SparkContext: Job finished in 0.001414429 s
(20090505,4)
(20090505,21)
(20090505,9)
(20090505,2)
(20090505,2)

scala&gt; val dailyPv = enKeyValuePairs.reduceByKey(_+_, 1)
dailyPv: spark.RDD[(java.lang.String, Int)] = spark.ShuffledRDD@50a934ec

scala&gt; dailyPv.take(5).foreach(println)
.
.
.
14/12/08 03:13:18 INFO spark.SparkContext: Job finished in 26.776559986 s
(20090506,204190442)
(20090507,202617618)
(20090505,207698578)

scala&gt; dailyPv.collect
.
.
.
14/12/08 03:13:45 INFO spark.SparkContext: Job finished in 0.145675681 s
res8: Array[(java.lang.String, Int)] = Array((20090506,204190442), (20090507,202617618), (20090505,207698578))
</code></pre></div>
<p>　　最后的collect方法会把RDD 转换成scala里的数组。take(n)方法是取出前n条，因为这里我们就分析3天的数据，所以最多也只能取钱3天的，这里take(5)是看看这样会不会有什么错误提示呢。<br>
　　上面我们大概用了3-4行语句来完成这个统计，这已经很强大了。而spark更强大的地方是它提供的编程模型，即transformation和action，虽然这些行为也就寥寥数十个，但已经足够处理大多数常见的问题了。比如说上面这个统计日PV的查询，在spark里其实完全可以把上面3-4行语句组合成一行语句，也就是说，在spark里，只有一行语句就可以统计当前wiki数据集下的日PV了。  </p>
<div class="highlight"><pre><code class="language-text" data-lang="text">scala&gt; enPages.map(line =&gt; line.split(&quot; &quot;)).map(line =&gt; (line(0).substring(0,8), line(3).toInt)).reduceByKey(_+_, 1).collect
.
.
.
14/12/08 03:25:03 INFO spark.SparkContext: Job finished in 27.144072883 s
res12: Array[(java.lang.String, Int)] = Array((20090506,204190442), (20090507,202617618), (20090505,207698578))
</code></pre></div>
<blockquote>
<p>　　<strong>可是老湿，你上面不是说只用一行语句就可以统计当前wiki数据集下的日PV的吗？可你这里用的是enPages啊！enPages不也是结果转换的吗，得把前几句加上吧？老湿，你骗我！！！</strong><br>
<img src="../../images/wawawa.gif" alt="wawawa">  </p>

<p>　　<strong>同学，你问这个问题是不是刚才又写情书去了？既然enPages也是由其他RDD转换而来的，那这里不也可以把enPages替换成其他的RDD与与对应的transformation吗？</strong></p>
</blockquote>

<p><img src="../../images/laoshi.gif" alt="laoshi">  </p>
<div class="highlight"><pre><code class="language-text" data-lang="text">scala&gt; pagecounts.filter(_.split(&quot; &quot;)(1) == &quot;en&quot;).map(line =&gt; line.split(&quot; &quot;)).map(line =&gt; (line(0).substring(0,8), line(3).toInt)).reduceByKey(_+_, 1).collect
 .
 .
 .
 14/12/08 04:33:52 INFO spark.SparkContext: Job finished in 151.78660518 s
res14: Array[(java.lang.String, Int)] = Array((20090506,204190442), (20090507,202617618), (20090505,207698578))
</code></pre></div>
<h3>4.4 做点有趣的事情，看看哪些网页浏览次数最多</h3>

<p>　　OK，其实分析每日PV已经是一个很有用的分析案例了。特别是长时间段的，比如说一周，一月，一季等，这些数据会让公司在容灾容错方面有很大启发。同样有用的是分析热点数据，即哪些页面是用户最常访问的，这个在缓存系统建立方面是绝对的关键啊。想一想，要是你把一个用户很少访问的页面放到缓存系统里，是不是既浪费了昂贵的缓存空间，又费力不讨好，简直是事倍功半啊。所以，现在我们就来做一件事，根据wiki这几日的流量数据，分析一下用户最常访问的wiki页面。 </p>

<blockquote>
<p>　　<strong>要不，我们先预测一下。我个人觉得，怎么说至少也应该有主页，帮助页面吧。</strong></p>
</blockquote>

<p>　　当然，首先还是需要继续温习一下数据流量的格式啊：</p>

<ul>
<li>date_time: 以YYYYMMDD-HHMMSS格式表示的访问时间，且以小时为单位；<br></li>
<li>project_code：表示对应的页面所使用的语言；<br></li>
<li>page_title：表示访问的wiki标题；<br></li>
<li>num<em>hits：表示从date</em>time起一小时内的浏览量；<br></li>
<li>page_size： 表示以字节为单位，这个页面的大小； </li>
</ul>

<p>　　既然我们要找到最常访问的热点数据，那就应该关注page<em>title和num</em>hits了。so，用分析日PV同样的思路，我们来分析一下热点数据：  </p>
<div class="highlight"><pre><code class="language-text" data-lang="text">scala&gt; enPages.take(5)
.
.
.
14/12/08 04:59:48 INFO spark.SparkContext: Job finished in 9.6166E-4 s
res15: Array[String] = Array(20090505-000000 en ! 4 170494, 20090505-000000 en !!! 21 306957, 20090505-000000 en !!!Fuck_You!!! 9 87025, 20090505-000000 en !!!Fuck_You!!!_And_Then_Some 2 18249, 20090505-000000 en !!!Fuck_You!!!_and_Then_Some 2 17960)

scala&gt; val enPageArray = enPages.map( l=&gt;l.split(&quot; &quot;))
enPageArray: spark.RDD[Array[java.lang.String]] = spark.MappedRDD@27174693

scala&gt; enPageArray.take(5)
.
.
.
14/12/08 05:02:28 INFO spark.SparkContext: Job finished in 0.001180471 s
res16: Array[Array[java.lang.String]] = Array(Array(20090505-000000, en, !, 4, 170494), Array(20090505-000000, en, !!!, 21, 306957), Array(20090505-000000, en, !!!Fuck_You!!!, 9, 87025), Array(20090505-000000, en, !!!Fuck_You!!!_And_Then_Some, 2, 18249), Array(20090505-000000, en, !!!Fuck_You!!!_and_Then_Some, 2, 17960))

scala&gt; val enPageKeyValue = enPageArray.map(l =&gt;(l(2), l(3).toInt))
enPageKeyValue: spark.RDD[(java.lang.String, Int)] = spark.MappedRDD@5b68b32

scala&gt; enPageKeyValue.take(5)
.
.
.
14/12/08 05:03:54 INFO spark.SparkContext: Job finished in 0.00113825 s
res17: Array[(java.lang.String, Int)] = Array((!,4), (!!!,21), (!!!Fuck_You!!!,9), (!!!Fuck_You!!!_And_Then_Some,2), (!!!Fuck_You!!!_and_Then_Some,2))

scala&gt; val keyValueUnion = enPageKeyValue.reduceByKey(_+_, 40)
keyValueUnion: spark.RDD[(java.lang.String, Int)] = spark.ShuffledRDD@7843f53

scala&gt; keyValueUnion.take(5).foreach(println)
.
.
.
14/12/08 05:17:35 INFO spark.SparkContext: Job finished in 98.416456363 s
(Einst%C3%83%C2%BCrzende_Neubauten,2)
(Maxemail,1)
(Michael_Carl,4)
(Boothe_Homestead,1)
(File:The_Photographer.jpg,20)

scala&gt; val valueKey = keyValueUnion.map(x=&gt;(x._2, x._1))
valueKey: spark.RDD[(Int, java.lang.String)] = spark.MappedRDD@47a82a6a

scala&gt; valueKey.take(5).foreach(println)

14/12/08 05:19:42 INFO spark.SparkContext: Job finished in 4.196323691 s
(2,Einst%C3%83%C2%BCrzende_Neubauten)
(4,Michael_Carl)
(1,Maxemail)
(1,Boothe_Homestead)
(20,File:The_Photographer.jpg)

scala&gt; valueKey.sortByKey(false).take(5).foreach(println)
.
.
.
14/12/08 05:21:59 INFO spark.SparkContext: Job finished in 41.025906283 s
(43822489,404_error/)
(18730347,Main_Page)
(17657352,Special:Search)
(5816953,Special:Random)
(3521336,Special:Randompage)
</code></pre></div>
<p>　　思路依然和分析每日PV是一样的，当然也可以组织成一行语句： </p>
<div class="highlight"><pre><code class="language-text" data-lang="text">val hotPage = enPages.map(l =&gt; l.split(&quot; &quot;)).map(l =&gt; (l(2), l(3).toInt)).reduceByKey(_+_, 40).map(x =&gt; (x._2, x._1)).sortByKey(false).take(10).foreach(println)
.
.
.
14/12/08 09:57:30 INFO spark.SparkContext: Job finished in 41.232081196 s
(43822489,404_error/)
(18730347,Main_Page)
(17657352,Special:Search)
(5816953,Special:Random)
(3521336,Special:Randompage)
(695817,Cinco_de_Mayo)
(534253,Swine_influenza)
(464935,Wiki)
(396776,Dom_DeLuise)
(382510,Deadpool_(comics))
</code></pre></div>
<p>　　好，这篇我们就先实践到这里。接下来体会一下shark的power，有兴趣的同志请移步<a href="../using-amazon-aws-3-shark">Spark Shark使用</a>。</p>

<h2>扫一扫</h2>

<p><img src="../../images/share/2014-12-02-using-amazon-aws-2.md.jpg" alt="2014-12-02-using-amazon-aws-2.md"></p>


    <div id="disqus_container">
      <div style="margin-bottom:20px">
      <!-- 多说评论框 start -->
        <div class="ds-thread" data-thread-key=/using-amazon-aws-2 data-title=［touch spark］3. 使用Spark分析wikipedia流量数据 data-url=/using-amazon-aws-2></div>
      <!-- 多说评论框 end -->
      <!-- 多说公共JS代码 start (一个网页只需插入一次) -->
      <script type="text/javascript">
      var duoshuoQuery = {short_name:"litaotao"};
        (function() {
          var ds = document.createElement('script');
          ds.type = 'text/javascript';ds.async = true;
          ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
          ds.charset = 'UTF-8';
          (document.getElementsByTagName('head')[0] 
           || document.getElementsByTagName('body')[0]).appendChild(ds);
        })();
        </script>
      <!-- 多说公共JS代码 end -->
      </div>
    </div>
  </div>

  <div id="menuIndex" class="sidenav">
    <div class="myinfo"><center>
      <div id="avatarHolder" class="avatar circle" style="width: 0px; height: 0px; box-shadow: none; margin-bottom: 20px;"></div>
      <a href="/index.html" title="Homepage"><i class="icon-home icon-large"></i> Home</a>
      <a href="http://www.linkedin.com/in/taotaoli"><i class="icon-linkedin-sign icon-large"></i><span> Profile</span></a>
      <a href="https://github.com/litaotao"><i class="icon-github icon-large"></i><span> Code</span></a>
      <a href="mailto:taotao.engineer@gmail.com"><i class="icon-envelope-alt icon-large"></i><span> Mail</span></a>
    </center></div>
    <div id="menu"></div>
  </div>
</div>

<script src="/js/post.js" type="text/javascript"></script>

</body>
</html>
