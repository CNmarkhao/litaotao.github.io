<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8" />
  <title>Taotao's Zone</title>
  <meta name="baidu-site-verification" content="6b2f48c1baf35f9e0eb29b4455265203"/>
  <meta name="baidu-site-verification" content="hgXDOPtWLn" />
  <meta name="google-site-verification" content="YqjJD80rZQfugWoznvslaHlII_viwiMiUDEEgPTLEDw" />
  <meta name="renderer" content="webkit">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="mobile-web-app-capable" content="yes">
  <link rel="stylesheet" href="/css/font-awesome/css/font-awesome.min.css" type="text/css" />
  <script src="/files/dc3da690b0d2a5655a8d6150862a2a07.html"></script>
  <!-- <link rel="stylesheet" href="/css/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="/css/default.css" type="text/css" />
  <link rel="stylesheet" href="/css/desktop.css" type="text/css" />
  <link rel="stylesheet" href="/css/mobile.css" type="text/css" />
  <link rel="shortcut icon" href="/css/favicon.ico" type="image/x-icon" />
  <link rel="icon" href="/css/favicon.ico" mce_href="/favicon.ico" type="image/x-icon">
  <link rel="alternate" type="application/atom+xml" title="Recent Entries" href="/atom.xml" />
  <script src="/js/jquery-1.11.0.min.js" type="text/javascript"></script>
  <script src="/js/jquery-migrate-1.2.1.js" type="text/javascript"></script>
  <script src="/js/jquery.transit.min.js" type="text/javascript"></script>
  <script src="/js/common.js" type="text/javascript"></script>
  <!-- growingIO code -->
  <script type='text/javascript'>
      var _vds = _vds || [];
      window._vds = _vds;
      (function(){
        _vds.push(['setAccountId', '9f3f34627219ccd1']);
        (function() {
          var vds = document.createElement('script');
          vds.type='text/javascript';
          vds.async = true;
          vds.src = ('https:' == document.location.protocol ? 'https://' : 'http://') + 'dn-growing.qbox.me/vds.js';
          var s = document.getElementsByTagName('script')[0];
          s.parentNode.insertBefore(vds, s);
        })();
      })();
  </script>
  <!-- baidu spider initiative push -->
  <script>
    (function(){
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
        }
        else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
  </script>
  <!-- google analytics push code -->
  <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-72176628-2', 'auto');
      ga('send', 'pageview');
  </script>

</head>

<!-- meiqia plug-in -->
<!-- 
<script type='text/javascript'>
    (function(m, ei, q, i, a, j, s) {
        m[a] = m[a] || function() {
            (m[a].a = m[a].a || []).push(arguments)
        };
        j = ei.createElement(q),
            s = ei.getElementsByTagName(q)[0];
        j.async = true;
        j.charset = 'UTF-8';
        j.src = i + '?v=' + new Date().getUTCDate();
        s.parentNode.insertBefore(j, s);
    })(window, document, 'script', '//static.meiqia.com/dist/meiqia.js', '_MEIQIA');
    _MEIQIA('entId', 15857);
</script>
 -->
<body>
  <?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

   <title>Taotao's Zone</title>
   <link href="http://litaotao.github.io/atom.xml" rel="self" type="application/atom+xml"/>
   <link href="http://litaotao.github.io" rel="alternate" type="text/html" />
   <updated>2016-05-13T22:10:34+08:00</updated>
   <id>http://litaotao.github.io</id>
   <author>
     <name></name>
     <email></email>
   </author>

   
   <entry>
     <title>『 Spark 』10. spark 应用程序性能优化｜12 个优化方法</title>
     <link href="/boost-spark-application-performance"/>
     <updated>2016-05-03T00:00:00+08:00</updated>
     <id>/boost-spark-application-performance</id>
     <content type="html">&lt;h2 id=&quot;section&quot;&gt;写在前面&lt;/h2&gt;

&lt;p&gt;本系列是综合了自己在学习spark过程中的理解记录 ＋ 对参考文章中的一些理解 ＋ 个人实践spark过程中的一些心得而来。写这样一个系列仅仅是为了梳理个人学习spark的笔记记录，所以一切以能够理解为主，没有必要的细节就不会记录了，而且文中有时候会出现英文原版文档，只要不影响理解，都不翻译了。若想深入了解，最好阅读参考文章和官方文档。&lt;/p&gt;

&lt;p&gt;其次，本系列是基于目前最新的 spark 1.6.0 系列开始的，spark 目前的更新速度很快，记录一下版本好还是必要的。 &lt;br /&gt;
最后，如果各位觉得内容有误，欢迎留言备注，所有留言 24 小时内必定回复，非常感谢。   &lt;br /&gt;
Tips: 如果插图看起来不明显，可以：1. 放大网页；2. 新标签中打开图片，查看原图哦。&lt;/p&gt;

&lt;h2 id=&quot;why-how-when-what&quot;&gt;1. 优化? Why? How? When? What?&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;../images/how_when_what_why.jpg&quot; alt=&quot;how_when_what_why.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;“spark 应用程序也需要优化？”，很多人可能会有这个疑问，“不是已经有代码生成器，执行优化器，pipeline 什么的了的吗？”。是的，spark 的确是有一些列强大的内置工具，让你的代码在执行时更快。但是，如果一切都依赖于工具，框架来做的话，我想那只能说明两个问题：1. 你对这个框架仅仅是知其然，而非知其所以然；2. 看来你也只是照葫芦画瓢而已，没了你，别人也可以轻轻松松的写这样一个 spark 应用程序，so you are replaceable;&lt;/p&gt;

&lt;p&gt;在做 spark 应用程序的优化的时候，从下面几个点出发就够了：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;为什么：因为你的资源有限，因为你的应用上生产环境了会有很多不稳定的因素，在上生产前做好优化和测试是唯一一个降低不稳定因素影响的办法；&lt;/li&gt;
  &lt;li&gt;怎么做：web ui ＋ log 是做优化的倚天剑和屠龙刀，能掌握好这两点就可以了；&lt;/li&gt;
  &lt;li&gt;何时做：应用开发成熟时，满足业务要求时，就可以根据需求和时间安排开始做了；&lt;/li&gt;
  &lt;li&gt;做什么：一般来说，spark 应用程序 80% 的优化，都是集中在三个地方：内存，磁盘io，网络io。再细点说，就是 driver，executor 的内存，shuffle 的设置，文件系统的配置，集群的搭建，集群和文件系统的搭建［e.g 尽量让文件系统和集群都在一个局域网内，网络更快；如果可以，可以让 driver 和 集群也在一个局域网内，因为有时候需要从 worker 返回数据到 driver］&lt;/li&gt;
  &lt;li&gt;备注：千万不要一心想着优化都从程序本身入手，虽然大多数时候都是程序自己的原因，但在入手检查程序之前最好先确认所有的 worker 机器情况都正常哦。比如说机器负载，网络情况。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下面这张图来自 databricks 的一个分享 &lt;a href=&quot;https://www.youtube.com/watch?v=kkOG_aJ9KjQ&quot;&gt;Tuning and Debugging Apache Spark&lt;/a&gt;，很有意思，说得非常对啊，哈哈。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/spark-optimization-5.png&quot; alt=&quot;spark-optimization-5.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;OK，下面我们来看看一些常见的优化方法。&lt;/p&gt;

&lt;h2 id=&quot;repartition-and-coalesce&quot;&gt;2. repartition and coalesce&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;https://www.safaribooksonline.com/library/view/learning-spark/9781449359034/ch04.html&quot;&gt;原文：&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Spark provides the `repartition()` function, which shuffles the data 
across the network to create a new set of partitions. Keep in mind 
that repartitioning your data is a fairly expensive operation. Spark 
also has an optimized version of `repartition()` called `coalesce()` 
that allows avoiding data movement, but only if you are decreasing 
the number of RDD partitions. To know whether you can safely call 
coalesce(), you can check the size of the RDD using `rdd.partitions.size()` 
in Java/Scala and `rdd.getNumPartitions()` in Python and make sure 
that you are coalescing it to fewer partitions than it currently has.
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;总结：当要对 rdd 进行重新分片时，如果目标片区数量小于当前片区数量，那么用 &lt;code class=&quot;highlighter-rouge&quot;&gt;coalesce&lt;/code&gt;，不要用 &lt;code class=&quot;highlighter-rouge&quot;&gt;repartition&lt;/code&gt;。关于 &lt;code class=&quot;highlighter-rouge&quot;&gt;partition&lt;/code&gt; 的更多优化细节，参考 &lt;a href=&quot;https://www.safaribooksonline.com/library/view/learning-spark/9781449359034/ch04.html&quot;&gt;chapter 4 of Learning Spark&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;passing-functions-to-spark&quot;&gt;3. Passing Functions to Spark&lt;/h2&gt;

&lt;p&gt;In Python, we have three options for passing functions into Spark.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;lambda expressions&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;word&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rdd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;filter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;error&quot;&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;top-level functions&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;my_personal_lib&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;word&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rdd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;filter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;my_personal_lib&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;containsError&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;locally defined functions&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;containsError&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;error&quot;&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;word&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rdd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;filter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;containsError&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;One issue to watch out for when passing functions is inadvertently serializing the object containing the function. When you pass a function that is the member of an object, or contains references to fields in an object (e.g., self.field), Spark sends the entire object to worker nodes, which can be much larger than the bit of information you need. Sometimes this can also cause your program to fail, if your class contains objects that Python can’t figure out how to pickle.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;### wrong way&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;SearchFunctions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;object&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
      &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;query&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;isMatch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;getMatchesFunctionReference&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rdd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
      &lt;span class=&quot;c&quot;&gt;# Problem: references all of &quot;self&quot; in &quot;self.isMatch&quot;&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rdd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;filter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;isMatch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;getMatchesMemberReference&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rdd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
      &lt;span class=&quot;c&quot;&gt;# Problem: references all of &quot;self&quot; in &quot;self.query&quot;&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rdd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;filter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;### the right way&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;WordFunctions&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;object&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;getMatchesNoReference&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rdd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
      &lt;span class=&quot;c&quot;&gt;# Safe: extract only the field we need into a local variable&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;query&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rdd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;filter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;query&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;worker-cpu-memroy-executors&quot;&gt;4. worker 的资源分配：cpu, memroy, executors&lt;/h2&gt;

&lt;p&gt;这个话题比较深，而且在不同的部署模式也不一样 [standalone, yarn, mesos]，这里给不了什么建议。唯一的一个宗旨是，不要一昧考虑把所有资源都独立给到 spark 来用，要考虑到机器本身的一些进程，spark 依赖的一些进程，网络情况，任务情况 [计算密集，IO密集，long-live task]等。&lt;/p&gt;

&lt;p&gt;这里只能推荐一些 video，slide 和 blog，具体情况具体分析，以后我遇到资源调优的时候再把实际案例发出来。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=WyfHUNnMutg&quot;&gt;Top 5 Mistakes When Writing Spark Applications&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;shuffle-block-size-limitation&quot;&gt;5. shuffle block size limitation&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;No Spark shuffle block can be greater than 2 GB&lt;/em&gt; — spark shuffle 里的 block size 不能大于 &lt;em&gt;2g&lt;/em&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/spark-optimization-1.png&quot; alt=&quot;spark-optimization-1.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Spark 使用一个叫 &lt;em&gt;ByteBuffer&lt;/em&gt; 的数据结构来作为 shuffle 数据的缓存，但这个 &lt;em&gt;ByteBuffer&lt;/em&gt; 默认分配的内存是 2g，所以一旦 shuffle 的数据超过 2g 的时候，shuflle 过程会出错。影响 shuffle 数据大小的因素有以下常见的几个：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;partition 的数量，partition 越多，分布到每个 partition 上的数据越少，越不容易导致 shuffle 数据过大;&lt;/li&gt;
  &lt;li&gt;数据分布不均匀，一般是 &lt;em&gt;groupByKey&lt;/em&gt; 后，存在某几个 key 包含的数据过大，导致该 key 所在的 partition 上数据过大，有可能触发后期 shuflle block 大于 2g;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;一般解决这类办法都是增加 partition 的数量，&lt;a href=&quot;https://www.youtube.com/watch?v=WyfHUNnMutg&quot;&gt;Top 5 Mistakes When Writing Spark Applications&lt;/a&gt; 这里说可以预计让每个 partition 上的数据为 128MB 左右，仅供参考，还是需要具体场景具体分析，这里只把原理讲清楚就行了，并没有一个完美的规范。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;sc.textfile 时指定一个比较大的 partition number&lt;/li&gt;
  &lt;li&gt;spark.sql.shuffle.partitions&lt;/li&gt;
  &lt;li&gt;rdd.repartition&lt;/li&gt;
  &lt;li&gt;rdd.coalesce&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;TIPS&lt;/code&gt;:&lt;/p&gt;

&lt;p&gt;在 partition 小于 2000 和大于 2000 的两种场景下，Spark 使用不同的数据结构来在 shuffle 时记录相关信息，在 partition 大于 2000 时，会有另一种更高效 [压缩] 的数据结构来存储信息。所以如果你的 partition 没到 2000，但是很接近 2000，可以放心的把 partition 设置为 2000 以上。&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-scala&quot; data-lang=&quot;scala&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;apply&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;BlockManagerId&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;uncompressedSizes&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;Array&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;Long&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;MapStatus&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;uncompressedSizes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;length&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2000&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;nc&quot;&gt;HighlyCompressedMapStatus&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;uncompressedSizes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;CompressedMapStatus&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;uncompressedSizes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;level-of-parallel--partition&quot;&gt;6. level of parallel － partition&lt;/h2&gt;

&lt;p&gt;先来看看一个 stage 里所有 task 运行的一些性能指标，其中的一些说明：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Scheduler Delay&lt;/code&gt;: spark 分配 task 所花费的时间&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Executor Computing Time&lt;/code&gt;: executor 执行 task 所花费的时间&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Getting Result Time&lt;/code&gt;: 获取 task 执行结果所花费的时间&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Result Serialization Time&lt;/code&gt;: task 执行结果序列化时间&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Task Deserialization Time&lt;/code&gt;: task 反序列化时间&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Shuffle Write Time&lt;/code&gt;: shuffle 写数据时间&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Shuffle Read Time&lt;/code&gt;: shuffle 读数据所花费时间&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../images/spark-optimization-7.png&quot; alt=&quot;spark-optimization-7.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;而这里要说的 &lt;code class=&quot;highlighter-rouge&quot;&gt;level of parallel&lt;/code&gt;，其实大多数情况下都是指 partition 的数量，partition 数量的变化会影响上面几个指标的变动。我们调优的时候，很多时候都会看上面的指标变化情况。当 partition 变化的时候，上面几个指标变动情况如下：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;partition 过小［容易引入 data skew 问题］
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Scheduler Delay&lt;/code&gt;: 无明显变化&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Executor Computing Time&lt;/code&gt;: 不稳定，有大有小，但平均下来比较大&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Getting Result Time&lt;/code&gt;: 不稳定，有大有小，但平均下来比较大&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Result Serialization Time&lt;/code&gt;: 不稳定，有大有小，但平均下来比较大&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Task Deserialization Time&lt;/code&gt;: 不稳定，有大有小，但平均下来比较大&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Shuffle Write Time&lt;/code&gt;: 不稳定，有大有小，但平均下来比较大&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Shuffle Read Time&lt;/code&gt;: 不稳定，有大有小，但平均下来比较大&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;partition 过大
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Scheduler Delay&lt;/code&gt;: 无明显变化&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Executor Computing Time&lt;/code&gt;: 比较稳定，平均下来比较小&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Getting Result Time&lt;/code&gt;: 比较稳定，平均下来比较小&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Result Serialization Time&lt;/code&gt;: 比较稳定，平均下来比较小&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Task Deserialization Time&lt;/code&gt;: 比较稳定，平均下来比较小&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Shuffle Write Time&lt;/code&gt;: 比较稳定，平均下来比较小&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Shuffle Read Time&lt;/code&gt;: 比较稳定，平均下来比较小&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;那应该怎么设置 partition 的数量呢？这里同样也没有专门的公式和规范，一般都在尝试几次后有一个比较优化的结果。但宗旨是：尽量不要导致 data skew 问题，尽量让每一个 task 执行的时间在一段变化不大的区间之内。&lt;/p&gt;

&lt;h2 id=&quot;data-skew&quot;&gt;7. data skew&lt;/h2&gt;

&lt;p&gt;大多数时候，我们希望的分布式计算带来的好处应该是像下图这样的效果：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/spark-optimization-2.png&quot; alt=&quot;spark-optimization-2.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;但是，有时候，却是下面这种效果，这就是所谓的 data skew。即数据没有被 &lt;code class=&quot;highlighter-rouge&quot;&gt;大致均匀&lt;/code&gt; 的分布到集群中，这样对一个 task 来说，整个 task 的执行时间取决于第一个数据块被处理的时间。在很多分布式系统中，data skew 都是一个很大的问题，比如说分布式缓存，假设有 10 台缓存机器，但有 50% 的数据都落到其中一台机器上，那么当这台机器 down 掉之后，整个缓存的数据就会丢掉一般，缓存命中率至少 [肯定大于] 降低 50%。这也是很多分布式缓存中要引入一致性哈希，要引入 &lt;code class=&quot;highlighter-rouge&quot;&gt;虚拟节点 vnode&lt;/code&gt; 的原因。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/spark-optimization-3.png&quot; alt=&quot;spark-optimization-3.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;一致性哈希原理图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/consistent_hashing_003.jpg&quot; alt=&quot;consistent_hashing_003.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;回到正题，在 spark 中如何解决 data skew 的问题？首先明确这个问题的发生场景和根源：一般来说，都是 (key, value) 型数据中，key 的分布不均匀，这种场景比较常见的方法是把 key 进行 salt 处理 [不知道 salt 中文应该怎么说]，比如说原来有 2 个 key (key1, key2)，并且 key1 对应的数据集很大，而 key2 对应的数据集相对较小，可以把 key 扩张成多个 key (key1-1, key1-2, …, key1-n, key2-1, key2-2, …, key2-m) ，并且保证 &lt;code class=&quot;highlighter-rouge&quot;&gt;key1-*&lt;/code&gt; 对应的数据都是原始 &lt;code class=&quot;highlighter-rouge&quot;&gt;key1&lt;/code&gt; 对应的数据集上划分而来的，&lt;code class=&quot;highlighter-rouge&quot;&gt;key2-*&lt;/code&gt; 上对应的数据都是原始的 &lt;code class=&quot;highlighter-rouge&quot;&gt;key2&lt;/code&gt; 对应的数据集上划分而来。这样之后，我们有 &lt;code class=&quot;highlighter-rouge&quot;&gt;m+n&lt;/code&gt; 个 key，而且每个 key 对应的数据集都相对较小，并行度增加，每个并行程序处理的数据集大小差别不大，可以大大提速并行处理效率。在这两个个分享里都有提到这种方法：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=WyfHUNnMutg&quot;&gt;Top 5 Mistakes When Writing Spark Applications&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=8hn2KVC8FvA&amp;amp;index=6&amp;amp;list=PL-x35fyliRwiuc6qy9z2erka2VX8LY53x&quot;&gt;Sparkling: Speculative Partition of Data for Spark Applications - Peilong Li&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;avoid-cartesian-operation&quot;&gt;8. avoid cartesian operation&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.cartesian&quot;&gt;rdd.cartesian&lt;/a&gt; 操作很耗时，特别是当数据集很大的时候，cartesian 的数量级都是平方级增长的，既耗时也耗空间。&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rdd&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parallelize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sorted&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cartesian&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rdd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;collect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;avoid-shuffle-when-possible&quot;&gt;9. avoid shuffle when possible&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;../images/spark-optimization-6.png&quot; alt=&quot;spark-optimization-6.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;spark 中的 shuffle 默认是把上一个 stage 的数据写到 disk 上，然后下一个 stage 再从 disk 上读取数据。这里的磁盘 IO 会对性能造成很大的影响，特别是数据量大的时候。&lt;/p&gt;

&lt;h2 id=&quot;use-reducebykey-instead-of-groupbykey-when-possible&quot;&gt;10. use reduceByKey instead of GroupByKey when possible&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;../images/spark-optimization-9.png&quot; alt=&quot;spark-optimization-9.png&quot; /&gt;
&lt;img src=&quot;../images/spark-optimization-10.png&quot; alt=&quot;spark-optimization-10.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;use-treereduce-instead-of-reduce-when-possible&quot;&gt;11. use treeReduce instead of reduce when possible&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;../images/spark-optimization-4.png&quot; alt=&quot;spark-optimization-4.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;use-kryo-serializer&quot;&gt;12. use Kryo serializer&lt;/h2&gt;

&lt;p&gt;spark 应用程序中，在对 RDD 进行 shuffle 和 cache 时，数据都是需要被序列化才可以存储的，此时除了 IO 外，数据序列化也可能是应用程序的瓶颈。这里推荐使用 kryo 序列库，在数据序列化时能保证较高的序列化效率。&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;sc_conf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SparkConf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sc_conf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;spark.serializer&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;org.apache.spark.serializer.KryoSerializer&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;next&quot;&gt;13. Next&lt;/h2&gt;

&lt;p&gt;这些都是一些实际实践中的经验和对一些高质量分享的总结［大多数是来自那些高质量分享］，里面可能有说得不完全正确的地方，在未来亲自实践，调试过后会再有一篇性能调试的 blog 的，本篇仅供参考哦。下一次，我们来看看怎么统一部署和配置 spark 的 cluster，那的确几乎来自个人实践经验了。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;14. 打开微信，扫一扫，点一点，棒棒的，^_^&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;../images/wechat_pay_6-6.png&quot; alt=&quot;wechat_pay_6-6.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;参考文章&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.safaribooksonline.com/library/view/learning-spark/9781449359034/ch04.html&quot;&gt;chapter 4 of Learning Spark&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.safaribooksonline.com/library/view/learning-spark/9781449359034/ch08.html&quot;&gt;chapter 8 of Learning Spark&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=WyfHUNnMutg&quot;&gt;Top 5 Mistakes When Writing Spark Applications&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.gitbook.com/book/databricks/databricks-spark-knowledge-base/details&quot;&gt;Databricks Spark Knowledge Base&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=8hn2KVC8FvA&amp;amp;index=6&amp;amp;list=PL-x35fyliRwiuc6qy9z2erka2VX8LY53x&quot;&gt;Sparkling: Speculative Partition of Data for Spark Applications - Peilong Li&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://datarus.wordpress.com/2015/05/04/fighting-the-skew-in-spark/&quot;&gt;Fighting the skew in Spark&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=kkOG_aJ9KjQ&quot;&gt;Tuning and Debugging Apache Spark&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://spark.apache.org/docs/latest/tuning.html#level-of-parallelism&quot;&gt;Tuning Spark&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/best_practices/prefer_reducebykey_over_groupbykey.html&quot;&gt;Avoid GroupByKey&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-3&quot;&gt;本系列文章链接&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/introduction-to-spark&quot;&gt;『 Spark 』1. spark 简介 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/spark-questions-concepts&quot;&gt;『 Spark 』2. spark 基本概念解析 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/spark-programming-model&quot;&gt;『 Spark 』3. spark 编程模式 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/spark-what-is-rdd&quot;&gt;『 Spark 』4. spark 之 RDD &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/spark-resouces-blogs-paper&quot;&gt;『 Spark 』5. 这些年，你不能错过的 spark 学习资源 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/deep-into-spark-exection-model&quot;&gt;『 Spark 』6. 深入研究 spark 运行原理之 job, stage, task&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/spark-dataframe-introduction&quot;&gt;『 Spark 』7. 使用 Spark DataFrame 进行大数据分析&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/spark-in-finance-and-investing&quot;&gt;『 Spark 』8. 实战案例 ｜ Spark 在金融领域的应用 ｜ 日内走势预测&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/ipython-notebook-spark&quot;&gt;『 Spark 』9. 搭建 IPython + Notebook + Spark 开发环境&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/boost-spark-application-performance&quot;&gt;『 Spark 』10. spark 应用程序性能优化｜12 个优化方法&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
   </entry>
   
   <entry>
     <title>『 读书笔记 』4月读书总结｜博文推荐</title>
     <link href="/books-recommend-and-summarize-on-apr-2016"/>
     <updated>2016-04-29T00:00:00+08:00</updated>
     <id>/books-recommend-and-summarize-on-apr-2016</id>
     <content type="html">&lt;h2 id=&quot;section&quot;&gt;写在前面&lt;/h2&gt;

&lt;p&gt;计划是每月读 5-10 本书，书籍类型大概是三个方面的：金融，技术，管理。之所以选择这三个方面，一方面是因为自己对这三个方面都很有兴趣，其次是被 linkedin 创始人 Hoffman 的 &lt;a href=&quot;http://techcrunch.com/2012/02/14/in-startups-and-life-you-need-plan-a-b-and-z/&quot;&gt;ABZ 理论&lt;/a&gt; 深度影响。建议大家都看看 abz 理论那篇文章，如果我有空，也会整理一些常用的这类理论模型到博客里的。&lt;/p&gt;

&lt;p&gt;月底读书总结的形式都很简单，只是简单的一个列表和简单的书评，对觉得比较好的书会有单独的读书笔记。另外推荐大家用 excel 来做一些简单的工作管理，我现在就用 google docs 来做工作安排和读书计划，个人感觉比一些常用的神马协同软件强大太多了，简单，够用，就行了。工作中见过太多人把时间都花到使用那些协同软件上去，不得不说避重就轻了，适得其反，哈哈。&lt;/p&gt;

&lt;p&gt;下面是一张我用 google docs 来做本月读书安排的截图，不同颜色代表不同类别的数据，清晰明了实用。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/book-reading-04.png&quot; alt=&quot;book-reading-04.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;本月看了 11 本书，其中第十本是一些研报的合集，就当是一本了；第十一本是 coursera 上的一门公开课 &lt;em&gt;Successful Negotiation: Essential Strategies and Skills&lt;/em&gt;，也当是一本书了。其中有电子书版的都放到亲爱的&lt;a href=&quot;http://pan.baidu.com/s/1boRIG5T&quot;&gt;度娘云&lt;/a&gt;里了，个人觉得不错的书都是纸板的，不知道有没有电子版的，推荐好书都看纸版的。&lt;/p&gt;

&lt;p&gt;ps: 我对好书的定义很简单：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;给自己有所启发的&lt;/li&gt;
  &lt;li&gt;高质量的，专业的教程类书籍&lt;/li&gt;
  &lt;li&gt;后期会再度回首的书&lt;/li&gt;
  &lt;li&gt;看完后会打算赠送给盆友看的书&lt;/li&gt;
  &lt;li&gt;留着给儿子看的书 [好吧，目前我只有个宝贝侄儿，哈哈]&lt;/li&gt;
  &lt;li&gt;最后一条，印刷质量要好&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;上月读书总结：&lt;a href=&quot;../books-recommend-and-summarize-on-mar-2016&quot;&gt;『 读书笔记 』3月读书总结和推荐&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;1. 读书总结&lt;/h2&gt;

&lt;h3 id=&quot;learning-sparkhttpswwwamazoncne5ada6e4b9a0spark-e58da1e58ab3dpb016ofnu9mrefsr11ieutf8qid1460795691sr8-1keywordslearningspark&quot;&gt;1.1 &lt;a href=&quot;https://www.amazon.cn/%E5%AD%A6%E4%B9%A0Spark-%E5%8D%A1%E5%8A%B3/dp/B016OFNU9M/ref=sr_1_1?ie=UTF8&amp;amp;qid=1460795691&amp;amp;sr=8-1&amp;amp;keywords=learning+spark&quot;&gt;Learning Spark&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;这是 spark 几个作者一起写的第一本系统性介绍 spark 的书籍，质量非常高，内容非常赞，强烈推荐对 spark 感兴趣的人读，即使在 spark 方面有很多经验的高手也可以看看。我个人非常喜欢这本书，我自己是在 &lt;a href=&quot;https://www.safaribooksonline.com/library/view/learning-spark/9781449359034/&quot;&gt;safaribooksonline&lt;/a&gt; 上看的，体验非常好。虽然这本书出版时间较久，2015年初出版的，里面肯定会介绍不到 spark 之后的一些特性，但是我依然强烈推荐。只要多读几遍这本书，把里面的知识点都掌握好了，对 spark 后来的特性掌握完全不是问题。&lt;/p&gt;

&lt;p&gt;总结：这是我读过的最好的技术书籍之一。&lt;/p&gt;

&lt;p&gt;推荐指数：&lt;code class=&quot;highlighter-rouge&quot;&gt;* * * * *&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&quot;httppanbaiducoms1borig5t&quot;&gt;1.2 &lt;a href=&quot;http://pan.baidu.com/s/1boRIG5T&quot;&gt;程序员跳槽全攻略&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;哈哈，这本书不知道是在哪个地方看到的。当时觉得很惊奇，难道连跳槽都还要有攻略？说实话，现在已经想不起来这本书讲了什么了。或许对我来说没什么用吧，我一直觉得找工作，换工作这类事都是一个水到渠成的问题。只要你真的准备好了，一切都有可能。就跟做项目管理一样，有的人一心想怎么提高员工的积极性，一心去找什么协同软件，项目软件来管理项目，我觉得这却是本末倒置了。时间要花在刀刃上，问题不要治标不治本。就项目管理这个事来说，我强烈推荐 西蒙·斯涅克 的这个 TED talk：&lt;a href=&quot;https://www.youtube.com/watch?v=u4ZoJKF_VuA&quot;&gt;Start with why – how great leaders inspire action&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;总结：如果你希望看攻略来获得 &lt;em&gt;自我提升&lt;/em&gt;，完全没必要看这本书；不过也可以花些闲暇时光来读这本书，不要带有任何目的性。&lt;/p&gt;

&lt;p&gt;推荐指数：&lt;code class=&quot;highlighter-rouge&quot;&gt;* *&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&quot;httpswwwamazoncne5a4a7e59e8be7bd91e7ab99e68a80e69cafe69eb6e69e84-e6a0b8e5bf83e58e9fe79086e4b88ee6a188e4be8be58886e69e90-e69d8ee699bae685a7dpb00f3z26g8refsr11ieutf8qid1461222402sr8-1keywordse5a4a7e59e8be7bd91e7ab99e68a80e69cafe69eb6e69e84&quot;&gt;1.3 &lt;a href=&quot;https://www.amazon.cn/%E5%A4%A7%E5%9E%8B%E7%BD%91%E7%AB%99%E6%8A%80%E6%9C%AF%E6%9E%B6%E6%9E%84-%E6%A0%B8%E5%BF%83%E5%8E%9F%E7%90%86%E4%B8%8E%E6%A1%88%E4%BE%8B%E5%88%86%E6%9E%90-%E6%9D%8E%E6%99%BA%E6%85%A7/dp/B00F3Z26G8/ref=sr_1_1?ie=UTF8&amp;amp;qid=1461222402&amp;amp;sr=8-1&amp;amp;keywords=%E5%A4%A7%E5%9E%8B%E7%BD%91%E7%AB%99%E6%8A%80%E6%9C%AF%E6%9E%B6%E6%9E%84&quot;&gt;大型网站技术架构:核心原理与案例分析&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;这本书写得还可以，虽然只有 200 多页，原理也讲得很浅，很多细节问题都没有深入去探究，甚至还专门花一章讲了自己的一个项目，略有点铺张浪费大意思。但是，这本书对于没有亲历过大型网站架构，但对这方面有兴趣的同学来说，还是能有所启发的，至少能有一个框架在脑子里。就跟注音版的百科全书一样，虽然不能方方面面都讲解到，但是能让小孩儿对这个世界有一个形象的认识。&lt;/p&gt;

&lt;p&gt;总结：对新手来说，可以了解大型网站的技术架构框架；对老手来说，也许可以偶尔花个下午茶时间随手翻翻。&lt;/p&gt;

&lt;p&gt;推荐指数：&lt;code class=&quot;highlighter-rouge&quot;&gt;* * * *&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&quot;apachesparkanalyticsmadesimplehttpsdatabrickscomblog20160331introducing-our-new-ebook-apache-spark-analytics-made-simplehtml&quot;&gt;1.4 &lt;a href=&quot;https://databricks.com/blog/2016/03/31/introducing-our-new-ebook-apache-spark-analytics-made-simple.html&quot;&gt;Apache_Spark_Analytics_Made_Simple&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;这是 databricks 出的一本电子书，其实是收录了 databricks 上几篇比较好的 blog。不得不再说一次，databricks 真的是我见过的最会做 marketing 的技术类公司了，summit，meetup，blog，news，linkedin 上经常都是他们的信息。这次简单把 blog 上几篇比较不错的几篇文章合成一本电子书，又宣传了一把，虽然说是换汤不换药，但是确实有效。好了，回归主题，这本电子书里的文章都能在 databricks blog 上看到，其中有几篇他们都在 summit，meetup 上提到过，内容上的确不错，但如果你看过很多他们的 summit 和 blog 的话，可以不用再重复看这本电子书了。&lt;/p&gt;

&lt;p&gt;总结：撇开新壶装旧酒不说，这本书里的内容还是很不错的，如果没看过blog，没看过 summit，可以看看。推荐指数全 5 星，因为参杂了个人主观因素，哈哈，我自己很喜欢 databricks 出的东西。&lt;/p&gt;

&lt;p&gt;推荐指数：&lt;code class=&quot;highlighter-rouge&quot;&gt;* * * * *&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&quot;spark-redis-official-documenthttppanbaiducoms1borig5t&quot;&gt;1.5 &lt;a href=&quot;http://pan.baidu.com/s/1boRIG5T&quot;&gt;spark-redis official document&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;之所以看官方这个文档，是因为一篇博文 &lt;a href=&quot;http://www.infoq.com/cn/articles/spark-processing-efficiency&quot;&gt;45倍加速Spark的处理效率？&lt;/a&gt;，当时还挺惊讶的，于是乎想了解了解 spark ＋ redis 会擦出什么样的火花。官方有两个文档，都挺简短的，在上面的链接里可以看到［感谢度娘，明明可以靠脸蛋儿］。总的来说，就是 redis 官方开发了一个 spark 的第三方包 &lt;a href=&quot;http://spark-packages.org/package/RedisLabs/spark-redis&quot;&gt;spark-redis&lt;/a&gt;，支持 rdd 从 redis 里读取数据。that’s all。但是我看完文档后并不是很感冒，因为文档里的例子就一个，而且我并没有觉得 spark 里有特别需要从 redis 里读取数据的需求，实在需要，为什么不把数据 cache 到内存呢？为何还要走 redis 这条路，路越长越容易出错呀。而且，还有一个很大的问题是，既然用到了 spark，那么数据量一般都不会小，那么如果采用 spark ＋ redis 的方案，是不是要考虑到 redis 扩容，容错的问题呢？那是不是要一个 redis cluster 呢？这个 redis cluster 是不是要和 spark cluster 在一起呢？如果在一起会不会互相有所影响呢？如果不在一起网络 IO 会不会让你情不自禁地说 fuck 呢？最重要的是，老板会问你是不是又要烧他的钱买机器了呢？&lt;/p&gt;

&lt;p&gt;总结：个人目前觉得必要性不大，但是，可以去了解。&lt;em&gt;见识&lt;/em&gt; 这东西，虽然有了不一定能了不起，但没有的话就很难了不起了，哈哈。&lt;/p&gt;

&lt;p&gt;推荐指数：&lt;code class=&quot;highlighter-rouge&quot;&gt;* * *&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&quot;httpswwwamazoncne585ace58fb8e8b4a2e58aa1e58e9fe79086-e79086e69fa5e5beb7-a-e5b883e99bb7e588a9dpb006z2yi32refsr12ieutf8qid1461250263sr8-2keywordse585ace58fb8e8b4a2e58aa1e58e9fe79086&quot;&gt;1.6 &lt;a href=&quot;https://www.amazon.cn/%E5%85%AC%E5%8F%B8%E8%B4%A2%E5%8A%A1%E5%8E%9F%E7%90%86-%E7%90%86%E6%9F%A5%E5%BE%B7-A-%E5%B8%83%E9%9B%B7%E5%88%A9/dp/B006Z2YI32/ref=sr_1_2?ie=UTF8&amp;amp;qid=1461250263&amp;amp;sr=8-2&amp;amp;keywords=%E5%85%AC%E5%8F%B8%E8%B4%A2%E5%8A%A1%E5%8E%9F%E7%90%86&quot;&gt;公司财务原理（第十版），第一，二章&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;这本书是今年以来我看到的最好的一本公司财务相关的书，推荐看第十版英文原版的，之前有看过几章第八版中文版的，但我觉得原版的看起来似乎比中文版更易懂。推荐看英文原版，遇到问题和不理解的地方可以参考中文版来促进理解。&lt;em&gt;公司财务原理&lt;/em&gt; 和上个月读的 &lt;em&gt;估值的艺术&lt;/em&gt;，是我觉得非常非常好的两本关于公司财务和发展，金融基础的书。当初想系统的看一些这类书最初是因为想在自己的量化策略里加上一些基本面的因子和指标，后来看了 &lt;em&gt;估值的艺术&lt;/em&gt; 和 &lt;em&gt;公司财务原理&lt;/em&gt; 前两章下来，我觉得这两本书对自己以后创业帮助极大，它能告诉你从哪些方面来跟踪公司的发展情况，以及制定财务计划，了解公司运作，金融市场运作流程等，极有用。这也是我特地降低这本书的看书速度的原因，因为是英文原版，而且内容很丰富，所以我打算这本书每个月看2章到3章就行了，关键是要真正理解。&lt;/p&gt;

&lt;p&gt;总结：从小来说，这本书能让你了解公司财务，金融市场运作的一些情况，对从事投资方面的人来说很有用；从大来说，这本书能交给你不少关于如何开展，运作一个公司的很多规则和经验，非常有用。严重推荐，值得认真研读。&lt;/p&gt;

&lt;p&gt;推荐指数：&lt;code class=&quot;highlighter-rouge&quot;&gt;* * * * *&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&quot;httpswwwamazoncne98791e5ad97e5a194e58e9fe79086-e9baa6e882afe994a140e5b9b4e7bb8fe585b8e59fb9e8aeade69599e69d90-e88aade88aade68b89-e6988ee68998dpb00g33nkp0refsr11ieutf8qid1461466347sr8-1keywordse98791e5ad97e5a194e58e9fe79086&quot;&gt;1.7 &lt;a href=&quot;https://www.amazon.cn/%E9%87%91%E5%AD%97%E5%A1%94%E5%8E%9F%E7%90%86-%E9%BA%A6%E8%82%AF%E9%94%A140%E5%B9%B4%E7%BB%8F%E5%85%B8%E5%9F%B9%E8%AE%AD%E6%95%99%E6%9D%90-%E8%8A%AD%E8%8A%AD%E6%8B%89-%E6%98%8E%E6%89%98/dp/B00G33NKP0/ref=sr_1_1?ie=UTF8&amp;amp;qid=1461466347&amp;amp;sr=8-1&amp;amp;keywords=%E9%87%91%E5%AD%97%E5%A1%94%E5%8E%9F%E7%90%86&quot;&gt;金字塔原理&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;这本书真正吸引我的是它的副标题：&lt;em&gt;思考，表达和解决问题的逻辑&lt;/em&gt;。通看下来，前面3章比较有用，介绍了所谓的 &lt;em&gt;金字塔原理&lt;/em&gt; 是个什么原理，但是后面的介绍过于细致了，有时候容易让读者迷失在那些细节之中。而且这本书读下来有一种干涩的感觉，有点像教科书。我不太确定是不是因为中文翻译的问题，过段时间可能会再看看原版的，到时候再回来说明一下。&lt;/p&gt;

&lt;p&gt;总结：&lt;em&gt;思考，表达和解决问题的逻辑&lt;/em&gt;，这个副标题很吸引我，但与其花很多时间通读本书，还不如细读两边前3章。也许是中文翻译的问题，文字比较干涩无力，读完原版的后再回来写点总结。anyway，还是很推荐这本书的前3章的。&lt;/p&gt;

&lt;p&gt;推荐指数：&lt;code class=&quot;highlighter-rouge&quot;&gt;* * *&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&quot;the-hitchhikers-guide-to-pythonhttpdocspython-guideorgenlatest&quot;&gt;1.8 &lt;a href=&quot;http://docs.python-guide.org/en/latest/&quot;&gt;The Hitchhiker’s Guide to Python!&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;这是 python 界一位大神，也是 &lt;em&gt;requests&lt;/em&gt; 包的作者，kennethreitz 写的一本入门教程。读这本书完全是佩服 kennethreitz，想看看这些有足够实战经验的大神写出来的书是什么样的。内容还算丰富，开发工具和相关 package 就列出了一大堆，应该都是大神了解过的。里面也解答了一些我一直以来的疑问，推荐新手和老手都看看，不会花很多时间，但肯定会有或多或少的收获，性价比挺高。&lt;/p&gt;

&lt;p&gt;总结：具有很多实战经验的大神写的书，不管是新手还是老手都值得看一下。虽然不会解决你所有的问题［当然不会，有哪本书能解决你所有的问题哦～］，但是肯定会有所收获。&lt;/p&gt;

&lt;p&gt;推荐指数：&lt;code class=&quot;highlighter-rouge&quot;&gt;* * * *&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&quot;httpswwwamazoncne5889be4b89ae7bbb4e889b0-e5a682e4bd95e5ae8ce68890e6af94e99abee69bb4e99abee79a84e4ba8b-e69cace280a2e99c8de6b49be7bbb4e88ca8dpb00smb8zvurefsr11ieutf8qid1461653303sr8-1keywordse5889be4b89ae7bbb4e889b0&quot;&gt;1.9 &lt;a href=&quot;https://www.amazon.cn/%E5%88%9B%E4%B8%9A%E7%BB%B4%E8%89%B0-%E5%A6%82%E4%BD%95%E5%AE%8C%E6%88%90%E6%AF%94%E9%9A%BE%E6%9B%B4%E9%9A%BE%E7%9A%84%E4%BA%8B-%E6%9C%AC%E2%80%A2%E9%9C%8D%E6%B4%9B%E7%BB%B4%E8%8C%A8/dp/B00SMB8ZVU/ref=sr_1_1?ie=UTF8&amp;amp;qid=1461653303&amp;amp;sr=8-1&amp;amp;keywords=%E5%88%9B%E4%B8%9A%E7%BB%B4%E8%89%B0&quot;&gt;创业维艰&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;这本书真正吸引我的，是它的英文原标题：&lt;em&gt;the hard thing about hard things&lt;/em&gt;。很有趣，这个英文标题一下子就吸引我了，而且公司大 boss 也推荐过，遂前两天从公司图书馆借来看了下，准备回头自己也买一本。［我有个习惯，想看的书，一般都会先在公司图书馆借来看看，有感觉再买］。事实证明，这本书内容很丰富，值得反复阅读，其中的很多内容我都很欣赏。比如说这几节：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;知道我今天为什么来上班吗？好公司与烂公司的区别&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;创业公司为什么要进行人员培训&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;

    &lt;p&gt;其中这节讲到了 &lt;code class=&quot;highlighter-rouge&quot;&gt;好的产品经理，差的产品经理&lt;/code&gt; 的故事，非常有鉴赏学习价值。&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;该不该招资深人士&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;打造企业文化&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;总结：&lt;em&gt;从0到1&lt;/em&gt; 和 &lt;em&gt;创业维艰&lt;/em&gt; 这两本书最近很火，可是最近看下来，我对 &lt;em&gt;创业维艰&lt;/em&gt; 这本书更喜欢，这是一本我觉得好的，并且会花钱买纸板的，并且会推荐给朋友看的书，特别是对于想要创业，正在创业中的同事，我强烈推荐。&lt;/p&gt;

&lt;p&gt;推荐指数：&lt;code class=&quot;highlighter-rouge&quot;&gt;* * * * *&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&quot;successful-negotiation-essential-strategies-and-skillshttpswwwcourseraorglearnnegotiation-skills&quot;&gt;1.10 &lt;a href=&quot;https://www.coursera.org/learn/negotiation-skills/&quot;&gt;Successful Negotiation: Essential Strategies and Skills&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;这是 coursera 上的一门公开课，讲商业谈判的。老师说话很清晰，课件也做得非常好，这门课我有单独做笔记的，稍后会放出来。就课程内容而言，我很喜欢，梳理了很多商业谈判的思考和执行流程，对销售人员我觉得用处很大。&lt;/p&gt;

&lt;p&gt;总结：这位老师讲得非常详细，例子也简单明了，课件清晰易懂，内容很丰富，值得学习。&lt;/p&gt;

&lt;p&gt;推荐指数：&lt;code class=&quot;highlighter-rouge&quot;&gt;* * * * *&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&quot;httppanbaiducoms1borig5t-1&quot;&gt;1.11.1 &lt;a href=&quot;http://pan.baidu.com/s/1boRIG5T&quot;&gt;研报系列: 大数据深度学习系列&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;推荐指数：&lt;code class=&quot;highlighter-rouge&quot;&gt;* * * *&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&quot;smartalphahttppanbaiducoms1borig5t&quot;&gt;1.11.2 &lt;a href=&quot;http://pan.baidu.com/s/1boRIG5T&quot;&gt;研报系列: 兴业金工-SmartAlpha系列&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;推荐指数：&lt;code class=&quot;highlighter-rouge&quot;&gt;* * * *&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&quot;httppanbaiducoms1borig5t-2&quot;&gt;1.11.3 &lt;a href=&quot;http://pan.baidu.com/s/1boRIG5T&quot;&gt;研报系列：聪明贝塔创造超额收益的秘密&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;推荐指数：&lt;code class=&quot;highlighter-rouge&quot;&gt;* * * *&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&quot;phbs---httpsmpweixinqqcomsbizmzaxmtgxotk1mgmid2652253014idx1snb46105f151dada9072b7e532f1039ce6scene1srcid042918y41xwxir3vkcze4vaikeyb28b03434249256bca154929931849b7030dd828c272e65879cb4066d506bd78a0cd49cee81f5897c05b4ee8029eebceascene0uinmtazntc2nzm4mg3d3ddevicetypeimacmacbookair62c2osxosx10105build14f1605version11020201passticketzro6lfuwcyd7lyl1iizja8mspeixaymakvhlukmp28ql9zuuwg5v463an4rqy0hs&quot;&gt;1.11.4 &lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MzAxMTgxOTk1Mg==&amp;amp;mid=2652253014&amp;amp;idx=1&amp;amp;sn=b46105f151dada9072b7e532f1039ce6&amp;amp;scene=1&amp;amp;srcid=042918Y41xWXir3VkCzE4Vai&amp;amp;key=b28b03434249256bca154929931849b7030dd828c272e65879cb4066d506bd78a0cd49cee81f5897c05b4ee8029eebce&amp;amp;ascene=0&amp;amp;uin=MTAzNTc2NzM4Mg%3D%3D&amp;amp;devicetype=iMac+MacBookAir6%2C2+OSX+OSX+10.10.5+build(14F1605)&amp;amp;version=11020201&amp;amp;pass_ticket=ZRo6lfuwcyd7LYl1iiZja8mspEIxaYmAkVHLUKMP28QL9ZUUWG5v463an4rQY0HS&quot;&gt;研报系列: PHBS 之 【内培分享】股票之量化择时 &lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;推荐指数：&lt;code class=&quot;highlighter-rouge&quot;&gt;* * * *&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;2. 博文推荐&lt;/h2&gt;

&lt;h3 id=&quot;httpsmpweixinqqcomsbizmjm5mtqznzu2namid2651640686idx1snc2053e953b94f2b6133230c2d0b48d83scene0keyb28b03434249256b1d6facc8de2ef11e0e4dcbdf601d1b76f0eb49a592cba264c4613048cc1946d7e2048b13dd15aa02ascene0uinmtazntc2nzm4mg3d3ddevicetypeimacmacbookair62c2osxosx10105build14f1605version11020201passticketmboekjt4scekrgkop52x3dw7dtf706nn06oet3552iiocf4nfr7p2fec0dm3jc7z3&quot;&gt;2.1 &lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MjM5MTQzNzU2NA==&amp;amp;mid=2651640686&amp;amp;idx=1&amp;amp;sn=c2053e953b94f2b6133230c2d0b48d83&amp;amp;scene=0&amp;amp;key=b28b03434249256b1d6facc8de2ef11e0e4dcbdf601d1b76f0eb49a592cba264c4613048cc1946d7e2048b13dd15aa02&amp;amp;ascene=0&amp;amp;uin=MTAzNTc2NzM4Mg%3D%3D&amp;amp;devicetype=iMac+MacBookAir6%2C2+OSX+OSX+10.10.5+build(14F1605)&amp;amp;version=11020201&amp;amp;pass_ticket=mBoeKJT4SCEkrgKop52x3Dw7dtf706Nn06oeT3552IIocF4nfR7p%2FEc0dm3Jc7Z3&quot;&gt;大数据背后的神秘公式（上）：贝叶斯公式&lt;/a&gt;&lt;/h3&gt;

&lt;h3 id=&quot;httpsmpweixinqqcomsbizmjm5mtqznzu2namid2651640692idx1sneffef2d07f3afc0e6506d45633e3f771scene0keyb28b03434249256b8a2e9811f53c2873930c9c4ab8fc855d766a6bdac7001085057ba88e71803c6bd7f0a00614c44fc8ascene0uinmtazntc2nzm4mg3d3ddevicetypeimacmacbookair62c2osxosx10105build14f1605version11020201passticketmboekjt4scekrgkop52x3dw7dtf706nn06oet3552iiocf4nfr7p2fec0dm3jc7z3&quot;&gt;2.2 &lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MjM5MTQzNzU2NA==&amp;amp;mid=2651640692&amp;amp;idx=1&amp;amp;sn=effef2d07f3afc0e6506d45633e3f771&amp;amp;scene=0&amp;amp;key=b28b03434249256b8a2e9811f53c2873930c9c4ab8fc855d766a6bdac7001085057ba88e71803c6bd7f0a00614c44fc8&amp;amp;ascene=0&amp;amp;uin=MTAzNTc2NzM4Mg%3D%3D&amp;amp;devicetype=iMac+MacBookAir6%2C2+OSX+OSX+10.10.5+build(14F1605)&amp;amp;version=11020201&amp;amp;pass_ticket=mBoeKJT4SCEkrgKop52x3Dw7dtf706Nn06oeT3552IIocF4nfR7p%2FEc0dm3Jc7Z3&quot;&gt;大数据背后的神秘公式（下）：贝叶斯革命&lt;/a&gt;&lt;/h3&gt;

&lt;h3 id=&quot;apache-kafka-httpwwwibmcomdeveloperworkscnopensourceos-cn-kafkaindexhtml&quot;&gt;2.3 &lt;a href=&quot;http://www.ibm.com/developerworks/cn/opensource/os-cn-kafka/index.html&quot;&gt;Apache kafka 工作原理介绍&lt;/a&gt;&lt;/h3&gt;

&lt;h3 id=&quot;httpsmpweixinqqcomsbizmjm5mtqznzu2namid403145071idx1sn7ab3a8fd92b622d3cd7362db5b20b82ascene0keyb28b03434249256bf46f5ae8286a91d594b55d0f3f709de24e82865fa245df906c24fdbd0dc7bb12df7f9e8ed45c7a37ascene0uinmtazntc2nzm4mg3d3ddevicetypeimacmacbookair62c2osxosx10105build14f1605version11020201passticketmboekjt4scekrgkop52x3dw7dtf706nn06oet3552iiocf4nfr7p2fec0dm3jc7z3&quot;&gt;2.4 &lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MjM5MTQzNzU2NA==&amp;amp;mid=403145071&amp;amp;idx=1&amp;amp;sn=7ab3a8fd92b622d3cd7362db5b20b82a&amp;amp;scene=0&amp;amp;key=b28b03434249256bf46f5ae8286a91d594b55d0f3f709de24e82865fa245df906c24fdbd0dc7bb12df7f9e8ed45c7a37&amp;amp;ascene=0&amp;amp;uin=MTAzNTc2NzM4Mg%3D%3D&amp;amp;devicetype=iMac+MacBookAir6%2C2+OSX+OSX+10.10.5+build(14F1605)&amp;amp;version=11020201&amp;amp;pass_ticket=mBoeKJT4SCEkrgKop52x3Dw7dtf706Nn06oeT3552IIocF4nfR7p%2FEc0dm3Jc7Z3&quot;&gt;优秀的数据产品经理如何炼成&lt;/a&gt;&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;

    &lt;p&gt;这些都是讲方法论的，个人觉得想要在自己所在的行业做到优秀，最核心，最关键的因素是自己打心底爱这个职业。人生两大境界：爱我所做，做我所爱。&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;httpsmpweixinqqcomsbizmjm5mdmymzg2mamid407893431idx1snbceacc10da607cb6b0e7c5d14c963b90scene0keyb28b03434249256b37259de5b86de1aed3f2f01e22bb962ca1590d9c6f9966bfcbf006f7be14d7d93377444fe2388ecfascene0uinmtazntc2nzm4mg3d3ddevicetypeimacmacbookair62c2osxosx10105build14f1605version11020201passticketmboekjt4scekrgkop52x3dw7dtf706nn06oet3552iiocf4nfr7p2fec0dm3jc7z3&quot;&gt;2.5 &lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MjM5MDMyMzg2MA==&amp;amp;mid=407893431&amp;amp;idx=1&amp;amp;sn=bceacc10da607cb6b0e7c5d14c963b90&amp;amp;scene=0&amp;amp;key=b28b03434249256b37259de5b86de1aed3f2f01e22bb962ca1590d9c6f9966bfcbf006f7be14d7d93377444fe2388ecf&amp;amp;ascene=0&amp;amp;uin=MTAzNTc2NzM4Mg%3D%3D&amp;amp;devicetype=iMac+MacBookAir6%2C2+OSX+OSX+10.10.5+build(14F1605)&amp;amp;version=11020201&amp;amp;pass_ticket=mBoeKJT4SCEkrgKop52x3Dw7dtf706Nn06oeT3552IIocF4nfR7p%2FEc0dm3Jc7Z3&quot;&gt;情商高就是心里装着别人&lt;/a&gt;&lt;/h3&gt;

&lt;h3 id=&quot;httpsmpweixinqqcomsbizmjm5mzu1ntqznqmid402506711idx1sn9b6cb08013e6eeeaa8f9e2685b927239scene0keyb28b03434249256b0b41bf07161162782850f60db9a4a83fea5016eda2be43295e33240989a80ed10c56deb90cdaabcaascene0uinmtazntc2nzm4mg3d3ddevicetypeimacmacbookair62c2osxosx10105build14f1605version11020201passticketmboekjt4scekrgkop52x3dw7dtf706nn06oet3552iiocf4nfr7p2fec0dm3jc7z3&quot;&gt;2.6 &lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MjM5MzU1NTQzNQ==&amp;amp;mid=402506711&amp;amp;idx=1&amp;amp;sn=9b6cb08013e6eeeaa8f9e2685b927239&amp;amp;scene=0&amp;amp;key=b28b03434249256b0b41bf07161162782850f60db9a4a83fea5016eda2be43295e33240989a80ed10c56deb90cdaabca&amp;amp;ascene=0&amp;amp;uin=MTAzNTc2NzM4Mg%3D%3D&amp;amp;devicetype=iMac+MacBookAir6%2C2+OSX+OSX+10.10.5+build(14F1605)&amp;amp;version=11020201&amp;amp;pass_ticket=mBoeKJT4SCEkrgKop52x3Dw7dtf706Nn06oeT3552IIocF4nfR7p%2FEc0dm3Jc7Z3&quot;&gt;凯利公式——仓位控制的利器&lt;/a&gt;&lt;/h3&gt;

&lt;h3 id=&quot;httpsmpweixinqqcomsbizmjm5nzewntmwmamid403311481idx1sn7b597caceb57c69b3c53c052e0050cf2scene0keyb28b03434249256be55dc6bc20a8bc9b34fe45739f7a6d6ec9fc84dde0eafabb7fa00e636f05592c74d23c3bac40654cascene0uinmtazntc2nzm4mg3d3ddevicetypeimacmacbookair62c2osxosx10105build14f1605version11020201passticketmboekjt4scekrgkop52x3dw7dtf706nn06oet3552iiocf4nfr7p2fec0dm3jc7z3&quot;&gt;2.7 &lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MjM5NzEwNTMwMA==&amp;amp;mid=403311481&amp;amp;idx=1&amp;amp;sn=7b597caceb57c69b3c53c052e0050cf2&amp;amp;scene=0&amp;amp;key=b28b03434249256be55dc6bc20a8bc9b34fe45739f7a6d6ec9fc84dde0eafabb7fa00e636f05592c74d23c3bac40654c&amp;amp;ascene=0&amp;amp;uin=MTAzNTc2NzM4Mg%3D%3D&amp;amp;devicetype=iMac+MacBookAir6%2C2+OSX+OSX+10.10.5+build(14F1605)&amp;amp;version=11020201&amp;amp;pass_ticket=mBoeKJT4SCEkrgKop52x3Dw7dtf706Nn06oeT3552IIocF4nfR7p%2FEc0dm3Jc7Z3&quot;&gt;「滚床单」有哪些优雅的叫法？&lt;/a&gt;&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;

    &lt;p&gt;轻松一下，哈哈。&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;httpsmpweixinqqcomsbizmjm5mde0mjc4mamid402796490idx1sn5f9fd2dbd9d0030c954084f2df75d410scene0keyb28b03434249256bcc21f98e1dce38db43a18cba063d4f11b77c9091c999e698be4dfddc847ac0ec70d1785d3f3d0473ascene0uinmtazntc2nzm4mg3d3ddevicetypeimacmacbookair62c2osxosx10105build14f1605version11020201passticketmboekjt4scekrgkop52x3dw7dtf706nn06oet3552iiocf4nfr7p2fec0dm3jc7z3&quot;&gt;2.8 &lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;amp;mid=402796490&amp;amp;idx=1&amp;amp;sn=5f9fd2dbd9d0030c954084f2df75d410&amp;amp;scene=0&amp;amp;key=b28b03434249256bcc21f98e1dce38db43a18cba063d4f11b77c9091c999e698be4dfddc847ac0ec70d1785d3f3d0473&amp;amp;ascene=0&amp;amp;uin=MTAzNTc2NzM4Mg%3D%3D&amp;amp;devicetype=iMac+MacBookAir6%2C2+OSX+OSX+10.10.5+build(14F1605)&amp;amp;version=11020201&amp;amp;pass_ticket=mBoeKJT4SCEkrgKop52x3Dw7dtf706Nn06oeT3552IIocF4nfR7p%2FEc0dm3Jc7Z3&quot;&gt;从算法到案例：推荐系统必读的10篇精选技术文章&lt;/a&gt;&lt;/h3&gt;

&lt;h3 id=&quot;httpsmpweixinqqcomsbizmjm5mde0mjc4mamid402656056idx1sn2ebfde6f4df33ec690836f22c7f14d07scene0keyb28b03434249256b1d5444866538d49c0aa9cc9979fe2fecc4a249659942d85afdb654ce26577ded111a887c020c517fascene0uinmtazntc2nzm4mg3d3ddevicetypeimacmacbookair62c2osxosx10105build14f1605version11020201passticketmboekjt4scekrgkop52x3dw7dtf706nn06oet3552iiocf4nfr7p2fec0dm3jc7z3&quot;&gt;2.9 &lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MjM5MDE0Mjc4MA==&amp;amp;mid=402656056&amp;amp;idx=1&amp;amp;sn=2ebfde6f4df33ec690836f22c7f14d07&amp;amp;scene=0&amp;amp;key=b28b03434249256b1d5444866538d49c0aa9cc9979fe2fecc4a249659942d85afdb654ce26577ded111a887c020c517f&amp;amp;ascene=0&amp;amp;uin=MTAzNTc2NzM4Mg%3D%3D&amp;amp;devicetype=iMac+MacBookAir6%2C2+OSX+OSX+10.10.5+build(14F1605)&amp;amp;version=11020201&amp;amp;pass_ticket=mBoeKJT4SCEkrgKop52x3Dw7dtf706Nn06oeT3552IIocF4nfR7p%2FEc0dm3Jc7Z3&quot;&gt;推荐系统和搜索引擎的关系&lt;/a&gt;&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;

    &lt;p&gt;曾经被问到过的一个问题，其实看了本文大多数人应该都有感触 ：”对啊，就这几个简单的区别嘛，我知道啊”。但是，”知道” 和 “能清晰简捷的表述出来” 之间的差别还是很大的, that tells the difference。&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;httpwwwinfoqcomcnarticles20-outstanding-enterprise-technology-blog&quot;&gt;2.10 &lt;a href=&quot;http://www.infoq.com/cn/articles/20-outstanding-enterprise-technology-blog&quot;&gt;最值得关注的20个优秀企业技术博客&lt;/a&gt;&lt;/h3&gt;

&lt;h3 id=&quot;in-startups-and-life-you-need-plan-a-b-and-zhttptechcrunchcom20120214in-startups-and-life-you-need-plan-a-b-and-z&quot;&gt;2.11 &lt;a href=&quot;http://techcrunch.com/2012/02/14/in-startups-and-life-you-need-plan-a-b-and-z/&quot;&gt;In Startups And Life, You Need Plan A, B, And Z&lt;/a&gt;&lt;/h3&gt;

&lt;h3 id=&quot;githubhttpsmpweixinqqcomsbizmjm5mzm3njm4mamid2654675314idx4sn841dacff037413cd290c691dca609a81scene0keyb28b03434249256bf0f6bffac4d224a7b5968a0c614784efcb6ab64dd13fa148235f7863184e6e75702c3a61550b7a3cascene0uinmtazntc2nzm4mg3d3ddevicetypeimacmacbookair62c2osxosx10105build14f1605version11020201passticketodhk3wvwf6tilgqsv51gbzwyzkwkfp2q0schpmkwqckg72bfu1kkvjjq0mwpw8f52&quot;&gt;2.12 &lt;a href=&quot;https://mp.weixin.qq.com/s?__biz=MjM5MzM3NjM4MA==&amp;amp;mid=2654675314&amp;amp;idx=4&amp;amp;sn=841dacff037413cd290c691dca609a81&amp;amp;scene=0&amp;amp;key=b28b03434249256bf0f6bffac4d224a7b5968a0c614784efcb6ab64dd13fa148235f7863184e6e75702c3a61550b7a3c&amp;amp;ascene=0&amp;amp;uin=MTAzNTc2NzM4Mg%3D%3D&amp;amp;devicetype=iMac+MacBookAir6%2C2+OSX+OSX+10.10.5+build(14F1605)&amp;amp;version=11020201&amp;amp;pass_ticket=odHK3wvWF6tiLGqSv51GbzWYZKWKFp2q0sChpmkwqCKg7%2Bfu1KKvJjq0MwPW8f52&quot;&gt;安全专业人士最爱的19个GitHub开源项目&lt;/a&gt;&lt;/h3&gt;

&lt;h3 id=&quot;githttpblogcsdnnetqwe6112071articledetails51118761&quot;&gt;2.13 &lt;a href=&quot;http://blog.csdn.net/qwe6112071/article/details/51118761&quot;&gt;git分支原理命令图文解析&lt;/a&gt;&lt;/h3&gt;

&lt;h3 id=&quot;sysdiglinuxhttpslinuxcnarticle-4341-1html&quot;&gt;2.14 &lt;a href=&quot;https://linux.cn/article-4341-1.html&quot;&gt;系统之锹sysdig：Linux服务器监控和排障利器&lt;/a&gt;&lt;/h3&gt;

&lt;h3 id=&quot;spark-architecturehttps0x0fffcomspark-architecture&quot;&gt;2.15 &lt;a href=&quot;https://0x0fff.com/spark-architecture/&quot;&gt;Spark Architecture&lt;/a&gt;&lt;/h3&gt;

&lt;h3 id=&quot;hadoophttpblogfensmehadoop-family-roadmap&quot;&gt;2.16 &lt;a href=&quot;http://blog.fens.me/hadoop-family-roadmap/&quot;&gt;Hadoop家族学习路线图&lt;/a&gt;&lt;/h3&gt;

&lt;h2 id=&quot;section-3&quot;&gt;3. 读书总结系列&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;../books-recommend-and-summarize-on-mar-2016&quot;&gt;『 读书笔记 』3月读书总结和推荐&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;../books-recommend-and-summarize-on-apr-2016&quot;&gt;『 读书笔记 』4月读书总结｜博文推荐&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
   </entry>
   
   <entry>
     <title>『 Spark 』9. 搭建 IPython + Notebook + Spark 开发环境</title>
     <link href="/ipython-notebook-spark"/>
     <updated>2016-04-10T00:00:00+08:00</updated>
     <id>/ipython-notebook-spark</id>
     <content type="html">&lt;h2 id=&quot;section&quot;&gt;写在前面&lt;/h2&gt;

&lt;p&gt;本系列是综合了自己在学习spark过程中的理解记录 ＋ 对参考文章中的一些理解 ＋ 个人实践spark过程中的一些心得而来。写这样一个系列仅仅是为了梳理个人学习spark的笔记记录，所以一切以能够理解为主，没有必要的细节就不会记录了，而且文中有时候会出现英文原版文档，只要不影响理解，都不翻译了。若想深入了解，最好阅读参考文章和官方文档。&lt;/p&gt;

&lt;p&gt;其次，本系列是基于目前最新的 spark 1.6.0 系列开始的，spark 目前的更新速度很快，记录一下版本好还是必要的。 &lt;br /&gt;
最后，如果各位觉得内容有误，欢迎留言备注，所有留言 24 小时内必定回复，非常感谢。   &lt;br /&gt;
Tips: 如果插图看起来不明显，可以：1. 放大网页；2. 新标签中打开图片，查看原图哦。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;1. 致谢&lt;/h2&gt;

&lt;p&gt;首先我忠心地感谢 IPython，Spark 的开源作者，真心谢谢你们开发这么方便，好用，功能强大的项目，而且还无私地奉献给大众使用。刚刚很轻松地搭建了一个机遇 IPython Notebook 的 Spark 客户端，真的感受到 The power of technology, the power of open source.&lt;br /&gt;
下面是这两个项目的 github 地址：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/ipython/ipython&quot;&gt;Ipython&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/spark&quot;&gt;Spark&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;同时，这篇文章在刚开始的部分，参考了很多 &lt;a href=&quot;http://blog.cloudera.com/blog/2014/08/how-to-use-ipython-notebook-with-apache-spark/&quot;&gt;这篇博客&lt;/a&gt;的内容，感谢这么多人能无私分享如此高质量的内容。 &lt;br /&gt;
但是，这篇文章不是简单记录怎么做，我尽量做到量少质高，所以有些地方会说得比较详细，其中也会提到在解决遇到的问题上的一些方法和思路。&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;2. 原理&lt;/h2&gt;

&lt;p&gt;Ipython 支持自定义的配置文件，而且配置文件可以极其灵活的定义，我们可以借此在启动 IPython 的时候去做一些自定义的事，比如说加载一些模块，做一些初始化的工作。这里我们就是利用 Ipython 的这个灵活的特性，在启动 Ipython 的时候，自动加载 spark 的 pyspark 包，甚至是初始化一个 SparkContext。&lt;/p&gt;

&lt;p&gt;具体我们来看如何创建一个配置文件，并且指定一个配置文件启动 ipython。&lt;/p&gt;

&lt;h2 id=&quot;ipython&quot;&gt;3. 配置Ipython&lt;/h2&gt;

&lt;h3 id=&quot;ipython-profile&quot;&gt;3.1 ipython 配置名profile介绍&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;profile 命令说明&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;profile 是 ipython 的一个子命令，其中 profile 又有两个子命令，分别是 create和list，顾名思义，create就是创建一个配置文件，list就是列出当前配置文件。如下：&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;root@ubuntu2[13:54:01]:~/Desktop#ipython profile
No subcommand specified. Must specify one of: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&#39;create&#39;&lt;/span&gt;, &lt;span class=&quot;s1&quot;&gt;&#39;list&#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;

Manage IPython profiles

Profile directories contain configuration, log and security related files and
are named using the convention &lt;span class=&quot;s1&quot;&gt;&#39;profile_&amp;lt;name&amp;gt;&#39;&lt;/span&gt;. By default they are located &lt;span class=&quot;k&quot;&gt;in
&lt;/span&gt;your ipython directory.  You can create profiles with &lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;ipython profile create
&amp;lt;name&amp;gt;&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;, or see the profiles you already have with &lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;ipython profile list&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;

To get started configuring IPython, simply &lt;span class=&quot;k&quot;&gt;do&lt;/span&gt;:

&lt;span class=&quot;gp&quot;&gt;$&amp;gt; &lt;/span&gt;ipython profile create

and IPython will create the default profile &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &amp;lt;ipython_dir&amp;gt;/profile_default,
where you can edit ipython_config.py to start configuring IPython.

Subcommands
-----------

Subcommands are launched as &lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;ipython cmd &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;args]&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;. For information on using
subcommand &lt;span class=&quot;s1&quot;&gt;&#39;cmd&#39;&lt;/span&gt;, &lt;span class=&quot;k&quot;&gt;do&lt;/span&gt;: &lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;ipython cmd -h&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;.

create
    Create an IPython profile by name
list
    List available IPython profiles&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;profile子命令 list 说明&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;本想 list 命令应该很简单的，和 linux 下的 ls 差不多嘛，但我自己看了下，其中还是有些细节值得推敲的。其中这项 &lt;em&gt;Available profiles in /root/.config/ipython:&lt;/em&gt; 是说目前有两个配置文件在那个目录下面，pyspark是我自己创建的了。在参考的&lt;a href=&quot;http://blog.cloudera.com/blog/2014/08/how-to-use-ipython-notebook-with-apache-spark/&quot;&gt;这篇文章&lt;/a&gt;中，作者说创建的配置文件会放到 &lt;em&gt;~/.ipython/profile_pyspark/&lt;/em&gt; 下，其实这并不是一定的，具体放在哪个目录下面，可以根据 profile list 的命令来查看。如此看来，我们在这台机器上创建的配置文件应该是放在目录 &lt;em&gt;/root/.config/ipython&lt;/em&gt; 下面的。&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;root@ubuntu2[14:09:12]:~/Desktop#ipython profile list

Available profiles &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;IPython:
    pysh
    math
    sympy
    cluster

    The first request &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;a bundled profile will copy it
    into your IPython directory &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;/root/.config/ipython&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;,
    where you can customize it.

Available profiles &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; /root/.config/ipython:
    default
    pyspark

To use any of the above profiles, start IPython with:
    ipython --profile&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&amp;lt;name&amp;gt;  &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;profile子命令 create 说明&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;简单介绍下create子命令的用法。&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;root@ubuntu2[09:25:57]:~/Desktop#ipython profile &lt;span class=&quot;nb&quot;&gt;help &lt;/span&gt;create
Create an IPython profile by name

Create an ipython profile directory by its name or profile directory path.
Profile directories contain configuration, log and security related files and
are named using the convention &lt;span class=&quot;s1&quot;&gt;&#39;profile_&amp;lt;name&amp;gt;&#39;&lt;/span&gt;. By default they are located &lt;span class=&quot;k&quot;&gt;in
&lt;/span&gt;your ipython directory. Once created, you will can edit the configuration files
&lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;the profile directory to configure IPython. Most users will create a profile
directory by name, &lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;ipython profile create myprofile&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;, which will put the
directory &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;&amp;lt;ipython_dir&amp;gt;/profile_myprofile&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;.&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3 id=&quot;ipython-1&quot;&gt;3.2 创建新的Ipython配置文件&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;创建配置文件&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;因为我之前已经配置过一个pyspark的配置文件了，这里我们创建一个测试用的配置文件，pytest。运行一下命令后，会在 &lt;em&gt;/root/.config/ipython&lt;/em&gt; 下生成一个 pytest 的目录。&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-shell&quot; data-lang=&quot;shell&quot;&gt;root@ubuntu2[14:54:14]:~/Desktop#ipython profile create pytest
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;ProfileCreate] Generating default config file: u&lt;span class=&quot;s1&quot;&gt;&#39;/root/.config/ipython/profile_pytest/ipython_config.py&#39;&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;ProfileCreate] Generating default config file: u&lt;span class=&quot;s1&quot;&gt;&#39;/root/.config/ipython/profile_pytest/ipython_notebook_config.py&#39;&lt;/span&gt;

root@ubuntu2[15:00:57]:~/Desktop#ls ~/.config/ipython/profile_pytest/
ipython_config.py  ipython_notebook_config.py  log  pid  security  startup&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3 id=&quot;section-3&quot;&gt;3.3 编辑配置文件&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;编辑 &lt;em&gt;ipython_notebook_config.py&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;需要更改的只有下面三项：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;c.NotebookApp.ip&lt;/em&gt;: 启动服务的地址，设置成 ‘*’ 可以从同一网段的其他机器访问到；&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;c.NotebookApp.open_browser&lt;/em&gt;: 设置成 ‘False’，表示启动 ipython notebook 的时候不会自动打开浏览器；&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;c.NotebookApp.password&lt;/em&gt;: 设置 ipython notebook 的登陆密码，怎么设置看下面；&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NotebookApp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ip&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;*&#39;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NotebookApp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;open_browser&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;    
&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NotebookApp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;password&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;sha1:c6b748a8e1e2:4688f91ccfb9a8e0afd041ec77cdda99d0e1fb8f&quot;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;设置访问密码 &lt;br /&gt;
如果你的 notebook server 是需要访问控制的，简单的话可以设置一个访问密码。&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;生成密码&lt;/li&gt;
      &lt;li&gt;编辑配置文件，设置密码&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;In&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;IPython.lib&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;passwd&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;In&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;passwd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Enter&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;password&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Verify&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;password&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;sha1:e819609871c8:1039dbc5a1392fc230d371d1ce19511490978685&#39;&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;In&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;### set password &lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NotebookApp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;password&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;sha1:c6b748a8e1e2:4688f91ccfb9a8e0afd041ec77cdda99d0e1fb8f&quot;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;设置启动文件&lt;br /&gt;
这一步算是比较重要的了，也是我在配置这个notebook server中遇到的比较难解的问题。这里我们首先需要创建一个启动文件，并在启动文件里设置一些spark的启动参数。如下：&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;root@ubuntu2[09:52:14]:~/Desktop#touch ~/.config/ipython/profile_pytest/startup/00-pytest-setup.py
root@ubuntu2[10:08:44]:~/Desktop#vi ~/.config/ipython/profile_pytest/startup/00-pytest-setup.py   

import os
import sys

spark_home = os.environ.get(&#39;SPARK_HOME&#39;, None)
if not spark_home:
    raise ValueError(&#39;SPARK_HOME environment variable is not set&#39;)
sys.path.insert(0, os.path.join(spark_home, &#39;python&#39;))
sys.path.insert(0, os.path.join(spark_home, &#39;python/lib/py4j-0.8.2.1-src.zip&#39;))
# execfile(os.path.join(spark_home, &#39;python/pyspark/shell.py&#39;))
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;上面的启动配置文件也还简单，即拿到 &lt;em&gt;spark_home&lt;/em&gt; 路径，并在系统环境变量 path 里加上两个路径，然后再执行一个 &lt;em&gt;shell.py&lt;/em&gt; 文件。不过，在保存之前还是先确认下配置文件写对了，比如说你的 SPARK_HOME 配置对了，并且下面有 python 这个文件夹，并且 &lt;em&gt;python/lib 下有 py4j-0.8.1&lt;/em&gt; 这个文件。我在检查的时候就发现我的包版本是 py4j-0.8.2.1 的，所以还是要改得和自己的包一致才行。 &lt;br /&gt;
这里得到一个经验，在这种手把手，step by step的教程中，一定要注意版本控制，毕竟各人的机器，操作系统，软件版本等都不可能完全一致，也许在别人机器上能成功，在自己的机器上不成功也是很正常的事情，毕竟细节决定成败啊！所以在我这里，这句我是这样写的： &lt;em&gt;sys.path.insert(0, os.path.join(spark_home, ‘python/lib/py4j-0.8.2.1-src.zip’))&lt;/em&gt;  &lt;br /&gt;
注意，上面的最后一行 &lt;em&gt;execfile(os.path.join(spark_home, ‘python/pyspark/shell.py’))&lt;/em&gt; 被注释掉了，表示在新建或打开一个 notebook 时并不去执行 &lt;em&gt;shell.py&lt;/em&gt; 这个文件，这个文件是创建 SparkContext 的，即如果执行改行语句，那在启动 notebook 时就会初始化一个 sc，但这个 sc 的配置都是写死了的，在 spark web UI 监控里的 appName 也是一样的，很不方便。而且考虑到并不是打开一个 notebook 就要用到 spark 的资源，所以最好是要用户自己定义 sc 了。&lt;/p&gt;

&lt;p&gt;python/pyspark/shell.py 的核心代码：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sc = SparkContext(appName=&quot;PySparkShell&quot;, pyFiles=add_files)
atexit.register(lambda: sc.stop())
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;okhere-we-go&quot;&gt;4. Ok，here we go&lt;/h2&gt;
&lt;p&gt;到这里差不多大功告成了，可以启动notebook server了。不过在启动之前，需要配置两个环境变量参数，同样，这两个环境变量参数在也是根据个人配置而定的。&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# for the CDH-installed Spark
export SPARK_HOME=&#39;/usr/local/spark-1.2.0-bin-cdh4/&#39;

# this is where you specify all the options you would normally add after bin/pyspark
export PYSPARK_SUBMIT_ARGS=&#39;--master spark://10.21.208.21:7077 --deploy-mode client&#39;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;ok，万事具备，只欠东风了。让我们来尝尝鲜吧：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;root@ubuntu2[10:40:50]:~/Desktop#ipython notebook --profile=pyspark
2015-02-01 10:40:54.850 [NotebookApp] Using existing profile dir: u&#39;/root/.config/ipython/profile_pyspark&#39;
2015-02-01 10:40:54.858 [NotebookApp] Using MathJax from CDN: http://cdn.mathjax.org/mathjax/latest/MathJax.js
2015-02-01 10:40:54.868 [NotebookApp] CRITICAL | WARNING: The notebook server is listening on all IP addresses and not using encryption. This is not recommended.
2015-02-01 10:40:54.869 [NotebookApp] Serving notebooks from local directory: /root/Desktop
2015-02-01 10:40:54.869 [NotebookApp] The IPython Notebook is running at: http://[all ip addresses on your system]:8880/
2015-02-01 10:40:54.869 [NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;在浏览器输入driver:8880即可访问notebook server了，首先会提示输入密码，密码正确后就可以使用了。
&lt;img src=&quot;http://litaotao.github.io/images/notebook-spark-1.jpg&quot; alt=&quot;notebook-spark-1&quot; /&gt;
&lt;img src=&quot;http://litaotao.github.io/images/notebook-spark-2.jpg&quot; alt=&quot;notebook-spark-2&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-4&quot;&gt;5. 总结&lt;/h2&gt;
&lt;p&gt;下面是简单的步骤总结：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;建立环境变量配置文件：*ipython_notebook_spark.bashrc *&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;export SPARK_HOME=&quot;/usr/local/spark-1.2.0-bin-cdh4/&quot;
export PYSPARK_SUBMIT_ARGS=&quot;--master spark://10.21.208.21:7077 --deploy-mode client&quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;配置Ipython notebook server
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;ipython profile create pyspark&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;编辑 &lt;em&gt;ipython_notebook_config.py&lt;/em&gt;&lt;/li&gt;
      &lt;li&gt;[可选]配置ipython notebook登录密码&lt;/li&gt;
      &lt;li&gt;设置启动文件&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;设置启动脚本&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;next&quot;&gt;4. next&lt;/h2&gt;

&lt;p&gt;这个例子还算 ok 吧，可是我每天都应用的投资策略的一部分啊，已经下血本了，各位还不打赏打赏吗？一转眼 spark 已经快要有十篇 blog 了，期间也写了一些 spark 的应用程序，读了一些高质量的书和博文，在 youtube 上也看了一些高质量的技术分享。也总结了一些写 spark 应用程序的时候的细节问题，下一篇就列一下这些细节问题，希望能优化，加速各位的 spark 应用程序。&lt;/p&gt;

&lt;h2 id=&quot;section-5&quot;&gt;5. 打开微信，扫一扫，点一点，棒棒的，^_^&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;../images/wechat_pay_6-6.png&quot; alt=&quot;wechat_pay_6-6.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-6&quot;&gt;本系列文章链接&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/introduction-to-spark&quot;&gt;『 Spark 』1. spark 简介 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/spark-questions-concepts&quot;&gt;『 Spark 』2. spark 基本概念解析 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/spark-programming-model&quot;&gt;『 Spark 』3. spark 编程模式 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/spark-what-is-rdd&quot;&gt;『 Spark 』4. spark 之 RDD &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/spark-resouces-blogs-paper&quot;&gt;『 Spark 』5. 这些年，你不能错过的 spark 学习资源 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/deep-into-spark-exection-model&quot;&gt;『 Spark 』6. 深入研究 spark 运行原理之 job, stage, task&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/spark-dataframe-introduction&quot;&gt;『 Spark 』7. 使用 Spark DataFrame 进行大数据分析&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/spark-in-finance-and-investing&quot;&gt;『 Spark 』8. 实战案例 ｜ Spark 在金融领域的应用 ｜ 日内走势预测&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/ipython-notebook-spark&quot;&gt;『 Spark 』9. 搭建 IPython + Notebook + Spark 开发环境&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/boost-spark-application-performance&quot;&gt;『 Spark 』10. spark 应用程序性能优化｜12 个优化方法&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
   </entry>
   
   <entry>
     <title>『 Spark 』8. 实战案例 ｜ Spark 在金融领域的应用 ｜ 日内走势预测</title>
     <link href="/spark-in-finance-and-investing"/>
     <updated>2016-04-01T00:00:00+08:00</updated>
     <id>/spark-in-finance-and-investing</id>
     <content type="html">&lt;h2 id=&quot;section&quot;&gt;写在前面&lt;/h2&gt;

&lt;p&gt;本系列是综合了自己在学习spark过程中的理解记录 ＋ 对参考文章中的一些理解 ＋ 个人实践spark过程中的一些心得而来。写这样一个系列仅仅是为了梳理个人学习spark的笔记记录，所以一切以能够理解为主，没有必要的细节就不会记录了，而且文中有时候会出现英文原版文档，只要不影响理解，都不翻译了。若想深入了解，最好阅读参考文章和官方文档。&lt;/p&gt;

&lt;p&gt;其次，本系列是基于目前最新的 spark 1.6.0 系列开始的，spark 目前的更新速度很快，记录一下版本好还是必要的。 &lt;br /&gt;
最后，如果各位觉得内容有误，欢迎留言备注，所有留言 24 小时内必定回复，非常感谢。   &lt;br /&gt;
Tips: 如果插图看起来不明显，可以：1. 放大网页；2. 新标签中打开图片，查看原图哦。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;1. 同花顺收费版之走势预测&lt;/h2&gt;

&lt;p&gt;2014年后半年开始，国内 A 股市场可谓是热火朝天啊，路上的人谈的都是股票。小弟虽然就职金融互联网公司，但之前从来没有买过股票，但每天听着别人又赚了几套房几辆车，那叫一个心痒痒啊，那感觉，就跟一个出浴美女和你共处一室，但你却要死忍住不去掀开浴巾一样。终于，小弟还是”犯了全天下男人都会犯的错误”，还是在 2015.03.19 那天入市了，还记得自己的第一次是献给了一支叫 &lt;code class=&quot;highlighter-rouge&quot;&gt;天建集团&lt;/code&gt; 的股票，好像当天还赚了一两百块吧，当时心情那叫一个激动，下班了第一时间就打电话给娘亲了。&lt;/p&gt;

&lt;p&gt;哦，似乎有点扯得远了。言归正传，当时自己为了投资更方便，就花了将近 300 大洋买了同花顺的 level 2 版，里面有个功能，叫做 &lt;code class=&quot;highlighter-rouge&quot;&gt;形态预测&lt;/code&gt;。具体就是，根据所有股票的历史行情，看看当前股票的未来一段时间的走势分布。下面是一个截图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/spark-in-finance-1.jpg&quot; alt=&quot;spark-in-finance-1.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;截图说明：颜色越深，概率越大，包括一组预测的 k 线走势。就像上面说的，上面的那支股票的预测结果是：未来3周收益大于 4.0% 的概率有 60%。amazing…&lt;/p&gt;

&lt;p&gt;先不说这个预测准确度有多高，但首先这个思路不错，至少可以作为一个信号吧［当然一个稳健的投资策略肯定不能仅仅依赖于一个信号］&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;2. 形态选股&lt;/h2&gt;

&lt;p&gt;同花顺这个功能，其实也挺实用的，因为本身在股票市场技术指标这个分类下面，就有形态选股这样一种指标。比如说，经常听财经频道主持人说的 &lt;a href=&quot;http://baike.baidu.com/item/%E4%B8%89%E9%98%B3%E5%BC%80%E6%B3%B0/18751451&quot;&gt;三阳开泰&lt;/a&gt;，&lt;a href=&quot;http://baike.baidu.com/view/1302521.htm&quot;&gt;圆弧底&lt;/a&gt; 什么的。&lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;3. 指数日内相似度&lt;/h2&gt;

&lt;p&gt;今天，我们就来尝试一下，通过指数日内走势来进行宏观择时: 我们在早盘 11:00 时，使用当天上证指数的分时图，预测一下当天走势情况。&lt;/p&gt;

&lt;p&gt;原理如下：使用上证指数历史分时数据，计算历史上每天 09:30 到 11:00 的分时段走势与今天早盘 09:30 到 11:00 走势的相似度。我们认为，相似度越高，则今日 11:00 到 15:00 走势和 15:00 的收盘涨跌，与历史当日的走势和收盘涨跌有较大的相似度。&lt;/p&gt;

&lt;p&gt;结果预览，如下图所示哦：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/spark-in-finance-2.jpg&quot; alt=&quot;spark-in-finance-2.jpg&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;spark-&quot;&gt;4. spark 实现指数日内相似度&lt;/h2&gt;

&lt;p&gt;同样，我们也用第三篇 &lt;a href=&quot;../spark-programming-model&quot;&gt;『 Spark 』3. spark 编程模式 &lt;/a&gt; 讲到的三个步骤来实现这个简单的，但有实践意义的 spark 应用程序。&lt;/p&gt;

&lt;p&gt;备注：为了方便理解，我把这个例子精简过了，只用上证指数 6 年的分钟线数据，对应的相似度算法也是采用最简单的算法。但是不影响对整个应用框架的理解和扩展。&lt;/p&gt;

&lt;h3 id=&quot;section-4&quot;&gt;4.1 加载数据集&lt;/h3&gt;

&lt;p&gt;本文用到的数据集已经上传到百度云了，上传文件是一个压缩文件，解压缩后把整个文件夹上传到 hadoop 上就行了，文件夹里有 1505 个文件，文件名表示上证指数某日的分钟线行情，文件内容即为历史当日分钟线行情：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/spark-in-finance-3.jpg&quot; alt=&quot;spark-in-finance-3.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;下载链接：&lt;a href=&quot;http://pan.baidu.com/s/1jIvW4mU&quot;&gt;minute_bar.zip on baidu&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;下面，我们先创建 SparkContext，然后加载存放在 hdfs 上的数据。&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;### 创建 sc&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SparkContext&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sc_conf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;except&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SparkContext&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sc_conf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;### 加载 hdfs 上的数据&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;url&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;hdfs://10.21.208.21:8020/user/mercury/minute_bar&#39;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;rdd_mkt_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;wholeTextFiles&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;minPartitions&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;80&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
                 &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;index_minute_bar&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
                 &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3 id=&quot;section-5&quot;&gt;4.2 处理数据&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;指定要预测的分钟线&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;### UDF 函数，从 rdd_mkt_data 获取某日历史分钟线行情数据&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;minute_bar_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;line_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;line_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rdd_mkt_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;filter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line_id&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;collect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;json&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;line_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sort&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;barTime&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ascending&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;### 指定想要预测的线的 id，这里我们预测上证指数 2016.03.17 的分钟线&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;target_line&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;000001.ZICN-20160317&#39;&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;### 指定用于计算相似度的分钟线长度，这里我们用 90 个分钟 bar，&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;### 即开盘 09:30 到 11:00 的分钟线&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;minute_bar_length&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;90&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;minute_bar_length_share&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;broadcast&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;minute_bar_length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;target_line_mkt_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;minute_bar_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target_line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;target_line_share&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;broadcast&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target_line_mkt_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;计算相似度&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;### 相似度计算函数&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;cal_similarity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;计算相似度
    &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;### 使用 sklearn，pandas 来简化计算流程&lt;/span&gt;
    &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pd&lt;/span&gt;
    &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.preprocessing&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;scaler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sklearn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;preprocessing&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MinMaxScaler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    
    &lt;span class=&quot;c&quot;&gt;### 通过广播变量获取预测的目标线和准备用来预测的分钟线长度&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;minute_length&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;minute_bar_length_share&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;target_line&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target_line_share&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;value&lt;/span&gt;
    
    &lt;span class=&quot;c&quot;&gt;### 参数 line 的格式是： (line_id, line_data)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;line_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;
    
    &lt;span class=&quot;c&quot;&gt;### 获取 pandas dataframe 格式的某日分钟线行情&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ticker&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tradeDate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;25&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;-&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;line_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;json&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;line_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;line_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sort&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;barTime&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ascending&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;c&quot;&gt;### 每天有 240 条分钟线的 bar，我们用 前 minute_length 来计算相似度&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;line1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target_line&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ratio&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;minute_length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;line2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;line_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ratio&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;minute_length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;tmp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;    
    &lt;span class=&quot;n&quot;&gt;tmp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;first&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tmp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;second&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line2&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tmp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;diff&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tmp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;first&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tmp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;second&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;diff_square&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tmp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;diff&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;### 返回格式：(分钟线id，该分钟线和目标线前 minute_length 个长度的相似度)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;line_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;25&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diff_square&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;                


&lt;span class=&quot;c&quot;&gt;### spark 相似度计算代码&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;rdd_similarity&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rdd_mkt_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cal_similarity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;\
                             &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;rdd_similarity&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
                             &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3 id=&quot;section-6&quot;&gt;4.3 结果展示&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;获取相似度高的分钟线&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;### UDF，从 rdd_mkt_data 里获取指定的多日分钟线数据&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_similary_line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;similarity_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;### 获取原始相似的分钟线数据&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;rdd_lines&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rdd_mkt_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;filter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;25&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;similarity_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;collect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;### 把原始分钟线数据转成 pandas dataframe 格式&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;similar_line&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;25&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;json&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt; 
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rdd_lines&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;similar_line&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;similar_line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sort&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;barTime&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ascending&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;similar_line&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;similar_line&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;### 获取相似度最高的30日分钟线&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;similarity_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rdd_similarity&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;takeOrdered&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;30&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;similar_line&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_similary_line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;similarity_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;根据相似分钟线绘制预测图&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;draw_similarity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target_line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;minute_bar_length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;similarity_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;similarity_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;line_id&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;line_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;similar_line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;line_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;line_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ratio&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;minute&#39;&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;minute&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;barTime&lt;/span&gt;  
        &lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;line_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;fitting&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;target_line&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target_line_mkt_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ratio&lt;/span&gt;
    
    &lt;span class=&quot;c&quot;&gt;### plot &lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;minute&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;13&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; 
                  &lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;u&#39;Minute Bar Prediction&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;target_line&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;linewidth&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;style&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;.b&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;fitting&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;linewidth&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;style&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;-y&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vlines&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;minute_bar_length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ymin&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.02&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ymax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.02&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
              &lt;span class=&quot;n&quot;&gt;linestyles&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;dashed&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_axis_bgcolor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;white&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;gray&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;y&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;c&quot;&gt;### plot area&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;avg_line&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;fitting&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;avg_line&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;avg_line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;minute_bar_length&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;predict_line&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;predict_line&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict_line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;minute_bar_length&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fill_between&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;minute_bar_length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;241&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;avg_line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                        &lt;span class=&quot;n&quot;&gt;predict_line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;r&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;draw_similarity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target_line&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;minute_bar_length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;similarity_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;../images/spark-in-finance-2.jpg&quot; alt=&quot;spark-in-finance-2.jpg&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;next&quot;&gt;5. Next&lt;/h2&gt;

&lt;p&gt;这个例子还算 ok 吧，可是我每天都应用的投资策略的一部分啊，已经下血本了，各位还不打赏打赏吗？一转眼 spark 已经快要有十篇 blog 了，本来原计划第九篇是总结一些 spark 性能优化的 tips 的。可是前几天一个朋友突然问我是怎么开发 spark 应用程序的。我才恍然大悟，一下子写了这么多篇，都没有把搭建开发环境的经验写出来的呢。&lt;/p&gt;

&lt;p&gt;下一篇我就总结一下自己怎么搭建的一个 ipython + spark 的开发环境；不管各位有没有用过 ipython [notebook]，我都强烈推荐使用，使用它能打打提高你的开发效率和开发体验，你一定会爱上他的，相信我。&lt;/p&gt;

&lt;h2 id=&quot;section-7&quot;&gt;6. 打开微信，扫一扫，点一点，棒棒的，^_^&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;../images/wechat_pay_6-6.png&quot; alt=&quot;wechat_pay_6-6.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-8&quot;&gt;参考文章&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://spark.apache.org/docs/latest/sql-programming-guide.html#dataframes&quot;&gt;Spark SQL, DataFrames and Datasets Guide&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html&quot;&gt;Introducing DataFrames in Spark for Large Scale Data Science&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://forums.databricks.com/questions/7257/from-webinar-spark-dataframes-what-is-the-differen-1.html&quot;&gt;From Webinar Apache Spark 1.5: What is the difference between a DataFrame and a RDD?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.infoq.com/cn/articles/apache-spark-sql&quot;&gt;用Apache Spark进行大数据处理——第二部分：Spark SQL&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://databricks.com/blog/2015/02/02/an-introduction-to-json-support-in-spark-sql.html&quot;&gt;An introduction to JSON support in Spark SQL&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.csdn.net/article/2015-02-18/2823997&quot;&gt;Spark新年福音：一个用于大规模数据科学的API——DataFrame&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://databricks.com/blog/2015/02/02/an-introduction-to-json-support-in-spark-sql.html&quot;&gt;An introduction to JSON support in Spark SQL&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-9&quot;&gt;本系列文章链接&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/introduction-to-spark&quot;&gt;『 Spark 』1. spark 简介 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/spark-questions-concepts&quot;&gt;『 Spark 』2. spark 基本概念解析 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/spark-programming-model&quot;&gt;『 Spark 』3. spark 编程模式 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/spark-what-is-rdd&quot;&gt;『 Spark 』4. spark 之 RDD &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/spark-resouces-blogs-paper&quot;&gt;『 Spark 』5. 这些年，你不能错过的 spark 学习资源 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/deep-into-spark-exection-model&quot;&gt;『 Spark 』6. 深入研究 spark 运行原理之 job, stage, task&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/spark-dataframe-introduction&quot;&gt;『 Spark 』7. 使用 Spark DataFrame 进行大数据分析&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/spark-in-finance-and-investing&quot;&gt;『 Spark 』8. 实战案例 ｜ Spark 在金融领域的应用 ｜ 日内走势预测&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/ipython-notebook-spark&quot;&gt;『 Spark 』9. 搭建 IPython + Notebook + Spark 开发环境&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/boost-spark-application-performance&quot;&gt;『 Spark 』10. spark 应用程序性能优化｜12 个优化方法&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content>
   </entry>
   
   <entry>
     <title>『 Spark 』7. 使用 Spark DataFrame 进行大数据分析</title>
     <link href="/spark-dataframe-introduction"/>
     <updated>2016-03-30T00:00:00+08:00</updated>
     <id>/spark-dataframe-introduction</id>
     <content type="html">&lt;h2 id=&quot;section&quot;&gt;写在前面&lt;/h2&gt;

&lt;p&gt;本系列是综合了自己在学习spark过程中的理解记录 ＋ 对参考文章中的一些理解 ＋ 个人实践spark过程中的一些心得而来。写这样一个系列仅仅是为了梳理个人学习spark的笔记记录，所以一切以能够理解为主，没有必要的细节就不会记录了，而且文中有时候会出现英文原版文档，只要不影响理解，都不翻译了。若想深入了解，最好阅读参考文章和官方文档。&lt;/p&gt;

&lt;p&gt;其次，本系列是基于目前最新的 spark 1.6.0 系列开始的，spark 目前的更新速度很快，记录一下版本好还是必要的。 &lt;br /&gt;
最后，如果各位觉得内容有误，欢迎留言备注，所有留言 24 小时内必定回复，非常感谢。   &lt;br /&gt;
Tips: 如果插图看起来不明显，可以：1. 放大网页；2. 新标签中打开图片，查看原图哦。&lt;/p&gt;

&lt;h2 id=&quot;spark-dataframe&quot;&gt;1. 什么是 spark dataframe&lt;/h2&gt;

&lt;p&gt;先来看看官方原汁原味的文档是怎么介绍的：&lt;/p&gt;

&lt;p&gt;A DataFrame is &lt;code class=&quot;highlighter-rouge&quot;&gt;a distributed collection of data&lt;/code&gt; organized into named columns. It is conceptually equivalent to a &lt;code class=&quot;highlighter-rouge&quot;&gt;table in a relational database&lt;/code&gt; or a data frame in R/Python, but with &lt;code class=&quot;highlighter-rouge&quot;&gt;richer optimizations&lt;/code&gt; under the hood. DataFrames can be constructed from a wide array of sources such as: &lt;code class=&quot;highlighter-rouge&quot;&gt;structured data files, tables in Hive, external databases, or existing RDDs&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;我们可以看到 spark dataframe 的几个关键点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;分布式的数据集&lt;/li&gt;
  &lt;li&gt;类似关系型数据库中的table，或者 excel 里的一张 sheet，或者 python/R 里的 dataframe&lt;/li&gt;
  &lt;li&gt;拥有丰富的操作函数，类似于 rdd 中的算子&lt;/li&gt;
  &lt;li&gt;一个 dataframe 可以被注册成一张数据表，然后用 sql 语言在上面操作&lt;/li&gt;
  &lt;li&gt;丰富的创建方式
    &lt;ul&gt;
      &lt;li&gt;已有的RDD&lt;/li&gt;
      &lt;li&gt;结构化数据文件&lt;/li&gt;
      &lt;li&gt;JSON数据集&lt;/li&gt;
      &lt;li&gt;Hive表&lt;/li&gt;
      &lt;li&gt;外部数据库&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;spark-dataframe-1&quot;&gt;2. 为什么要用 spark dataframe&lt;/h2&gt;

&lt;p&gt;为什么要用 dataframe，从细节实现上来说，这个问题比较复杂，不过，基本上下面这张图就能说明所有问题了：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/spark-dataframe-flow.png&quot; alt=&quot;spark-dataframe-flow.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;但是，本文是从基础角度来说 spark dataframe，先不纠结这些细节问题，先了解一些基础的原理和优势，关于上面那张图里面的内容，看后期安排，也许在之后第 15 篇左右会专门讲。&lt;/p&gt;

&lt;p&gt;DataFrame API 是在 R 和 Python data frame 的设计灵感之上设计的，具有以下功能特性：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;从KB到PB级的数据量支持；&lt;/li&gt;
  &lt;li&gt;多种数据格式和多种存储系统支持；&lt;/li&gt;
  &lt;li&gt;通过Spark SQL 的 Catalyst优化器进行先进的优化，生成代码；&lt;/li&gt;
  &lt;li&gt;通过Spark无缝集成所有大数据工具与基础设施；&lt;/li&gt;
  &lt;li&gt;为Python、Java、Scala和R语言（SparkR）API；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;简单来说，dataframe 能够更方便的操作数据集，而且因为其底层是通过 spark sql 的 Catalyst优化器生成优化后的执行代码，所以其执行速度会更快。总结下来就是，使用 spark dataframe 来构建 spark app，能：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;write less : 写更少的代码&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;do more : 做更多的事情&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;faster : 以更快的速度&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;dataframe&quot;&gt;3. 创建 dataframe&lt;/h2&gt;

&lt;p&gt;因为 spark sql，dataframe，datasets 都是共用 spark sql 这个库的，三者共享同样的代码优化，生成以及执行流程，所以 sql，dataframe，datasets 的入口都是 sqlContext。可用于创建 spark dataframe 的数据源有很多，我们就讲最简单的从结构化文件创建 dataframe。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/spark-dataframe-3.jpg&quot; alt=&quot;spark-dataframe-3.jpg&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;step 1 : 创建 sqlContext&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下面是我自己创建 spark sc 都模版：&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;sc_conf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SparkConf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sc_conf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setAppName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;03-DataFrame-01&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sc_conf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setMaster&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SPARK_MASTER&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sc_conf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;spark.executor.memory&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;2g&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sc_conf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;spark.logConf&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sc_conf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getAll&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sleep&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;except&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SparkContext&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sc_conf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sqlContext&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SQLContext&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;step 2 : 创建 dataframe，从 json 文件&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;数据文件说明：中国 A 股上市公司基本信息，可以在这里取到：&lt;a href=&quot;http://pan.baidu.com/s/1pLxN851&quot;&gt;stock_5.json&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/spark-dataframe-1.jpg&quot; alt=&quot;spark-dataframe-1.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;注：这里的 json 文件并不是标准的 json 文件，spark 目前也不支持读取标准的 json 文件。你需要预先把标准的 json 文件处理成 spark 支持的格式: 每一行是一个 json 对象。&lt;/p&gt;

&lt;p&gt;比如说，官网的 &lt;code class=&quot;highlighter-rouge&quot;&gt;people.json&lt;/code&gt; 这个例子，它要求的格式是：&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Yin&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;address&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:{&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;city&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Columbus&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;state&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Ohio&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Michael&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;address&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:{&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;city&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;null&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;state&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;California&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;但对这个文件来看，标准的 json 格式只有下面两种：&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Yin&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Michael&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
 &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;address&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;city&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Columbus&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;state&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Ohio&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;city&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;null&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;state&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;California&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;###&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;或者&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Yin&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;address&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:{&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;city&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Columbus&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;state&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Ohio&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Michael&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;address&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:{&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;city&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;null&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;state&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;California&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;所以在用 spark sql 来读取一个 json 文件的时候，务必要提前处理好 json 的文件格式，这里我们已经提前处理好了，文件如下所示：&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;ticker&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;000001&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;tradeDate&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;2016-03-30&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;exchangeCD&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;XSHE&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;secShortName&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;\u5e73\u5b89\u94f6\u884c&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;preClosePrice&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;10.43&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;openPrice&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;10.48&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;dealAmount&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;19661&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;turnoverValue&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;572627417.1299999952&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;highestPrice&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;10.7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;lowestPrice&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;10.47&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;closePrice&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;10.7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;negMarketValue&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;126303384220.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;marketValue&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;153102835340.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;isOpen&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;secID&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;000001.XSHE&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;listDate&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;1991-04-03&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;ListSector&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;\u4e3b\u677f&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;totalShares&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;14308676200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;ticker&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;000002&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;tradeDate&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;2016-03-30&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;exchangeCD&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;XSHE&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;secShortName&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;\u4e07\u79d1A&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;preClosePrice&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;24.43&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;openPrice&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;dealAmount&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;turnoverValue&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;highestPrice&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;lowestPrice&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;closePrice&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;24.43&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;negMarketValue&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;237174448154.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;marketValue&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;269685994760.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;isOpen&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;secID&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;000002.XSHE&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;listDate&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;1991-01-29&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;ListSector&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;\u4e3b\u677f&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;totalShares&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;11039132000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;### df is short for dataframe&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sqlContext&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;json&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;hdfs://10.21.208.21:8020/user/mercury/stock_5.json&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;printSchema&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;select&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;ticker&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;secID&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;tradeDate&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;listDate&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;openPrice&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;closePrice&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                 &lt;span class=&quot;s&quot;&gt;&#39;highestPrice&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;lowestPrice&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;isOpen&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;../images/spark-dataframe-2.jpg&quot; alt=&quot;spark-dataframe-2.jpg&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;dataframe-1&quot;&gt;4. 操作 dataframe&lt;/h2&gt;

&lt;p&gt;同 rdd 一样，dataframe 也有很多专属于自己的算子，用于操作整个 dataframe 数据集，我们以后都简称为 dataframe api 吧，用 &lt;code class=&quot;highlighter-rouge&quot;&gt;算子&lt;/code&gt;， &lt;code class=&quot;highlighter-rouge&quot;&gt;DSL&lt;/code&gt; 这类的称呼对不熟悉的人来说不易理解，下面这里是完整的 api 列表：&lt;a href=&quot;http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame&quot;&gt;spark dataframe api&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;dataframe--sql-&quot;&gt;4.1 在 dataframe 上执行 sql 语句&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;../images/spark-dataframe-4.jpg&quot; alt=&quot;spark-dataframe-4.jpg&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;spark-dataframe--pandas-dataframe-&quot;&gt;4.2 spark dataframe 与 pandas dataframe 转换&lt;/h3&gt;

&lt;p&gt;一图胜千言啊：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/spark-dataframe-6.jpg&quot; alt=&quot;spark-dataframe-6.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;纵观 spark 的诞生和发展，我觉得 spark 有一点做得非常明智：&lt;em&gt;对同类产品的兼容&lt;/em&gt;。从大的方面来说，就像 spark 官网的这段话一样: &lt;em&gt;Runs Everywhere: Spark runs on Hadoop, Mesos, standalone, or in the cloud. It can access diverse data sources including HDFS, Cassandra, HBase, and S3.&lt;/em&gt;，spark 对 hadoop 系产品的兼容，让 hadoop 系的开发人员可以轻松的从 hadoop 转到 spark；从小的方面来说，spark 对一些细分工具也照顾 [兼容] 得很好，比如说 spark 推出了 dataframe，人家就可以支持 spark dataframe 和 pandas dataframe 的转换。&lt;/p&gt;

&lt;p&gt;熟悉 pandas dataframe 的都了解，pandas 里的 dataframe 可以做很多事情，比如说画图，保存为各种类型的文件，做数据分析什么的。我觉得，可以在 spark 的 dataframe 里做数据处理，分析的整个逻辑，然后可以把最后的结果转化成 pandas 的 dataframe 来展示。当然，如果你的数据量小，也可以直接用 pandas dataframe 来做。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/spark-dataframe-7.jpg&quot; alt=&quot;spark-dataframe-7.jpg&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;5. 一些经验&lt;/h2&gt;

&lt;h3 id=&quot;spark-json-&quot;&gt;5.1 spark json 格式问题&lt;/h3&gt;

&lt;p&gt;spark 目前也不支持读取标准的 json 文件。你需要预先把标准的 json 文件处理成 spark 支持的格式: 每一行是一个 json 对象。&lt;/p&gt;

&lt;h3 id=&quot;spark-dataframe--pandas-dataframe--1&quot;&gt;5.2 spark dataframe 和 pandas dataframe 选择问题&lt;/h3&gt;

&lt;p&gt;如果数据量小，结构简单，可以直接用 pandas dataframe 来做分析；如果数据量大，结构复杂 [嵌套结构]，那么推荐用 spark dataframe 来做数据分析，然后把结果转成 pandas dataframe，用 pandas dataframe 来做展示和报告。&lt;/p&gt;

&lt;h2 id=&quot;next&quot;&gt;6. Next&lt;/h2&gt;

&lt;p&gt;ok，dataframe 简单的也说了几句了。我们先缓一缓，上个例子，再接着讲起他的，例子的话就用一个我正在实践的：用 spark 来做量化投资。&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;7. 打开微信，扫一扫，点一点，棒棒的，^_^&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;../images/wechat_pay_6-6.png&quot; alt=&quot;wechat_pay_6-6.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;参考文章&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://spark.apache.org/docs/latest/sql-programming-guide.html#dataframes&quot;&gt;Spark SQL, DataFrames and Datasets Guide&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html&quot;&gt;Introducing DataFrames in Spark for Large Scale Data Science&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://forums.databricks.com/questions/7257/from-webinar-spark-dataframes-what-is-the-differen-1.html&quot;&gt;From Webinar Apache Spark 1.5: What is the difference between a DataFrame and a RDD?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.infoq.com/cn/articles/apache-spark-sql&quot;&gt;用Apache Spark进行大数据处理——第二部分：Spark SQL&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://databricks.com/blog/2015/02/02/an-introduction-to-json-support-in-spark-sql.html&quot;&gt;An introduction to JSON support in Spark SQL&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.csdn.net/article/2015-02-18/2823997&quot;&gt;Spark新年福音：一个用于大规模数据科学的API——DataFrame&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://databricks.com/blog/2015/02/02/an-introduction-to-json-support-in-spark-sql.html&quot;&gt;An introduction to JSON support in Spark SQL&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-4&quot;&gt;本系列文章链接&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/introduction-to-spark&quot;&gt;『 Spark 』1. spark 简介 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/spark-questions-concepts&quot;&gt;『 Spark 』2. spark 基本概念解析 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/spark-programming-model&quot;&gt;『 Spark 』3. spark 编程模式 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/spark-what-is-rdd&quot;&gt;『 Spark 』4. spark 之 RDD &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/spark-resouces-blogs-paper&quot;&gt;『 Spark 』5. 这些年，你不能错过的 spark 学习资源 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/deep-into-spark-exection-model&quot;&gt;『 Spark 』6. 深入研究 spark 运行原理之 job, stage, task&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/spark-dataframe-introduction&quot;&gt;『 Spark 』7. 使用 Spark DataFrame 进行大数据分析&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/spark-in-finance-and-investing&quot;&gt;『 Spark 』8. 实战案例 ｜ Spark 在金融领域的应用 ｜ 日内走势预测&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/ipython-notebook-spark&quot;&gt;『 Spark 』9. 搭建 IPython + Notebook + Spark 开发环境&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/boost-spark-application-performance&quot;&gt;『 Spark 』10. spark 应用程序性能优化｜12 个优化方法&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
   <entry>
     <title>『 读书笔记 』3月读书总结和推荐</title>
     <link href="/books-recommend-and-summarize-on-mar-2016"/>
     <updated>2016-03-26T00:00:00+08:00</updated>
     <id>/books-recommend-and-summarize-on-mar-2016</id>
     <content type="html">&lt;h2 id=&quot;section&quot;&gt;写在前面&lt;/h2&gt;

&lt;p&gt;计划是每月读 5-10 本书，书籍类型大概是三个方面的：金融，技术，管理。之所以选择这三个方面，一方面是因为自己对这三个方面都很有兴趣，其次是被 linkedin 创始人 Hoffman 的 &lt;a href=&quot;http://techcrunch.com/2012/02/14/in-startups-and-life-you-need-plan-a-b-and-z/&quot;&gt;ABZ 理论&lt;/a&gt; 深度影响。建议大家都看看 abz 理论那篇文章，如果我有空，也会整理一些常用的这类理论模型到博客里的。&lt;/p&gt;

&lt;p&gt;月底读书总结的形式都很简单，只是简单的一个列表和简单的书评，对觉得比较好的书会有单独的读书笔记。另外推荐大家用 excel 来做一些简单的工作管理，我现在就用 google docs 来做工作安排和读书计划，个人感觉比一些常用的神马协同软件强大太多了，简单，够用，就行了。工作中见过太多人把时间都花到使用那些协同软件上去，不得不说避重就轻了，适得其反，哈哈。&lt;/p&gt;

&lt;p&gt;下面是一张我用 google docs 来做读书安排的截图，不同颜色代表不同类别的数据，清晰明了实用。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/book-reading.jpg&quot; alt=&quot;book-reading.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;本月看了 7 本书，其中的电子书链接都放到亲爱的&lt;a href=&quot;http://pan.baidu.com/s/1pL26FZd&quot;&gt;度娘云&lt;/a&gt;里了，个人觉得不错的书都是纸板的，不知道有没有电子版的，推荐好书都看纸版的。&lt;/p&gt;

&lt;p&gt;ps: 我对好书的定义很简单：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;给自己有所启发的&lt;/li&gt;
  &lt;li&gt;高质量的，专业的教程类书籍&lt;/li&gt;
  &lt;li&gt;后期会再度回首的书&lt;/li&gt;
  &lt;li&gt;看完后会打算赠送给盆友看的书&lt;/li&gt;
  &lt;li&gt;留着给儿子看的书 [好吧，目前我只有个宝贝侄儿，哈哈]&lt;/li&gt;
  &lt;li&gt;最后一条，印刷质量要好&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;httppanbaiducoms1pl26fzd&quot;&gt;1. &lt;a href=&quot;http://pan.baidu.com/s/1pL26FZd&quot;&gt;中国顶尖技术团队访谈录 - 电子版&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;这是 infoq 出的一系列电子书中的一本，总共有 4 季访谈录，都是对一些公司技术领导人的访谈，虽然访谈都讲得很粗，但是在遇到相关问题时也可以参考参考别人是怎么处理的，比如说当你要搭建一个大型 docker 集群时，可以参考参考第二季访谈录中这篇 &lt;em&gt;腾讯罗韩梅 :万台规模的 Docker 应用实践&lt;/em&gt; ，虽然说肯定不能解决你的所有问题，但是你肯定知道在腾讯有这样一个牛人有这个经验啊，去 linkedin 什么的找找这个人，邮件或者微信或者通过其他方式请教人家也行啊，是吧，哈哈。&lt;/p&gt;

&lt;p&gt;总结：不要奢求能从这系列访谈里学到降龙十八掌，但是对于一个 tech leader 来说，看看这些书是应该的，&lt;em&gt;书中自有颜如玉，书外自有黄金屋&lt;/em&gt;。btw，第三季和第四季做得没前两季好，页数都少了很多，估计是 infoq 不想做这个访谈了吧，anyway。&lt;/p&gt;

&lt;p&gt;推荐指数：&lt;code class=&quot;highlighter-rouge&quot;&gt;* * * *&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;python-for-finance---httppanbaiducoms1pl26fzd&quot;&gt;2. &lt;a href=&quot;http://pan.baidu.com/s/1pL26FZd&quot;&gt;python for finance - 电子版&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;通读下来，这本书更应该叫 &lt;code class=&quot;highlighter-rouge&quot;&gt;python for finance - python tutoial and introduction to some basic financial theories&lt;/code&gt;，干货不多，大多数篇幅都去讲 &lt;code class=&quot;highlighter-rouge&quot;&gt;python&lt;/code&gt; 了，也讲了一些基础的金融理论，比如说蒙特卡罗模拟，期权定价原理什么的。如果你会 python，会用 pandas，懂一些基础的金融知识，可以不看这本书了。读下来对这本书没有什么大的感触，就不发表太多看法了。&lt;/p&gt;

&lt;p&gt;总结：如果你会 python，会用 pandas，懂一些基础的金融知识，可以不看这本书了；如果你不懂 python，不会 pandas，那也不推荐用这本书来学 python。&lt;/p&gt;

&lt;p&gt;推荐指数：&lt;code class=&quot;highlighter-rouge&quot;&gt;*&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;etf----httppanbaiducoms1pl26fzd&quot;&gt;3. &lt;a href=&quot;http://pan.baidu.com/s/1pL26FZd&quot;&gt;etf 投资，从入门到精通 - 电子版&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;之所以想看这本书，是一位大神觉得股票市场波动太大，去玩 etf 风险低，手续费也便宜，推荐我玩玩 etf。因为自己对 etf 一点不通，就买了这本书来看，上交所出版的，很专业，也讲得挺细致，对想玩，喜欢玩 etf 的人来说应该算是本好的手册。&lt;/p&gt;

&lt;p&gt;总结：etf 基础书籍里比较好的，对 etf 感兴趣的人可以看看哦。&lt;/p&gt;

&lt;p&gt;推荐指数：&lt;code class=&quot;highlighter-rouge&quot;&gt;* * * *&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;httpwwwamazoncne7bd97e8be91e6809de7bbb4-e69c89e7a78d-e69c89e8b6a3-e69c89e69699-e7bd97e68cafe5ae87dpb00fvha2f0refsr11ieutf8qid1459046888sr8-1keywordse980bbe8be91e6809de7bbb4&quot;&gt;4. &lt;a href=&quot;http://www.amazon.cn/%E7%BD%97%E8%BE%91%E6%80%9D%E7%BB%B4-%E6%9C%89%E7%A7%8D-%E6%9C%89%E8%B6%A3-%E6%9C%89%E6%96%99-%E7%BD%97%E6%8C%AF%E5%AE%87/dp/B00FVHA2F0/ref=sr_1_1?ie=UTF8&amp;amp;qid=1459046888&amp;amp;sr=8-1&amp;amp;keywords=%E9%80%BB%E8%BE%91%E6%80%9D%E7%BB%B4&quot;&gt;罗辑思维:有种、有趣、有料 - 纸版&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;罗辑思维出了几本书了，我看的是第一本，很有意思。不仅是观点上新颖独到，老罗还把网络上的一些评论也放到书里去了，甚至还放了一些&lt;code class=&quot;highlighter-rouge&quot;&gt;负面&lt;/code&gt;的评论，对读者来说这样很不错。读这本书，能让人在看待问题，处理问题时的思路更开阔，更宽容一些，学会从更多方面，更多角度去挖掘一个问题的根本原因。这本书还有个比较让我喜欢的地方，每章都会有一些推荐的书，其中不乏好书。经常听到人说很想看书，但是不知道看什么书，对此我的回答的 “随便挑本书来看，看着看着就知道该看什么书了”。还准备看看之后的版本，虽然同事说后面的版本没有前面的有意思了，不过打算先去书店翻看翻看，如果后续的版本不是换汤不换药，仍然满足上面我对好书的定义，那我也会毫不犹豫的买纸版来看的。&lt;/p&gt;

&lt;p&gt;总结：比较适合学生，职场人士读的书，尝试学会从更多的方面去待人待物。&lt;/p&gt;

&lt;p&gt;推荐指数：&lt;code class=&quot;highlighter-rouge&quot;&gt;* * * * *&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;httppanbaiducoms1pl26fzd-1&quot;&gt;5. &lt;a href=&quot;http://pan.baidu.com/s/1pL26FZd&quot;&gt;从0到1 - 电子版&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;这本书曾经很火，还记得当时公司群里时常都在讨论。虽然我看的是电子版的，不过我也觉得这本书值得买纸版的，如果能容忍那外强中干的印刷质量的话。这本书单独有总结帖的：&lt;/p&gt;

&lt;p&gt;总结：很适合工作 3 年以上的人看，特别是想创业，创业中的，在创业公司上班的人，以创业心态工作的人看。或者再宽泛一点，适合想把事情做好的人看。&lt;/p&gt;

&lt;p&gt;推荐指数：&lt;code class=&quot;highlighter-rouge&quot;&gt;* * * *&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;httpwwwamazoncne69cbae599a8e5ada6e4b9a0-e591a8e5bf97e58d8edpb01arkev1grefsr11ieutf8qid1459046918sr8-1keywordse69cbae599a8e5ada6e4b9a0&quot;&gt;6. &lt;a href=&quot;http://www.amazon.cn/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%91%A8%E5%BF%97%E5%8D%8E/dp/B01ARKEV1G/ref=sr_1_1?ie=UTF8&amp;amp;qid=1459046918&amp;amp;sr=8-1&amp;amp;keywords=%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0&quot;&gt;机器学习（周志华）- 纸版&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;啊哈，这本书怎么说呢？之所以买他全是因为同事朋友圈里的一篇转发，说这个似乎是泰斗级的教授写了一本机器学习的书，当时也看了下这个教授的介绍 &lt;em&gt;[&lt;/em&gt; 哈哈，对天朝的老师们没什么好感。按照我的理解，所谓 &lt;code class=&quot;highlighter-rouge&quot;&gt;师者，传道，授业，解惑也&lt;/code&gt;，不知道天朝有几个老师敢读了韩愈的这段话还敢自称师者的 &lt;em&gt;]&lt;/em&gt;，觉得还行，amazon 上的书评也还可以，就剁手买了下来。&lt;/p&gt;

&lt;p&gt;读下来，只能说还可以吧，just so so，但是这本书有种很浓烈的味道 －－ 书生味。也许是工作的原因，对这类有太多书生味的书没太大感觉。还是更喜欢实在一些的书，比如 Mitchell 的 《Machine Learning》, 图灵出版的《Machine Learning in Action》，或者细分下来的《推荐系统实战》这类书。&lt;/p&gt;

&lt;p&gt;总结：书生味太浓，内容倒是也不差。&lt;/p&gt;

&lt;p&gt;推荐指数：&lt;code class=&quot;highlighter-rouge&quot;&gt;* * *&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;httpwwwamazoncne4bcb0e580bce79a84e889bae69caf-110e4b8aae8a7a3e8afbbe6a188e4be8b-e5b0bce58fa4e68b89e696afc2b7e696afe5af86e5beb7e69e97dpb014d1mc5wrefsr11ieutf8qid1459046933sr8-1keywordse4bcb0e580bce79a84e889bae69caf&quot;&gt;7. &lt;a href=&quot;http://www.amazon.cn/%E4%BC%B0%E5%80%BC%E7%9A%84%E8%89%BA%E6%9C%AF-110%E4%B8%AA%E8%A7%A3%E8%AF%BB%E6%A1%88%E4%BE%8B-%E5%B0%BC%E5%8F%A4%E6%8B%89%E6%96%AF%C2%B7%E6%96%AF%E5%AF%86%E5%BE%B7%E6%9E%97/dp/B014D1MC5W/ref=sr_1_1?ie=UTF8&amp;amp;qid=1459046933&amp;amp;sr=8-1&amp;amp;keywords=%E4%BC%B0%E5%80%BC%E7%9A%84%E8%89%BA%E6%9C%AF&quot;&gt;估值的艺术:110个解读案例 - 纸版&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;和这本书的第一次相遇是在陆家嘴正大广场的书店里看到的，当时我很想找一本公司基本面的书来看看，准备在自己的投资模型里多加一些公司基本面的因子。当时第一次看到这本书，翻看了十来分钟，知道这就是我想要的，简单，够用，还有翔实的例子，比《公司财务原理》这类书要来得痛快干脆，btw，我并不是说《公司财务原理》这本书不好，我也在看这本书的，只是《估值的艺术》这本书更适合当时的需求。而且，从小的方面来说，这本书能教你一些公司基本面的东西，对投资有所帮助；从大到方面来说，这本书教你怎么挖掘一个潜力公司，或者教你怎么管理自己的公司，或者说教你当你有了自己的公司的时候，应该从哪些方面实时查看自己公司的发展情况。很有价值。&lt;/p&gt;

&lt;p&gt;总结：如果你做股票投资，这本书值得一看；如果你有自己的公司，或者以后想要有自己的公司，那这本书更值得反复品读。&lt;/p&gt;

&lt;p&gt;推荐指数：&lt;code class=&quot;highlighter-rouge&quot;&gt;* * * * *&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;master-apache-sparkhttpswwwgitbookcombookjaceklaskowskimastering-apache-sparkdetails&quot;&gt;8. &lt;a href=&quot;https://www.gitbook.com/book/jaceklaskowski/mastering-apache-spark/details&quot;&gt;Master Apache Spark&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;这本书是一个国外咨询师写的开源书籍，居然还有一本纸质版的 &lt;a href=&quot;http://shop.oreilly.com/product/9781783987146.do&quot;&gt;Master Apache Spark&lt;/a&gt;，不过和这本开源书籍应该没什么关系。之所以想先看这本书是因为 spark 更新得很快，作者应该会及时更新相关内容到最新的 spark 版本。看下来感觉还行，都是作者根据相关文档，相关书籍，以及自己的理解和实践来写的。但是里面还是有一些问题，也有的地方没有写。不推荐作为第一本学习spark的书籍，可以在有一定经验后翻翻看。下月还是准备看 Matei 合写的 &lt;a href=&quot;http://shop.oreilly.com/product/0636920028512.do&quot;&gt;Learning Spark&lt;/a&gt;，虽然出版时间很早，但是毕竟是 spark 的作者参与的，内容应该更清晰，深入，等待下个月我的读书笔记吧。&lt;/p&gt;

&lt;p&gt;总结：内容还行，作者更新也挺频繁的，但是不推荐作为第一本学习 spark 的书，有一定经验后可以看看。&lt;/p&gt;

&lt;p&gt;推荐指数：&lt;code class=&quot;highlighter-rouge&quot;&gt;* * * *&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;参考文章&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://techcrunch.com/2012/02/14/in-startups-and-life-you-need-plan-a-b-and-z/&quot;&gt;In Startups And Life, You Need Plan A, B, And Z&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
   <entry>
     <title>『 Spark 』6. 深入研究 spark 运行原理之 job, stage, task</title>
     <link href="/deep-into-spark-exection-model"/>
     <updated>2016-03-18T00:00:00+08:00</updated>
     <id>/deep-into-spark-exection-model</id>
     <content type="html">&lt;h2 id=&quot;section&quot;&gt;写在前面&lt;/h2&gt;

&lt;p&gt;本系列是综合了自己在学习spark过程中的理解记录 ＋ 对参考文章中的一些理解 ＋ 个人实践spark过程中的一些心得而来。写这样一个系列仅仅是为了梳理个人学习spark的笔记记录，所以一切以能够理解为主，没有必要的细节就不会记录了，而且文中有时候会出现英文原版文档，只要不影响理解，都不翻译了。若想深入了解，最好阅读参考文章和官方文档。&lt;/p&gt;

&lt;p&gt;其次，本系列是基于目前最新的 spark 1.6.0 系列开始的，spark 目前的更新速度很快，记录一下版本好还是必要的。 &lt;br /&gt;
最后，如果各位觉得内容有误，欢迎留言备注，所有留言 24 小时内必定回复，非常感谢。   &lt;br /&gt;
Tips: 如果插图看起来不明显，可以：1. 放大网页；2. 新标签中打开图片，查看原图哦。&lt;/p&gt;

&lt;h2 id=&quot;spark-&quot;&gt;1. spark 运行原理&lt;/h2&gt;

&lt;p&gt;这一节是本文的核心，我们可以先抛出一个问题，如果看完这一节，或者这一章之后，你能理解你的整个 spark 应用的执行流程，那就可以关掉这个网页了［对了，关掉网页之前记得分享一下哦，哈哈］&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Problem: How does user program get translated into units of physical execution ?&lt;/code&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;我们用一个例子来说明，结合例子和运行截图来理解。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;1.1 例子，美国 1880 － 2014 年新生婴儿数据统计&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;目标&lt;/code&gt;：用美国 1880 － 2014 年新生婴儿的数据来做做简单的统计&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;数据源&lt;/code&gt;：&lt;a href=&quot;https://catalog.data.gov/dataset/baby-names-from-social-security-card-applications-national-level-data&quot;&gt; https://catalog.data.gov&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;数据格式&lt;/code&gt;：
    &lt;ul&gt;
      &lt;li&gt;每年的新生婴儿数据在一个文件里面&lt;/li&gt;
      &lt;li&gt;每个文件的每一条数据格式：&lt;code class=&quot;highlighter-rouge&quot;&gt;姓名,性别,新生人数&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../images/baby-data-format.jpg&quot; alt=&quot;baby-data-format.jpg&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;代码和结果展示&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;### packages&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pd&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;### spark UDF (User Defined Functions)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;map_extract&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;element&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;file_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;content&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;element&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;year&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;file_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;year&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;content&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\r\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;### spark logic&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;res&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;wholeTextFiles&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;hdfs://10.21.208.21:8020/user/mercury/names&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                        &lt;span class=&quot;n&quot;&gt;minPartitions&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;40&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  \
        &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;map_extract&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
        &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flatMap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
        &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;,&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])))&lt;/span&gt; \
        &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reduceByKey&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;operator&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
        &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;collect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;### result displaying&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_records&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;year&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;birth&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;\
         &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sort&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;year&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ascending&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;year&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;birth&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; 
                &lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; 
                &lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;US Baby Birth Data from 1897 to 2014&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                &lt;span class=&quot;n&quot;&gt;linewidth&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_axis_bgcolor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;white&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;gray&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;y&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;../images/baby-name-1.jpg&quot; alt=&quot;baby-name-1.jpg&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;1.2 运行流程概览&lt;/h2&gt;

&lt;p&gt;还记得我们在  &lt;a href=&quot;../spark-programming-model&quot;&gt;『 Spark 』3. spark 编程模式 &lt;/a&gt; 讲到的构建一个 spark application 的过程吗：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;加载数据集&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;处理数据&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;结果展示&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;上面的 22 行代码，就已经把构建一个 spark app 的三大步骤完成了，amazing, right? 今天我们主要讲 spark 的运行逻辑，所以我们就以核心的 11 － 16 ，这六行代码来作为今天的主线，了解了解 spark 的原理。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/baby-name-2.jpg&quot; alt=&quot;baby-name-2.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;可以看到，整个逻辑实际上就用了 sparkContext 的一个函数，rdd 的 3 个 transformation 和 1 个 action。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/baby-name-job.jpg&quot; alt=&quot;baby-name-job.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;现在让我们从 WEB UI 上来看看，当我们运行这段代码的时候，后台都发生了什么。
可以看到，执行这段代码的时候，spark 通过分析，优化代码，知道这段代码需要一个 job 来完成，所以 web ui 上只有一个 job。值得深究的是，这个 job 由两个 stage 完成，这两个 state 一共有 66 个 task。&lt;/p&gt;

&lt;p&gt;所以，这里我们就再次理解下 spark 里，job，stage，task 的概念：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;job&lt;/em&gt; : A job is triggered by an action, like count() or saveAsTextFile(). Click on a job to see information about the stages of tasks inside it. 理解了吗，所谓一个 job，就是由一个 rdd 的 action 触发的动作，可以简单的理解为，当你需要执行一个 rdd 的 action 的时候，会生成一个 job。&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;stage&lt;/em&gt; : stage 是一个 job 的组成单位，就是说，一个 job 会被切分成 1 个或 1 个以上的 stage，然后各个 stage 会按照执行顺序依次执行。至于 job 根据什么标准来切分 stage，可以回顾第二篇博文：&lt;a href=&quot;../spark-questions-concepts&quot;&gt;『 Spark 』2. spark 基本概念解析 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;task&lt;/em&gt; : A unit of work within a stage, corresponding to one RDD partition。即 stage 下的一个任务执行单元，一般来说，一个 rdd 有多少个 partition，就会有多少个 task，因为每一个 task 只是处理一个 partition 上的数据。从 web ui 截图上我们可以看到，这个 job 一共有 2 个 stage，66 个 task，平均下来每个 stage 有 33 个 task，相当于每个 stage 的数据都有 33 个 partition [注意：这里是平均下来的哦，并不都是每个 stage 有 33 个 task，有时候也会有一个 stage 多，另外一个 stage 少的情况，就看你有没有在不同的 stage 进行 repartition 类似的操作了。]&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../images/baby-name-ui-1.jpg&quot; alt=&quot;baby-name-ui-1.jpg&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;job&quot;&gt;1.3 运行流程之 : job&lt;/h2&gt;

&lt;p&gt;根据上面的截图和再次重温，我们知道这个 spark 应用里只有一个 job，那就是因为我们执行了一个 &lt;code class=&quot;highlighter-rouge&quot;&gt;collect&lt;/code&gt; 操作，即把处理后的数据全部返回到我们的 driver 上，进行后续的画图，返回的数据如下图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/baby-name-3.jpg&quot; alt=&quot;baby-name-3.jpg&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;stage&quot;&gt;1.4 运行流程之 : stage&lt;/h2&gt;

&lt;p&gt;我们这个 spark 应用，生成了一个 job，这个 job 由 2 个 stage 组成，并且每个 stage 都有 33 个task，说明每个 stage 的数据都在 33 个 partition 上，这下我们就来看看，这两个 stage 的情况。&lt;/p&gt;

&lt;p&gt;首先，我们先看看为什么这里会有两个 stage，根据 &lt;a href=&quot;../spark-questions-concepts&quot;&gt;『 Spark 』2. spark 基本概念解析 &lt;/a&gt; 中对 stage 的描述，目前有两个划分 stage 的标准：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;当触发 rdd 的 action 时 : 在我们的应用中就是最后的 &lt;code class=&quot;highlighter-rouge&quot;&gt;collect&lt;/code&gt; 操作，关于这个操作的说明，可以看官方文档: &lt;a href=&quot;https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.collect&quot;&gt;rdd.collect&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;当触发 rdd 的 shuffle 操作时 : 在我们的应用中就是 &lt;code class=&quot;highlighter-rouge&quot;&gt;reduceByKey&lt;/code&gt; 这个操作，官方文档: &lt;a href=&quot;https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduceByKey&quot;&gt;rdd.reduceByKey&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../images/baby-name-4.jpg&quot; alt=&quot;baby-name-4.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;再次回顾上面那张图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/baby-name-job.jpg&quot; alt=&quot;baby-name-job.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这下应该就明了了，关于两个 stage 的情况：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/baby-name-5.jpg&quot; alt=&quot;baby-name-5.jpg&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;第一个 stage，即截图中 stage id 为 0 的 stage，其执行了 &lt;code class=&quot;highlighter-rouge&quot;&gt;sc.wholeTextFiles().map().flatMap().map().reduceByKey()&lt;/code&gt; 这几个步骤，因为这是一个 &lt;code class=&quot;highlighter-rouge&quot;&gt;Shuffle&lt;/code&gt; 操作，所以后面会有 &lt;code class=&quot;highlighter-rouge&quot;&gt;Shuffle Read&lt;/code&gt; 和 &lt;code class=&quot;highlighter-rouge&quot;&gt;Shuffle Write&lt;/code&gt;。具体来说，就是在 stage 0 这个 stage 中，发生了一个 Shuffle 操作，这个操作读入 22.5 MB 的数据，生成 41.7 KB 的数据，并把生成的数据写在了硬盘上。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;第二个 stage，即截图中 stage id 为 1 到 stage，其执行了 &lt;code class=&quot;highlighter-rouge&quot;&gt;collect()&lt;/code&gt; 这个操作，因为这是一个 &lt;code class=&quot;highlighter-rouge&quot;&gt;action&lt;/code&gt; 操作，并且它上一步是一个 Shuffle 操作，且没有后续操作，所以这里 &lt;code class=&quot;highlighter-rouge&quot;&gt;collect()&lt;/code&gt; 这个操作被独立成一个 stage 了。这里它把上一个 Shuffle 写下的数据读取进来，然后一起返回到 driver 端，所以这里可以看到他的 &lt;code class=&quot;highlighter-rouge&quot;&gt;Shuffle Read&lt;/code&gt; 这里刚好读取了上一个 stage 写下的数据。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;task&quot;&gt;1.5 运行流程之 : task&lt;/h2&gt;

&lt;p&gt;其实到这里应该都理解得差不多了，至于为什么每个 stage 会有 33 个 task [即我们的数据文件存放到 33 个partition 上，可是明明 &lt;code class=&quot;highlighter-rouge&quot;&gt;sc.wholeTextFiles(&#39;hdfs://10.21.208.21:8020/user/mercury/names&#39;, minPartitions=40)&lt;/code&gt; 这里指定了最小要 40 个partition 到啊]，这个问题我们留到以后说，在后面我们会有一篇讲怎么调试，优化 spark app 的博文，到时候我们会继续回到这里，解答这里的问题。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/baby-name-7.jpg&quot; alt=&quot;baby-name-7.jpg&quot; /&gt;
&lt;img src=&quot;../images/baby-name-8.jpg&quot; alt=&quot;baby-name-8.jpg&quot; /&gt;
&lt;img src=&quot;../images/baby-name-9.jpg&quot; alt=&quot;baby-name-9.jpg&quot; /&gt;
&lt;img src=&quot;../images/baby-name-10.jpg&quot; alt=&quot;baby-name-10.jpg&quot; /&gt;
&lt;img src=&quot;../images/baby-name-11.jpg&quot; alt=&quot;baby-name-11.jpg&quot; /&gt;
&lt;img src=&quot;../images/baby-name-12.jpg&quot; alt=&quot;baby-name-12.jpg&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;next&quot;&gt;2. Next&lt;/h2&gt;

&lt;p&gt;既然我们都慢慢开始深入理解 spark 的执行原理了，那下次我们就来说说 spark 的一些配置吧，然后再说说 spark 应用的优化。&lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;7. 打开微信，扫一扫，点一点，棒棒的，^_^&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;../images/wechat_pay_6-6.png&quot; alt=&quot;wechat_pay_6-6.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-4&quot;&gt;参考文章&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.slideshare.net/pwendell/tuning-and-debugging-in-apache-spark&quot;&gt;Tuning and Debugging in Apache Spark&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.amazon.com/Learning-Spark-Lightning-Fast-Data-Analysis/dp/1449358624/ref=sr_1_1?ie=UTF8&amp;amp;qid=1458293667&amp;amp;sr=8-1&amp;amp;keywords=learning+spark&quot;&gt;learning spark&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://aiyanbo.gitbooks.io/spark-programming-guide-zh-cn/content/more/spark-configuration.html&quot;&gt;Spark配置&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://colobu.com/2014/12/10/spark-configuration/&quot;&gt;Spark 配置指南&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-5&quot;&gt;本系列文章链接&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/introduction-to-spark&quot;&gt;『 Spark 』1. spark 简介 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/spark-questions-concepts&quot;&gt;『 Spark 』2. spark 基本概念解析 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/spark-programming-model&quot;&gt;『 Spark 』3. spark 编程模式 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/spark-what-is-rdd&quot;&gt;『 Spark 』4. spark 之 RDD &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/spark-resouces-blogs-paper&quot;&gt;『 Spark 』5. 这些年，你不能错过的 spark 学习资源 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/deep-into-spark-exection-model&quot;&gt;『 Spark 』6. 深入研究 spark 运行原理之 job, stage, task&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/spark-dataframe-introduction&quot;&gt;『 Spark 』7. 使用 Spark DataFrame 进行大数据分析&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/spark-in-finance-and-investing&quot;&gt;『 Spark 』8. 实战案例 ｜ Spark 在金融领域的应用 ｜ 日内走势预测&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/ipython-notebook-spark&quot;&gt;『 Spark 』9. 搭建 IPython + Notebook + Spark 开发环境&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/boost-spark-application-performance&quot;&gt;『 Spark 』10. spark 应用程序性能优化｜12 个优化方法&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
   <entry>
     <title>『 Spark 』5. 这些年，你不能错过的 spark 学习资源</title>
     <link href="/spark-resouces-blogs-paper"/>
     <updated>2016-03-10T00:00:00+08:00</updated>
     <id>/spark-resouces-blogs-paper</id>
     <content type="html">&lt;h2 id=&quot;section&quot;&gt;写在前面&lt;/h2&gt;

&lt;p&gt;本系列是综合了自己在学习spark过程中的理解记录 ＋ 对参考文章中的一些理解 ＋ 个人实践spark过程中的一些心得而来。写这样一个系列仅仅是为了梳理个人学习spark的笔记记录，所以一切以能够理解为主，没有必要的细节就不会记录了，而且文中有时候会出现英文原版文档，只要不影响理解，都不翻译了。若想深入了解，最好阅读参考文章和官方文档。&lt;/p&gt;

&lt;p&gt;其次，本系列是基于目前最新的 spark 1.6.0 系列开始的，spark 目前的更新速度很快，记录一下版本好还是必要的。 &lt;br /&gt;
最后，如果各位觉得内容有误，欢迎留言备注，所有留言 24 小时内必定回复，非常感谢。   &lt;br /&gt;
Tips: 如果插图看起来不明显，可以：1. 放大网页；2. 新标签中打开图片，查看原图哦。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;1. 书籍&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.amazon.cn/Spark%E5%BF%AB%E9%80%9F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E7%BE%8E-%E5%8D%A1%E5%8A%B3-%E7%BE%8E-%E8%82%AF%E7%BB%B4%E5%B0%BC%E6%96%AF%E7%A7%91-%E7%BE%8E-%E6%B8%A9%E5%BE%B7%E5%B0%94-%E5%8A%A0-%E6%89%8E%E5%93%88%E9%87%8C%E4%BA%9A/dp/B016DWSEXI/ref=sr_1_1?ie=UTF8&amp;amp;qid=1460447269&amp;amp;sr=8-1&amp;amp;keywords=spark+%E5%BF%AB%E9%80%9F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90&quot;&gt;Learning Spark&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.gitbook.com/book/jaceklaskowski/mastering-apache-spark/details&quot;&gt;Mastering Apache Spark&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-2&quot;&gt;2. 网站&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://spark.apache.org/&quot;&gt;official site&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://apache-spark-user-list.1001560.n3.nabble.com/&quot;&gt;user mailing list&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/user/TheApacheSpark&quot;&gt;spark channel on youtube&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://spark-summit.org/&quot;&gt;spark summit&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.meetup.com/&quot;&gt;meetup&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://spark-packages.org/&quot;&gt;spark third party packages&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://databricks.com/blog&quot;&gt;databricks blog&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.cloud.databricks.com/docs/latest/sample_applications/index.html#Introduction%20(Readme).html&quot;&gt;databricks docs&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.cloud.databricks.com/docs/latest/courses/index.html#Introduction%20to%20Big%20Data%20with%20Apache%20Spark%20(CS100-1x)/Introduction%20(README).html&quot;&gt;databricks training&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.cloudera.com/blog/category/spark/&quot;&gt;cloudera blog about spark&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://0x0fff.com&quot;&gt;https://0x0fff.com&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://techsuppdiva.github.io/&quot;&gt;http://techsuppdiva.github.io/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://lib.csdn.net/base/10&quot;&gt;csdn spark 知识库&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.iteblog.com/archives/category/spark&quot;&gt;过往记忆&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-3&quot;&gt;3. 文章，博客&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf&quot;&gt;RDD论文英文版&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://code.csdn.net/CODE_Translation/spark_matei_phd&quot;&gt;RDD论文中文版&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-12.pdf&quot;&gt;An Architecture for Fast and General Data Processing
on Large Clusters&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-1/&quot;&gt;How-to: Tune Your Apache Spark Jobs (Part 1)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-2/&quot;&gt;How-to: Tune Your Apache Spark Jobs (Part 2)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://dataunion.org/22985.html&quot;&gt;借助 Redis ，让 Spark 提速 45 倍！&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.csdn.net/article/2015-10-06/2825849&quot;&gt;量化派基于Hadoop、Spark、Storm的大数据风控架构&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://geek.csdn.net/news/detail/58867&quot;&gt;基于Spark的异构分布式深度学习平台&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.36dsj.com/archives/40723&quot;&gt;你对Hadoop和Spark生态圈了解有几许？&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.yuntoutiao.com/dongtai/5389.html&quot;&gt;Hadoop vs Spark&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://geek.csdn.net/news/detail/57656&quot;&gt;雅虎开源CaffeOnSpark：基于Hadoop/Spark的分布式深度学习&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/files/1. spark_meetup.pdf&quot;&gt;2016 上海第二次 spark meetup: 1. spark_meetup.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/files/2. Flink_ An unified stream engine.pdf&quot;&gt;2016 上海第二次 spark meetup: 2. Flink_ An unified stream engine.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/files/3. Spark在计算广告领域的应用实践.pdf&quot;&gt;2016 上海第二次 spark meetup: 3. Spark在计算广告领域的应用实践.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/files/4. splunk_spark.pdf&quot;&gt;2016 上海第二次 spark meetup: 4. splunk_spark.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://prezi.com/w4wjzdq7y0lj/spark/&quot;&gt;基于Spark的医疗和金融大数据&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.hammerlab.org/2015/02/27/monitoring-spark-with-graphite-and-grafana/&quot;&gt;Monitoring Spark with Graphite and Grafana&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-4&quot;&gt;4. 视频&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=cs3_3LdCny8&quot;&gt;YouTube: what is apache spark&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=65aV15uDKgA&quot;&gt;Introduction to Spark Architecture&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=WyfHUNnMutg&quot;&gt;Top 5 Mistakes When Writing Spark Applications&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.slideshare.net/hadooparchbook/top-5-mistakes-when-writing-spark-applications&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;slide&lt;/code&gt; Top 5 mistakes when writing Spark applications&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=kkOG_aJ9KjQ&quot;&gt;Tuning and Debugging Apache Spark&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.slideshare.net/pwendell/tuning-and-debugging-in-apache-spark&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;slide&lt;/code&gt; Tuning and Debugging Apache Spark&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=dmL0N3qfSc8&quot;&gt;A Deeper Understanding of Spark Internals - Aaron Davidson (Databricks)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://spark-summit.org/2014/wp-content/uploads/2014/07/A-Deeper-Understanding-of-Spark-Internals-Aaron-Davidson.pdf&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;slide&lt;/code&gt; A Deeper Understanding of Spark Internals - Aaron Davidson (Databricks)&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=OednhGRp938&quot;&gt;Building, Debugging, and Tuning Spark Machine Learning Pipelines - Joseph Bradley (Databricks)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.slideshare.net/SparkSummit/building-debugging-and-tuning-spark-machine-leaning-pipelinesjoseph-bradley&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;slide&lt;/code&gt; Building, Debugging, and Tuning Spark Machine Learning Pipelines&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=xWkJCUcD55w&quot;&gt;Spark DataFrames Simple and Fast Analysis of Structured Data - Michael Armbrust (Databricks)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.slideshare.net/databricks/spark-dataframes-simple-and-fast-analytics-on-structured-data-at-spark-summit-2015&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;slide&lt;/code&gt; Spark DataFrames Simple and Fast Analysis of Structured Data - Michael Armbrust (Databricks)&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=HBZuB3pPri0&amp;amp;feature=youtu.be&quot;&gt;Spark Tuning for Enterprise System Administrators&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.slideshare.net/AnyaBida/bida-sse2016final-58237248&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;slide&lt;/code&gt; Spark Tuning for Enterprise System Administrators&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=i7l3JQRx7Qw&quot;&gt;Structuring Spark: DataFrames, Datasets, and Streaming&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.slideshare.net/databricks/structuring-spark-dataframes-datasets-and-streaming&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;slide&lt;/code&gt; Structuring Spark: DataFrames, Datasets, and Streaming&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=GzG9RTRTFck&quot;&gt;Spark in Production: Lessons from 100+ Production Users&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.slideshare.net/databricks/spark-summit-eu-2015-lessons-from-300-production-users&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;slide&lt;/code&gt; Spark in Production: Lessons from 100+ Production Users&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=rBrsxM091KA&quot;&gt;Production Spark and Tachyon use Cases&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.slideshare.net/SparkSummit/using-spark-with-tachyon-by-gene-pang&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;slide&lt;/code&gt; Production Spark and Tachyon use Cases&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=VQOKk9jJGcw&amp;amp;index=5&amp;amp;list=PL-x35fyliRwif48cPXQ1nFM85_7e200Jp&quot;&gt;SparkUI Visualization&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.slideshare.net/databricks/spark-summit-eu-2015-sparkui-visualization-a-lens-into-your-application&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;slide&lt;/code&gt; SparkUI Visualization&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=Wg2boMqLjCg&quot;&gt;
Everyday I’m Shuffling - Tips for Writing Better Spark Programs, Strata San Jose 2015&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.slideshare.net/databricks/strata-sj-everyday-im-shuffling-tips-for-writing-better-spark-programs&quot;&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;slide&lt;/code&gt; Everyday I’m Shuffling - Tips for Writing Better Spark Programs, Strata San Jose 2015&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=FA3ArTyXNoo&quot;&gt;Large Scale Distributed Machine Learning on Apache Spark&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=Aups6UcGiQQ&amp;amp;list=PL-x35fyliRwif48cPXQ1nFM85_7e200Jp&amp;amp;index=1&quot;&gt;Securing your Spark Applications&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.slideshare.net/cloudera/securing-your-apache-spark-applications&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;slide&lt;/code&gt; Securing your Spark Applications&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=AHYq91i-ohI&quot;&gt;Building a REST Job Server for Interactive Spark as a Service&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.slideshare.net/SparkSummit/building-a-rest-job-server-for-interactive-spark-as-a-service-by-romain-rigaux-and-erick-tryzelaar&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;slide&lt;/code&gt; Building a REST Job Server for Interactive Spark as a Service&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=PPQRi484bNo&amp;amp;list=PL-x35fyliRwif48cPXQ1nFM85_7e200Jp&amp;amp;index=2&quot;&gt;Exploiting GPUs for Columnar DataFrame Operations&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.slideshare.net/SparkSummit/exploiting-gpus-for-columnar-datafrrames-by-kiran-lonikar&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;slide&lt;/code&gt; Exploiting GPUs for Columnar DataFrame Operations&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=MFSUAkDBSdQ&quot;&gt;Easy JSON Data Manipulation in Spark - Yin Huai (Databricks)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://spark-summit.org/2014/wp-content/uploads/2014/07/Easy-json-Data-Manipulation-Yin-Huai.pdf&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;slide&lt;/code&gt; Easy JSON Data Manipulation in Spark - Yin Huai (Databricks)&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=8hn2KVC8FvA&amp;amp;index=6&amp;amp;list=PL-x35fyliRwiuc6qy9z2erka2VX8LY53x&quot;&gt;Sparkling: Speculative Partition of Data for Spark Applications - Peilong Li&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://spark-summit.org/2014/wp-content/uploads/2014/07/Sparkling-Indentification-of-Task-Skew-and-Speculative-Partition-of-Data-for-Spark-Applications-Peilong-Li.pdf&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;slide&lt;/code&gt; Sparkling: Speculative Partition of Data for Spark Applications - Peilong Li&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=HG2Yd-3r4-M&amp;amp;list=PLTPXxbhUt-YWGNTaDj6HSjnHMxiTD1HCR&amp;amp;index=1&quot;&gt;Advanced Spark Internals and Tuning – Reynold Xin&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://databricks-training.s3.amazonaws.com/slides/advanced-spark-training.pdf&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;slide&lt;/code&gt; Advanced Spark Internals and Tuning – Reynold Xin&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=oXkxXDG0gNk&quot;&gt;The Future of Real Time in Spark&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.slideshare.net/databricks/the-future-of-realtime-in-spark-58433411&quot;&gt;The Future of Real Time in Spark&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=ZFBgY0PwUeY&amp;amp;feature=youtu.be&quot;&gt;Spark 2 0&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.slideshare.net/databricks/2016-spark-summit-east-keynote-matei-zaharia&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;slide&lt;/code&gt; Spark 2 0&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=BPotQuqFnyw&amp;amp;feature=youtu.be&quot;&gt;Democratizing Access to Data&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.slideshare.net/databricks/2016-spark-summit-east-keynote-ali-ghodsi-and-databricks-community-edition-demo&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;slide&lt;/code&gt; Democratizing Access to Data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;next&quot;&gt;5. next&lt;/h2&gt;

&lt;p&gt;上面的资源我都会不断更新的，里面 80% 以上的都是我亲自看过并且觉得有价值的，可不是胡乱收集一通的，推荐欣赏哦。&lt;/p&gt;

&lt;h2 id=&quot;section-5&quot;&gt;6. 打开微信，扫一扫，点一点，棒棒的，^_^&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;../images/wechat_pay.png&quot; alt=&quot;wechat_pay.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-6&quot;&gt;本系列文章链接&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/introduction-to-spark&quot;&gt;『 Spark 』1. spark 简介 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/spark-questions-concepts&quot;&gt;『 Spark 』2. spark 基本概念解析 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/spark-programming-model&quot;&gt;『 Spark 』3. spark 编程模式 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/spark-what-is-rdd&quot;&gt;『 Spark 』4. spark 之 RDD &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/spark-resouces-blogs-paper&quot;&gt;『 Spark 』5. 这些年，你不能错过的 spark 学习资源 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/deep-into-spark-exection-model&quot;&gt;『 Spark 』6. 深入研究 spark 运行原理之 job, stage, task&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/spark-dataframe-introduction&quot;&gt;『 Spark 』7. 使用 Spark DataFrame 进行大数据分析&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/spark-in-finance-and-investing&quot;&gt;『 Spark 』8. 实战案例 ｜ Spark 在金融领域的应用 ｜ 日内走势预测&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/ipython-notebook-spark&quot;&gt;『 Spark 』9. 搭建 IPython + Notebook + Spark 开发环境&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/boost-spark-application-performance&quot;&gt;『 Spark 』10. spark 应用程序性能优化｜12 个优化方法&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
   <entry>
     <title>『 Spark 』4. spark 之 RDD</title>
     <link href="/spark-what-is-rdd"/>
     <updated>2016-03-08T00:00:00+08:00</updated>
     <id>/spark-what-is-rdd</id>
     <content type="html">&lt;h2 id=&quot;section&quot;&gt;写在前面&lt;/h2&gt;

&lt;p&gt;本系列是综合了自己在学习spark过程中的理解记录 ＋ 对参考文章中的一些理解 ＋ 个人实践spark过程中的一些心得而来。写这样一个系列仅仅是为了梳理个人学习spark的笔记记录，所以一切以能够理解为主，没有必要的细节就不会记录了，而且文中有时候会出现英文原版文档，只要不影响理解，都不翻译了。若想深入了解，最好阅读参考文章和官方文档。&lt;/p&gt;

&lt;p&gt;其次，本系列是基于目前最新的 spark 1.6.0 系列开始的，spark 目前的更新速度很快，记录一下版本好还是必要的。 &lt;br /&gt;
最后，如果各位觉得内容有误，欢迎留言备注，所有留言 24 小时内必定回复，非常感谢。   &lt;br /&gt;
Tips: 如果插图看起来不明显，可以：1. 放大网页；2. 新标签中打开图片，查看原图哦。&lt;/p&gt;

&lt;h2 id=&quot;rdd&quot;&gt;1. 什么是RDD&lt;/h2&gt;
&lt;p&gt;先看下源码里是怎么描述RDD的。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;

    &lt;p&gt;Internally, each RDD is characterized by five main properties:&lt;br /&gt;
A list of partitions&lt;br /&gt;
A function for computing each split &lt;br /&gt;
A list of dependencies on other RDDs&lt;br /&gt;
Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned) &lt;br /&gt;
Optionally, a list of preferred locations to compute each split on (e.g. block locations for an HDFS file)&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;p&gt;可以知道，每个 RDD 有以下5个主要的属性：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;

    &lt;ul&gt;
      &lt;li&gt;一组分片（partition），即数据集的基本组成单位&lt;/li&gt;
      &lt;li&gt;一个计算每个分片的函数&lt;/li&gt;
      &lt;li&gt;对parent RDD的依赖，这个依赖描述了RDD之间的 &lt;code class=&quot;highlighter-rouge&quot;&gt;lineage&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;对于key-value的RDD，一个Partitioner，这是可选择的&lt;/li&gt;
      &lt;li&gt;一个列表，存储存取每个partition的preferred位置。对于一个HDFS文件来说，存储每个partition所在的块的位置。这也是可选择的&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;p&gt;把上面这5个主要的属性总结一下，可以得出RDD的大致概念：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;

    &lt;p&gt;首先，RDD 大概是这样一种表示数据集的东西，它具有以上列出的一些属性。是设计用来表示数据集的一种数据结构。为了让 RDD 能 handle 更多的问题，规定 RDD 应该是只读的，分区记录的一种数据集合。可以通过两种方式来创建 RDD：一种是基于物理存储中的数据，比如说磁盘上的文件；另一种，也是大多数创建 RDD 的方式，即通过其他 RDD 来创建【以后叫做转换】而成。而正因为 RDD 满足了这么多特性，所以 spark 把 RDD 叫做 &lt;code class=&quot;highlighter-rouge&quot;&gt;Resilient Distributed Datasets&lt;/code&gt;，中文叫做弹性分布式数据集。很多文章都是先讲 RDD 的定义，概念，再来说 RDD 的特性。我觉得其实也可以倒过来，通过 RDD 的特性反过来理解 RDD 的定义和概念，通过这种由果溯因的方式来理解 RDD 也未尝不可，至少对我个人而言这种方式是挺好的。&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;rdd-1&quot;&gt;2. 理解RDD的几个关键概念&lt;/h2&gt;

&lt;p&gt;本来我是想参考 RDD 的论文和自己的理解来整理这篇文章的，可是后来想想这样是不是有点过于细致了。我想，认识一个新事物，在时间、资源有限的情况下，不必锱铢必较，可以先 focus on 几个关键点，到后期应用的时候再步步深入。&lt;/p&gt;

&lt;p&gt;所以，按照我个人的理解，我认为想用好 spark，必须要理解 RDD ，而为了理解 RDD ，我认为只要了解下面几个 RDD 的几个关键点就能 handle 很多情况下的问题了。所以，下面所有列到的点，都是在我个人看来很重要的，但也许有所欠缺，大家如果想继续深入，可以看第三部分列出的参考资料，或者直接联系我，互相交流。&lt;/p&gt;

&lt;h3 id=&quot;rdd-2&quot;&gt;2.1 RDD的背景及解决的痛点问题&lt;/h3&gt;

&lt;p&gt;RDD 的设计是为了充分利用分布式系统中的内存资源，使得提升一些特定的应用的效率。这里所谓的特定的应用没有明确定义，但可以理解为一类应用到迭代算法，图算法等需要重复利用数据的应用类型；除此之外，RDD 还可以应用在交互式大数据处理方面。所以，我们这里需要明确一下：&lt;code class=&quot;highlighter-rouge&quot;&gt;RDD并不是万能的，也不是什么带着纱巾的少女那样神奇。简单的理解，就是一群大牛为了解决一个问题而设计的一个特定的数据结构，that&#39;s all&lt;/code&gt;。&lt;/p&gt;

&lt;h3 id=&quot;what-is-dag---&quot;&gt;2.2 What is DAG - 趣说有向无环图&lt;/h3&gt;

&lt;p&gt;DAG - Direct Acyclic Graph，有向无环图，好久没看图片了，先发个图片来理解理解吧。
&lt;img src=&quot;../../images/dag.jpg&quot; alt=&quot;DAG&quot; /&gt;&lt;br /&gt;
要理解DAG，只需弄明白三个概念就可以毕业了，首先，我们假设上图图二中的A,B,C,D,E都代表spark里不同的RDD：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;图：图是表达&lt;code class=&quot;highlighter-rouge&quot;&gt;RDD Lineage&lt;/code&gt;信息的一个结构，在 spark 中，大部分 RDD 都是通过其他 RDD 进行转换而来的，比如说上图图二中，B和D都是通过A转换而来的，而C是通过B转换而来，E的话是通过B和D一起转换来的。&lt;/li&gt;
  &lt;li&gt;有向：有向就更容易理解了，简单来说就是 linage 是一个 top-down 的结构，而且是时间序列上的 top-down 结构，这里如果没有理解的话，我们在下面讲“无环”这个概念时一起说明。&lt;/li&gt;
  &lt;li&gt;无环：这里就是重点要理解的地方了，spark 的优化器在这里也发挥了很大的作用。首先，我们先理解一下无环的概念，假设有图三中左下 B,D,E 这样一个 RDD 转换图，那当我们的需要执行 D.collect 操作的时候，就会引发一个死循环了。不过，仔细想过的话，就会知道，“无环”这个问题其实已经在“有向”这个概念中提现了，上面说的“有向”，其实更详细的说是一个时间上的先来后到，即祖先与子孙的关系，是不可逆的。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;what-is-data-locality---rdd&quot;&gt;2.3 What is Data Locality - RDD的位置可见性&lt;/h3&gt;
&lt;p&gt;这个问题就不重复造轮子了，直接引用Quora上的一个&lt;a href=&quot;https://www.quora.com/How-do-I-make-clear-the-concept-of-RDD-in-Spark&quot;&gt;问答了&lt;/a&gt;:&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;RDD is a dataset which is &lt;code class=&quot;highlighter-rouge&quot;&gt;distributed&lt;/code&gt;, that is, it is divided into &lt;code class=&quot;highlighter-rouge&quot;&gt;&quot;partitions&quot;&lt;/code&gt;. Each of these partitions can be present in the memory or disk of different machines. If you want Spark to process the RDD, then Spark needs to &lt;code class=&quot;highlighter-rouge&quot;&gt;launch one task per partition of the RDD&lt;/code&gt;. It’s best that each task be sent to the machine have the partition that task is supposed to process. In that case, the task will be able to read the data of the partition from the local machine. Otherwise, the task would have to pull the partition data over the network from a different machine, which is less efficient. This scheduling of tasks (that is, allocation of tasks to machines) such that the tasks can read data “locally” is known as “&lt;code class=&quot;highlighter-rouge&quot;&gt;locality aware scheduling&lt;/code&gt;”.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;what-is-lazy-evaluation---&quot;&gt;2.4 What is Lazy Evaluation - 神马叫惰性求值&lt;/h3&gt;
&lt;p&gt;本来不想叫“惰性求值”的，看到“惰”这个字实在是各种不爽，实际上，我觉得应该叫”后续求值”，”按需计算”，”晚点搞”这类似的，哈哈。这几天一直在想应该怎么简单易懂地来表达Lazy Evaluation这个概念，本来打算引用MongoDB的Cursor来类比一下的，可总觉得还是小题大做了。这个概念就懒得解释了，主要是觉得太简单了，没有必要把事情搞得这么复杂，哈哈。&lt;/p&gt;

&lt;h3 id=&quot;what-is-narrowwide-dependency---rdd&quot;&gt;2.5 What is Narrow/Wide Dependency - RDD的宽依赖和窄依赖&lt;/h3&gt;
&lt;p&gt;首先，先从原文看看宽依赖和窄依赖各自的定义。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;narrow dependencies&lt;/code&gt;: where each partition of the parent RDD is used by at most one partition of the child RDD, &lt;code class=&quot;highlighter-rouge&quot;&gt;wide dependencis&lt;/code&gt;, where multiple child partitions may depend on it.&lt;/p&gt;

&lt;p&gt;按照&lt;a href=&quot;http://shiyanjun.cn/archives/744.html&quot;&gt;这篇RDD论文中文译文&lt;/a&gt;的解释，窄依赖是指子RDD的每个分区依赖于常数个父分区（即与数据规模无关）；宽依赖指子RDD的每个分区依赖于所有父RDD分区。暂且不说这样理解是否有偏差，我们先来从两个方面了解下计算一个窄依赖的子RDD和一个宽依赖的RDD时具体都有什么区别，然后再回顾这个定义。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;计算方面：
    &lt;ul&gt;
      &lt;li&gt;计算窄依赖的子RDD：可以在某一个计算节点上直接通过父RDD的某几块数据（通常是一块）计算得到子RDD某一块的数据；&lt;/li&gt;
      &lt;li&gt;计算宽依赖的子RDD：子RDD某一块数据的计算必须等到它的父RDD所有数据都计算完成之后才可以进行，而且需要对父RDD的计算结果进行hash并传递到对应的节点之上；&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;容错恢复方面：
    &lt;ul&gt;
      &lt;li&gt;窄依赖：当父RDD的某分片丢失时，只有丢失的那一块数据需要被重新计算；&lt;/li&gt;
      &lt;li&gt;宽依赖：当父RDD的某分片丢失时，需要把父RDD的所有分区数据重新计算一次，计算量明显比窄依赖情况下大很多；&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../images/spark-rdd-dependency.png&quot; alt=&quot;spark-rdd-dependency.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;3. 尚未提到的一些重要概念&lt;/h2&gt;
&lt;p&gt;还有一些基本概念上面没有提到，一些是因为自己还没怎么弄清楚，一些是觉得重要但是容易理解的，所以就先不记录下来了。比如说：粗粒度、细粒度；序列化和反序列化等。&lt;/p&gt;

&lt;h2 id=&quot;next&quot;&gt;4. Next&lt;/h2&gt;

&lt;p&gt;基础的概念和理论都讲得差不多了，该小试牛刀了，哈哈。&lt;/p&gt;

&lt;p&gt;下几篇的安排：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;列一些学习 spark 比较好的&lt;code class=&quot;highlighter-rouge&quot;&gt;资源&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;详细从 job，stage，task 的定义来谈谈 spark 的运行原理&lt;/li&gt;
  &lt;li&gt;准备几个稍稍复杂一点的&lt;code class=&quot;highlighter-rouge&quot;&gt;例子&lt;/code&gt;, 例子个数根据时间安排发布
    &lt;ul&gt;
      &lt;li&gt;spark 在金融领域的应用之 指数相似度计算&lt;/li&gt;
      &lt;li&gt;spark 在搜索领域的应用之 pagerank&lt;/li&gt;
      &lt;li&gt;spark 在社交领域的应用之 评分计算&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;开始讲 &lt;code class=&quot;highlighter-rouge&quot;&gt;dataframe 和 datasets&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-2&quot;&gt;5. 打开微信，扫一扫，点一点，棒棒的，^_^&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;../images/wechat_pay.png&quot; alt=&quot;wechat_pay.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;参考文章&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.csdn.net/anzhsoft/article/details/39851421&quot;&gt;Spark技术内幕：究竟什么是RDD&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf&quot;&gt;Resilient Distributed Datasets: A Fault-Tolerant Abstraction for
In-Memory Cluster Computing&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://shiyanjun.cn/archives/744.html&quot;&gt;RDD 论文中文版&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-4&quot;&gt;本系列文章链接&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/introduction-to-spark&quot;&gt;『 Spark 』1. spark 简介 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/spark-questions-concepts&quot;&gt;『 Spark 』2. spark 基本概念解析 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/spark-programming-model&quot;&gt;『 Spark 』3. spark 编程模式 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/spark-what-is-rdd&quot;&gt;『 Spark 』4. spark 之 RDD &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/spark-resouces-blogs-paper&quot;&gt;『 Spark 』5. 这些年，你不能错过的 spark 学习资源 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/deep-into-spark-exection-model&quot;&gt;『 Spark 』6. 深入研究 spark 运行原理之 job, stage, task&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/spark-dataframe-introduction&quot;&gt;『 Spark 』7. 使用 Spark DataFrame 进行大数据分析&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/spark-in-finance-and-investing&quot;&gt;『 Spark 』8. 实战案例 ｜ Spark 在金融领域的应用 ｜ 日内走势预测&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/ipython-notebook-spark&quot;&gt;『 Spark 』9. 搭建 IPython + Notebook + Spark 开发环境&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/boost-spark-application-performance&quot;&gt;『 Spark 』10. spark 应用程序性能优化｜12 个优化方法&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
   <entry>
     <title>『 Spark 』3. spark 编程模式</title>
     <link href="/spark-programming-model"/>
     <updated>2016-03-04T00:00:00+08:00</updated>
     <id>/spark-programming-model</id>
     <content type="html">&lt;h2 id=&quot;section&quot;&gt;写在前面&lt;/h2&gt;

&lt;p&gt;本系列是综合了自己在学习spark过程中的理解记录 ＋ 对参考文章中的一些理解 ＋ 个人实践spark过程中的一些心得而来。写这样一个系列仅仅是为了梳理个人学习spark的笔记记录，所以一切以能够理解为主，没有必要的细节就不会记录了，而且文中有时候会出现英文原版文档，只要不影响理解，都不翻译了。若想深入了解，最好阅读参考文章和官方文档。&lt;/p&gt;

&lt;p&gt;其次，本系列是基于目前最新的 spark 1.6.0 系列开始的，spark 目前的更新速度很快，记录一下版本好还是必要的。 &lt;br /&gt;
最后，如果各位觉得内容有误，欢迎留言备注，所有留言 24 小时内必定回复，非常感谢。   &lt;br /&gt;
Tips: 如果插图看起来不明显，可以：1. 放大网页；2. 新标签中打开图片，查看原图哦。&lt;/p&gt;

&lt;h2 id=&quot;spark-&quot;&gt;1. spark 基本编程模式&lt;/h2&gt;

&lt;p&gt;spark 里有两个很重要的概念：SparkContext [一般简称为 sc] 和 RDD，在上一篇文章中 &lt;a href=&quot;../spark-questions-concepts&quot;&gt;『 Spark 』2. spark 基本概念解析 &lt;/a&gt; 有讲到。可以说，sc 和 RDD 贯穿了 spark app 的大部分生命周期，从 app 的初始化，到数据的清洗，计算，到最后获取，展示结果。&lt;/p&gt;

&lt;p&gt;为了更加深入的了解 RDD 和基于 RDD 的编程模型，我们先把 RDD 的属性简单的分一个类，然后再通过一张流程图来理解。&lt;/p&gt;

&lt;h3 id=&quot;rdd-&quot;&gt;1.1 RDD 的属性&lt;/h3&gt;

&lt;p&gt;接触过 RDD 的人肯定都知道 &lt;em&gt;transform&lt;/em&gt; 和 &lt;em&gt;action&lt;/em&gt; 这两个核心概念，甚至很多人都认为 RDD 仅仅有 &lt;em&gt;transform&lt;/em&gt; 和 &lt;em&gt;action&lt;/em&gt; 这两个概念。殊不知其实 RDD 里面还有很多其他方法，下面我们来简单的分个类，在看这里的时候最好参考一下官方的 &lt;a href=&quot;http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD&quot;&gt;api 文档&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;RDD
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;action&lt;/code&gt;     : count, take, sample, first, collect  …&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;transform&lt;/code&gt;  : foreach, glom, map …&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;method&lt;/code&gt;     : cache, checkpoint, id, isCheckpointed, isEmpty, keys, lookup, max, mean, name, setName …&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;property&lt;/code&gt;   : context&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;看到了吗，这里其实 RDD 其实有很多既不是 &lt;em&gt;transform&lt;/em&gt; 也不是 &lt;em&gt;action&lt;/em&gt; 的函数和属性，在编写 spark app 的时候，其实很多时候我们都会用到那些 method，这样在开发调试过程中都会更加方便。比如说 &lt;code class=&quot;highlighter-rouge&quot;&gt;cache&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;setName&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;lookup&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;id&lt;/code&gt; 这些，在开发过程中都很有用。&lt;/p&gt;

&lt;h3 id=&quot;spark--1&quot;&gt;1.2 spark 编程模式图&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;../images/spark-programming-model.jpg&quot; alt=&quot;spark-programming-model.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;如图所示，我们构建 spark app，一般都是三个步骤:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;加载数据集&lt;/em&gt;，这里的数据集大概分为两组:
    &lt;ul&gt;
      &lt;li&gt;一种是不变的，静态数据集，大多数场景都是从数据库，文件系统上面加载进来&lt;/li&gt;
      &lt;li&gt;另一种是动态的数据集，一般做 streaming 应用的时候用到，大多数场景是通过 socket 来加载数据，复杂场景可以通过文件系统，akka actors，kafka，kinesis 和 一些第三方提供的 streaming api [twitter 等] 来作为数据源加载数据&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;处理数据&lt;/em&gt;，这是重点中的重点，不过不外乎都是从三个方面来完成这里的数据清理，逻辑运算等:
    &lt;ul&gt;
      &lt;li&gt;自定义的一些复杂处理函数或者第三方包 [下面我们称为函数集]&lt;/li&gt;
      &lt;li&gt;通过 RDD 的 transform，action 和函数集来完成整个处理，计算流程&lt;/li&gt;
      &lt;li&gt;通过 RDD 提供的 cache，persist，checkpoint 方法把一些处理流程中的重要处理节点和常用数据缓存和备份，以加速处理，计算速度&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;结果展示&lt;/em&gt;，这里一般情况都是使用 RDD 的 collect，take，first，top 等方法把结果取出来，更常用的是先把结果取出来，放到一个数据库或文件系统上，然后再提供给专门展示结果的另一个 application 使用。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;mc-monte-carlo&quot;&gt;2. 例子：MC [Monte Carlo]&lt;/h2&gt;

&lt;p&gt;下面我将从几个方面来介绍这个例子：首先是介绍蒙特卡罗方法的基本概念和应用，然后是介绍如何用蒙特卡罗方法来估算 pi 的值，最后是看在 spark 集群中如何用多种方法来实现一个蒙特卡洛应用来计算 pi 的值。&lt;/p&gt;

&lt;h3 id=&quot;section-1&quot;&gt;2.1 蒙特卡罗方法介绍&lt;/h3&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;## from wiki 
Monte Carlo methods (or Monte Carlo experiments) are a broad class of 
computational algorithms that rely on repeated random sampling to obtain 
numerical results. They are often used in physical and mathematical problems 
and are most useful when it is difficult or impossible to use other mathematical 
methods. Monte Carlo methods are mainly used in three distinct problem 
classes:[1] optimization, numerical integration, and generating draws from 
a probability distribution.

## 
总的来说，蒙特卡罗是一种基于随机样本实验来进行估值的一种计算方法。&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3 id=&quot;pi-&quot;&gt;2.2 蒙特卡罗方法估算 pi 值原理&lt;/h3&gt;

&lt;p&gt;用蒙特卡罗方法估算 pi 值，核心方法是利用正方形和圆形面积的比例：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;首先，我们在坐标轴上构造一个边长为 1 的正方形&lt;/li&gt;
  &lt;li&gt;其次，我们以 (0, 0) 为圆心，构造一个半径为 1 的圆形&lt;/li&gt;
  &lt;li&gt;此时我们知道这个圆形有 1/4 是在正方形中的，正方形的面积和这 1/4 圆的面积分别是：1 和 pi/4，即 1/4 圆的面积和正方形面积之比刚好是 pi/4&lt;/li&gt;
  &lt;li&gt;然后通过蒙特卡罗模拟，看看这个比例大概是多少，模拟方法如下：
    &lt;ul&gt;
      &lt;li&gt;随机扔 n 个点 (x, y)，其中 x, y 都在 0 和 1 之间&lt;/li&gt;
      &lt;li&gt;如果 x^2 + y^2 &amp;lt; 1，则把这个点标注为红色，表示这个点落在圆内&lt;/li&gt;
      &lt;li&gt;最后数数有 n 个点中有多少点是红点，即落在圆内，假设点数为 m&lt;/li&gt;
      &lt;li&gt;则这个 1/4 圆的面积和正方形面积的比例应该是：m/n，即 m/n = pi/4 =&amp;gt; pi = 4*m/n&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../images/mc.gif&quot; alt=&quot;mc.gif&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;python--pi-&quot;&gt;2.3 Python 实现蒙特卡罗方法估算 pi 值&lt;/h3&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;import numpy as np

def mc_pi(n=100):
    &quot;&quot;&quot;Use Monte Calo Method to estimate pi.
    &quot;&quot;&quot;
    m = 0
    i = 0
    while i &amp;lt; n:
        x, y = np.random.rand(2)
        if x**2 + y**2 &amp;lt; 1:
            m += 1
        i += 1

    pi = 4. * m / n
    res = {&#39;total_point&#39;: n, &#39;point_in_circle&#39;: m, &#39;estimated_pi&#39;: pi}
    
    return res&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;../images/spark-programming-model-11.jpg&quot; alt=&quot;spark-programming-model-11.jpg&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;spark--2&quot;&gt;2.4 在 spark 集群中实现蒙特卡罗方法&lt;/h3&gt;

&lt;p&gt;我们按照上面写的三大步骤来写这个 spark 应用：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;加载数据集&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;### iterate number&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;total&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;local_collection&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;xrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;total&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;### parallelize a data set into the cluster&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;rdd&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parallelize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;local_collection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;       \
        &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;parallelized_data&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;        \
        &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;处理数据&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;### randomly generate points&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;map_func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;element&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;       &lt;span class=&quot;c&quot;&gt;## [0, 1)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;       &lt;span class=&quot;c&quot;&gt;## [0, 1)&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;             &lt;span class=&quot;c&quot;&gt;## random point&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;map_func_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;element&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;element&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;rdd2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rdd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;map_func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;            \
          &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;random_point&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  \
          &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;### calculate the number of points in and out the circle&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;rdd3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rdd2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;map_func_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;                 \
           &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;points_in_out_circle&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
           &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;结果展示&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;### how many points are in the circle&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;in_circle&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rdd3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;reduce&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;operator&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;4.&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;in_circle&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;total&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;iterate {} times&#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;total&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;estimated pi : {}&#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3 id=&quot;seems-a-little-complex-really&quot;&gt;2.5 Seems a little complex, really?&lt;/h3&gt;

&lt;p&gt;上面这个例子，可能会让一些初步接触 spark 的人很困惑，”明明几行代码就能解决的问题在 spark 里还有按照这些步骤写这么多代码？难道是老湿又骗我了吗？”。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/wawawa.gif&quot; alt=&quot;wawawa.gif&quot; /&gt;&lt;/p&gt;

&lt;p&gt;其实，就从上面这个例子看起来，似乎 spark 真的没有什么优势，但是，上面这个例子的目的是表明 spark 的编程模式，如果你还不相信，可以把模拟次数加到千万或者亿次以上看看效果。&lt;/p&gt;

&lt;p&gt;如果，如果你还是纠结于 “我骗了你，spark 没有梦想中的那么好” 的话，那看下面这一行代码吧，它也完成了同样的事情：&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;### version 1&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parallelize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;xrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;total&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;                                 \
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()))&lt;/span&gt;        \
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;         \
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;reduce&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;                               \
    &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;total&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;### version 2&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parallelize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;xrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;total&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;                                  \
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  \
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;reduce&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;                                \
    &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;total&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;next&quot;&gt;3. Next&lt;/h2&gt;

&lt;p&gt;下一篇，介绍 spark 的 RDD，之后会单独介绍 spark 的 dataframe 和 datasets。&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;4. 打开微信，扫一扫，点一点，棒棒的，^_^&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;../images/wechat_pay.png&quot; alt=&quot;wechat_pay.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;参考文章&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;../files/spark-rdd-paper.pdf&quot;&gt;spark-rdd-paper : Resilient Distributed Datasets: A Fault-Tolerant Abstraction for
In-Memory Cluster Computing&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://spark.apache.org/docs/latest/api/python/pyspark.html&quot;&gt;spark python API&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext&quot;&gt;spark context API&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://snap.stanford.edu/data/&quot;&gt;机器学习相关数据集-斯坦福&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/spark/blob/master/examples/src/main/python/pagerank.py&quot;&gt;spark pagerank example&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://latex.91maths.com/&quot;&gt;latex online editor 在线latex公式编辑器&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.ruanyifeng.com/blog/2015/07/monte-carlo-method.html&quot;&gt;阮一峰：蒙特卡罗&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Monte_Carlo_method&quot;&gt;蒙特卡罗，wikipedia&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.sciencenet.cn/blog-324394-292355.html&quot;&gt;科学网：蒙特卡罗&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-4&quot;&gt;本系列文章链接&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/introduction-to-spark&quot;&gt;『 Spark 』1. spark 简介 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/spark-questions-concepts&quot;&gt;『 Spark 』2. spark 基本概念解析 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/spark-programming-model&quot;&gt;『 Spark 』3. spark 编程模式 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/spark-what-is-rdd&quot;&gt;『 Spark 』4. spark 之 RDD &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/spark-resouces-blogs-paper&quot;&gt;『 Spark 』5. 这些年，你不能错过的 spark 学习资源 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/deep-into-spark-exection-model&quot;&gt;『 Spark 』6. 深入研究 spark 运行原理之 job, stage, task&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/spark-dataframe-introduction&quot;&gt;『 Spark 』7. 使用 Spark DataFrame 进行大数据分析&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/spark-in-finance-and-investing&quot;&gt;『 Spark 』8. 实战案例 ｜ Spark 在金融领域的应用 ｜ 日内走势预测&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/ipython-notebook-spark&quot;&gt;『 Spark 』9. 搭建 IPython + Notebook + Spark 开发环境&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/boost-spark-application-performance&quot;&gt;『 Spark 』10. spark 应用程序性能优化｜12 个优化方法&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   

</feed>



  <script type="text/javascript">
    var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");
    document.write(unescape("%3Cspan id='cnzz_stat_icon_1258855744'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s11.cnzz.com/z_stat.php%3Fid%3D1258855744' type='text/javascript'%3E%3C/script%3E"));
  </script>

</body>
</html>
