<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8" />
  <title>Taotao's Zone</title>
  <meta name="renderer" content="webkit">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="mobile-web-app-capable" content="yes">
  <link rel="stylesheet" href="/css/font-awesome/css/font-awesome.min.css" type="text/css" />
  <link rel="stylesheet" href="/css/default.css" type="text/css" />
  <link rel="stylesheet" href="/css/desktop.css" type="text/css" />
  <link rel="stylesheet" href="/css/mobile.css" type="text/css" />
  <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
  <link rel="alternate" type="application/atom+xml" title="Recent Entries" href="/atom.xml" />
  <script src="/js/jquery-1.11.0.min.js" type="text/javascript"></script>
  <script src="/js/jquery-migrate-1.2.1.js" type="text/javascript"></script>
  <script src="/js/jquery.transit.min.js" type="text/javascript"></script>
  <script src="/js/common.js" type="text/javascript"></script>
</head>
<body>
  <?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

   <title>Taotao's Zone</title>
   <link href="http://litaotao.github.io/atom.xml" rel="self" type="application/atom+xml"/>
   <link href="http://litaotao.github.io" rel="alternate" type="text/html" />
   <updated>2015-02-04T17:28:37+08:00</updated>
   <id>http://litaotao.github.io</id>
   <author>
     <name></name>
     <email></email>
   </author>

   
   <entry>
     <title>［touch spark］6. 终于等到你，spark streaming + 新浪微博数据</title>
     <link href="/weibo-api-in-action"/>
     <updated>2015-01-05T00:00:00+08:00</updated>
     <id>/weibo-api-in-action</id>
     <content type="html">&lt;p&gt;注：与本文相关的所有源代码已放在最最喜爱的 &lt;a href=&quot;https://github.com/litaotao/weibostreaming&quot;&gt;Github&lt;/a&gt; 上。&lt;/p&gt;

&lt;h2&gt;&lt;/h2&gt;

&lt;h2&gt;1. 写在前面&lt;/h2&gt;

&lt;p&gt;　　从最开始想尝试streaming的时候，我一心想实现databricks在spark summit 2014上的那个演示---实时获取twiiter数据，并做分析展示。无奈，在折腾twitter几天后只得放弃。刚开始注册twitter得需要手机号，可是twitter居然不支持中国内地的号码，这里不知道是twitter不支持，还是中国内地的运营商不支持twitter，我想应该是后者吧。当时纠结了好久，一种心碎的感觉。后来终于突然有一天无需手机号也能注册了，我欣喜若狂地注册了一个twitter号，后来发现尼玛新建一个twitter app需要手机验证，这下我就完全down机了，无奈，twitter这条路是完全走不通了。这段时间的心情，就像过山车一样，从心碎到兴奋再到最后的心死。&lt;br/&gt;
&lt;img src=&quot;../../images/guoshanche.gif&quot; alt=&quot;过山车&quot; /&gt;
　　可是我不死心啊，还行想玩玩streaming，想体验体验streaming的power，怎么办怎么办？我左思右想，想要做到databricks的效果，唯一的办法就是利用咱们天朝的新浪微博了。在调研了一下新浪微博的API后，其原理和twitter API的原理也是一样的，但是新浪微博的streaming API被取消了。好吧，硬骨头挺多，还得自己啃了。没办法，我想了想，就2个选择，要么不做，然后天天后悔；要么做，然后天天折腾。好吧，我承认我还是义无返顾地选择了第二条路，sigh。&lt;br/&gt;
　　在写这篇博客的时候看到徐静蕾的新片《有一个地方只有我们知道》，里面一句话让我深有感触---&lt;strong&gt;没有在一起的，就是不对的人，对的人，你是不会失去他的&lt;/strong&gt;，我也想说，没有学到的技术，就是你不喜爱的技术，喜欢的技术，你是不可能学不会的。&lt;/p&gt;

&lt;h2&gt;2. 实验目的&lt;/h2&gt;

&lt;p&gt;　　这次实验是想尝试一下spark streaming的效果，预期是这样的：通过每隔 &lt;strong&gt;几秒&lt;/strong&gt; 从新浪微博拿到 &lt;strong&gt;一些&lt;/strong&gt; 公开的微博数据，然后实时 &lt;strong&gt;处理&lt;/strong&gt; 一下这些数据并 &lt;strong&gt;展现&lt;/strong&gt; 出来。&lt;br/&gt;
　　ok，这里的关键上面已经用粗体标记出来了。有两个方面，一是时间间隔的设置，数据流量的设置，这关系到streaming的稳定性，比如说若处理速度小于数据流入的速度的话，那数据会慢慢堆积起来；若数据流入速度小于处理速度的话，展现处理结果肯定也不好看。这里是属于tuning的环境，可以在spark官网上仔细瞧瞧，不过具体还是要根据应用需求来定。第二个方面是数据处理和展示，这应该是应用的核心了。这里我们做得很初级，简单做一些TF-IDF的测试或者是更简单的包含性测试。 &lt;br/&gt;
　　既然是第一次，那就不要太那啥，还是温柔一点比较好。暂时定一个目标，我们想看看实时微博中哪些是包含某某字段的，然后输出这些微博的信息。比如说，我想知道实时微博中，哪些是包含 &lt;strong&gt;建设银行&lt;/strong&gt;，&lt;strong&gt;涨&lt;/strong&gt; 这样两个关键字的微博，并实时打印出来。&lt;/p&gt;

&lt;h2&gt;3. streaming，我的目标&lt;/h2&gt;

&lt;p&gt;　　既然要玩，那就玩得痛快点，下面是我准备在微博streaming这块做的一些各个版本的安排：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;数据方面：

&lt;ul&gt;
&lt;li&gt;第一步，能够获取微博伪实时数据即可【微博API请求有限制】；&lt;/li&gt;
&lt;li&gt;第二步，自己设计一个搜集系统，能获取近实时的微博数据，希望能媲美原来微博的streaming API；&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;应用方面：

&lt;ul&gt;
&lt;li&gt;第一步，啥都不干，确认spark   cluster能连到我的数据源，把所有接收到的数据简单打印出来；&lt;/li&gt;
&lt;li&gt;第二步，简单处理，对实时数据流进行一个简单的filter操作，比如说，看看哪些消息是提到了某人，或某支股票；&lt;/li&gt;
&lt;li&gt;第三步，复杂一点，用用MLib来对每条消息学习一下，其实就是高级的filter；&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;　　目前我打算从上面几个小目标一点一点上，最近发现一个NB项目，Zeppelin，底层可以集成spark，届时看看是否有需要，可以尝试下
eppelin+spark。【最近我尝试编译过Zeppelin，遇到很多问题，目前这个项目还不是很成熟，不过项目组说了，他们正在迁移到Apache的孵化器中，完成迁移后会专心发布新版本。Good，看来的确是一个NB项目】 &lt;br/&gt;
　　冲啊，每天进步一点点。
&lt;img src=&quot;../../images/stepbystep.jpg&quot; alt=&quot;每天进步一点点咯&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;4. 步骤规划&lt;/h2&gt;

&lt;p&gt;　　我们第一次的目标很简单，我准备在数据方面，简单地完成第一步；在应用方面，也是简单地完成第一步，算是一个最小的MVP了，暂定为MVP 0.1.0
吧，哈哈。好，现在简单地分析下，大概有下面几个步骤：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;获取数据，通过新浪微博API，这里需要我们设计一个数据收集器&lt;/li&gt;
&lt;li&gt;发送/接收数据，因为我改用Python来玩spark了，目前spark 1.2版的python streaming只支持socket包，当然socket包也是最简单的了，所以我准备用socket方式进行数据的收发。So，这里我们需要写一个简单的Socket Server&lt;/li&gt;
&lt;li&gt;展示数据，简单的打印下来即可&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;　　下面，我们就按照MVP 0.1.0 的步骤规划，一步一步来搞定咱们这个小系统。&lt;/p&gt;

&lt;h2&gt;5. 获取数据：新浪微博API使用&lt;/h2&gt;

&lt;p&gt;　　微博官方已经有详细的新手引导了，这里就不重复造轮子了，大家可以直接参考 &lt;a href=&quot;http://open.weibo.com/wiki/%E6%96%B0%E6%89%8B%E6%8C%87%E5%8D%97&quot;&gt;这里&lt;/a&gt;。我用的是&lt;a href=&quot;https://github.com/michaelliao/sinaweibopy&quot;&gt;Python SDK&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;6. 收发数据：Socket Server&lt;/h2&gt;

&lt;p&gt;　　好久没有接触网络编程这块了，这里为了快速完成MVP的效果，我用了最简单的多线程socket server模型，即新建一个线程用于处理一个新的连接。整个socket server的模型如下：&lt;br/&gt;
&lt;img src=&quot;../../images/socket-server.jpg&quot; alt=&quot;simple-socket-server&quot; /&gt;&lt;/p&gt;

&lt;p&gt;　　socket server的核心代码如下，完整代码请查看github。这里send_data应该是到去拿新浪微博的数据的，但是我在测试的时候为了方便起见，先简单地用了一条测试数据： data = &#39;hello, I am litaotao&#39;。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def send_data(conn, client):
    # data = get_data(client)
    data = &#39;hello, I am litaotao&#39;
    conn.sendall(data.encode(&#39;utf-8&#39;))
    print &#39;\nIN THREAD: send to {}, data length: {}&#39;.format(str(conn), str(len(data)))
    conn.close()

def socket_server(HOST, PORT):
    client = get_local_weibo_client() or get_weibo_client()

    # s = socket.socket(socket.AF_INET,socket.SOCK_STREAM)  
    s = socket.socket()    
    s.bind((HOST, PORT))
    s.listen(10)

    while  True:
        print &#39;wait for connection ...&#39;
        conn, addr = s.accept()
        print &#39;connect with {} : {}&#39;.format(addr[0], str(addr[1]))
        thread.start_new_thread(send_data, (conn, client))      
    s.shutdown()

if __name__ == &#39;__main__&#39;:
    HOST, PORT = &#39;&#39;, 9999
    socket_server(HOST, PORT)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;7. 展示数据: Just print&lt;/h2&gt;

&lt;p&gt;　　现在socket server已经准备就绪了，接下来准备一下spark端的任务逻辑。在这之前，我先简单介绍一下目前我的spark集群环境，为了方便理解，我也把环境的IP地址列出来了，这样在以后启动命令的时候也比较清楚。&lt;br/&gt;
　　可以清楚的看到，目前spark cluster测试环境里一共有10台机器，其中一台&lt;code&gt;10.21.208.21&lt;/code&gt;作为cluster manager，即master使用，其他9台作为worker使用。而我们写spark任务程序以及提交任务，拿到任务运行结果，都在一台driver机器上，driver机器ip是&lt;code&gt;10.20.70.80&lt;/code&gt;。
&lt;img src=&quot;../../images/spark-cluster.jpg&quot; alt=&quot;spark-cluster&quot; /&gt;
　　ok，现在可以来写spark的任务程序了，很简单，就是一个print语句，我是用python写的，代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# -*- coding: utf-8 -*-
import sys

from pyspark import SparkContext
from pyspark.streaming import StreamingContext


def change_nothing(lines):
    return lines

if __name__ == &quot;__main__&quot;:
    if len(sys.argv) != 3:
        print &amp;gt;&amp;gt; sys.stderr, &quot;Usage: weibo_message.py &amp;lt;hostname&amp;gt; &amp;lt;port&amp;gt;&quot;
        exit(-1)
    sc = SparkContext(appName=&quot;PythonStreamingWeiboMessage&quot;)
    ssc = StreamingContext(sc, 5)

    lines = ssc.socketTextStream(sys.argv[1], int(sys.argv[2]))
    lines = change_nothing(lines)
    lines.pprint()
    ssc.start()
    ssc.awaitTermination()
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;8. Opps, 为神马只有一个worker接收到数据了&lt;/h2&gt;

&lt;p&gt;　　原本以为这样就大功告成，可是当我兴奋地运行程序的时候，突然发现一个极为严重的问题---只有一个worker会到socket server这里来获取数据，并&lt;strong&gt;处理&lt;/strong&gt;后返回给driver，而且有时候是第一台worker来拿数据，有时候却又是另外一台worker来拿数据，anyway，问题就是：9太worker中，一直只有1台worker来拿数据，处理并返回，且这台worker并不是固定的。 &lt;br/&gt;
　　下面是日志：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;socket server的运行日志&lt;/li&gt;
&lt;/ul&gt;


&lt;pre&gt;&lt;code&gt;C:\Users\taotao.li\Desktop\weibostreaming (master)                                        
λ python socket_server_1.py                                                               
wait for connection ...                                                                   
-----------------------                                                                   
connect with 10.21.208.30 : 48927                                                         
wait for connection ...                                                                   
-----------------------                                                                   

IN THREAD: send to &amp;lt;socket._socketobject object at 0x0000000002C45E18&amp;gt;, data length: 20   
connect with 10.21.208.30 : 48929                                                         
wait for connection ...                                                                   
-----------------------                                                                   
IN THREAD: send to &amp;lt;socket._socketobject object at 0x0000000002C45E80&amp;gt;, data length: 20   

connect with 10.21.208.30 : 48931                                                         
wait for connection ...                                                                   
 -----------------------                                                                  
IN THREAD: send to &amp;lt;socket._socketobject object at 0x0000000002C45E18&amp;gt;, data length: 20   

connect with 10.21.208.30 : 48933                                                         
wait for connection ...                                                                   
-----------------------                                                                   

IN THREAD: send to &amp;lt;socket._socketobject object at 0x0000000002C45E80&amp;gt;, data length: 20   
connect with 10.21.208.30 : 48935                                                         
wait for connection ...                                                                   
IN THREAD: send to &amp;lt;socket._socketobject object at 0x0000000002C45E18&amp;gt;, data length: 20   

-----------------------                                                                   
connect with 10.21.208.30 : 48937                                                         
wait for connection ...                                                                   
IN THREAD: send to &amp;lt;socket._socketobject object at 0x0000000002C45E80&amp;gt;, data length: 20   

-----------------------                                                                   
connect with 10.21.208.30 : 48938                                                         
wait for connection ...                                                                   
 -----------------------                                                                  
IN THREAD: send to &amp;lt;socket._socketobject object at 0x0000000002C45E18&amp;gt;, data length: 20   

connect with 10.21.208.30 : 48940                                                         
wait for connection ...                                                                   
IN THREAD: send to &amp;lt;socket._socketobject object at 0x0000000002C45E80&amp;gt;, data length: 20   

-----------------------                                                                   
connect with 10.21.208.30 : 48942                                                         
wait for connection ...                                                                   
IN THREAD: send to &amp;lt;socket._socketobject object at 0x0000000002C45E18&amp;gt;, data length: 20   

-----------------------                                                                   
connect with 10.21.208.30 : 48944                                                         
wait for connection ...                                                                   
-----------------------                                                                   
IN THREAD: send to &amp;lt;socket._socketobject object at 0x0000000002C45E80&amp;gt;, data length: 20   

connect with 10.21.208.30 : 48946                                                         
wait for connection ...                                                                   
 -----------------------                                                                  
IN THREAD: send to &amp;lt;socket._socketobject object at 0x0000000002C45E18&amp;gt;, data length: 20   

connect with 10.21.208.30 : 48948                                                         
wait for connection ...                                                                   
 -----------------------                                                                  
IN THREAD: send to &amp;lt;socket._socketobject object at 0x0000000002C45E80&amp;gt;, data length: 20   

connect with 10.21.208.30 : 48952                                                         
wait for connection ...                                                                   
IN THREAD: send to &amp;lt;socket._socketobject object at 0x0000000002C45E18&amp;gt;, data length: 20   
-----------------------                                                                   

connect with 10.21.208.30 : 48953                                                         
wait for connection ...                                                                   
IN THREAD: send to &amp;lt;socket._socketobject object at 0x0000000002C45E80&amp;gt;, data length: 20   

-----------------------                                                                   
connect with 10.21.208.30 : 48955                                                         
wait for connection ...                                                                   
IN THREAD: send to &amp;lt;socket._socketobject object at 0x0000000002C45E18&amp;gt;, data length: 20   

-----------------------                                                                   
connect with 10.21.208.30 : 48956                                                         
wait for connection ...                                                                   
IN THREAD: send to &amp;lt;socket._socketobject object at 0x0000000002C45E80&amp;gt;, data length: 20   

-----------------------                                                                   
connect with 10.21.208.30 : 48957                                                         
wait for connection ...                                                                   
 -----------------------                                                                  
IN THREAD: send to &amp;lt;socket._socketobject object at 0x0000000002C45E18&amp;gt;, data length: 20   

-----------------------
Traceback (most recent call last):
  File &quot;socket_server_1.py&quot;, line 24, in &amp;lt;module&amp;gt;
    socket_server(HOST, PORT)
  File &quot;socket_server_1.py&quot;, line 16, in socket_server
    conn, addr = s.accept()
  File &quot;C:\Anaconda\lib\socket.py&quot;, line 202, in accept
    sock, addr = self._sock.accept()
KeyboardInterrupt
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;spark 任务的启动命令，我把处理返回的结果重定向到log.txt里，方便查看&lt;/li&gt;
&lt;/ul&gt;


&lt;pre&gt;&lt;code&gt;root@ubuntu2[17:41:01]:~/Desktop/streaming#spark-submit --master spark://10.21.208.21:7077 weibo_message.py 10.20.102.52 9999 &amp;gt; log.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;spark 任务运行日志，太多了，完整的日志可以到 &lt;a href=&quot;../../files/spark-console.log&quot;&gt;这里下载 spark-console.log&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;spark 任务运行结果日志 log.txt，完整的日志可以到 &lt;a href=&quot;../../files/log.txt&quot;&gt;这里下载 log.txt&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;pre&gt;&lt;code&gt;-------------------------------------------
Time: 2015-01-21 18:50:05
-------------------------------------------

-------------------------------------------
Time: 2015-01-21 18:50:10
-------------------------------------------
hello, I am litaotao

-------------------------------------------
Time: 2015-01-21 18:50:15
-------------------------------------------
hello, I am litaotao

-------------------------------------------
Time: 2015-01-21 18:50:20
-------------------------------------------
hello, I am litaotao
hello, I am litaotao

-------------------------------------------
Time: 2015-01-21 18:50:25
-------------------------------------------

-------------------------------------------
Time: 2015-01-21 18:50:30
-------------------------------------------

-------------------------------------------
Time: 2015-01-21 18:50:35
-------------------------------------------

-------------------------------------------
Time: 2015-01-21 18:50:40
-------------------------------------------

-------------------------------------------
Time: 2015-01-21 18:50:45
-------------------------------------------

-------------------------------------------
Time: 2015-01-21 18:50:50
-------------------------------------------

-------------------------------------------
Time: 2015-01-21 18:50:55
-------------------------------------------

-------------------------------------------
Time: 2015-01-21 18:51:00
-------------------------------------------
hello, I am litaotao

-------------------------------------------
Time: 2015-01-21 18:51:05
-------------------------------------------
hello, I am litaotao

-------------------------------------------
Time: 2015-01-21 18:51:10
-------------------------------------------

-------------------------------------------
Time: 2015-01-21 18:51:15
-------------------------------------------

-------------------------------------------
Time: 2015-01-21 18:51:20
-------------------------------------------

-------------------------------------------
Time: 2015-01-21 18:51:25
-------------------------------------------

-------------------------------------------
Time: 2015-01-21 18:51:30
-------------------------------------------
hello, I am litaotao
hello, I am litaotao

-------------------------------------------
Time: 2015-01-21 18:51:35
-------------------------------------------
hello, I am litaotao
hello, I am litaotao
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;Web UI 监控截图，显示只有一个receiver
&lt;img src=&quot;../../images/only-one-receiver.jpg&quot; alt=&quot;only-one-receiver&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;9. Why! What happened?&lt;/h2&gt;

&lt;p&gt;　　百思不得其解，这是为什么呢，这里我有两个疑点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;难道我对这个任务的理解有误吗。我的理解是，当运行spark-submit提交任务后，master应该会把这个weibo_message代码分发到9个worker上，然后9个worker分别在自己的机器上新建TCP连接到socket server，并从这个socket server上获取数据，然后处理后各自独立返回给driver。难道是我理解错误了吗。&lt;/li&gt;
&lt;li&gt;我仔细研读了官方&lt;a href=&quot;http://spark.apache.org/docs/latest/streaming-programming-guide.html#reducing-the-processing-time-of-each-batch&quot;&gt;spark streaming的教程&lt;/a&gt;，在里面发现这样一个主题 &lt;a href=&quot;http://spark.apache.org/docs/latest/streaming-programming-guide.html#reducing-the-processing-time-of-each-batch&quot;&gt;Level of Parallelism in Data Receiving&lt;/a&gt;，似乎对比起上图的Web UI监控图来看，难道是要自己根据worker的数据自定义receiver的数量。即目前我有9太worker，那我必须手动定义9个receiver？难道真的应该是这样的吗，我怎么觉得这样设计会很不灵活呢？why？&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
   <entry>
     <title>记录MongoDB一些优化方法</title>
     <link href="/mongodb-optimizing"/>
     <updated>2014-12-24T00:00:00+08:00</updated>
     <id>/mongodb-optimizing</id>
     <content type="html">&lt;h2&gt;&lt;/h2&gt;

&lt;h2&gt;1. 决定是否优化&lt;/h2&gt;

&lt;h3&gt;1.1 使用MongoDB自带的explain命令查看查询性能&lt;/h3&gt;

&lt;p&gt;　　下面是我在本地测试的一个例子。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; db.user.find({&#39;user&#39;:&#39;taotao.li@datayes.com&#39;}).explain()         
{             
        // 返回游标类型(BasicCursor | BtreeCursor | GeoSearchCursor | Complex Plan | multi)                                                         
        &quot;cursor&quot; : &quot;BasicCursor&quot;,
        // 是否使用多重索引(true | false)                                     
        &quot;isMultiKey&quot; : false,    
        // 返回的文档数量   (number)                                      
        &quot;n&quot; : 1,    
        // 被扫描的文档数量，这个值小于或等于nscanned   (number)                                                   
        &quot;nscannedObjects&quot; : 9,
        // 被扫描的文档数量，我们应该尽量使这个值和上面提到的n的值相近    (number)                                          
        &quot;nscanned&quot; : 9,    
        // 这个值表明在进行一次查询时，数据库计划扫描的文档数量 (number)                                            
        &quot;nscannedObjectsAllPlans&quot; : 9,                                
        &quot;nscannedAllPlans&quot; : 9,    
        // 若为true，表示查询不能利用文档在索引里的排序来返回结果，用户需要手动对返回进行排序操作，反之(true | false)                                     
        &quot;scanAndOrder&quot; : false,    
        // 若为true，表示查询是充分利用了现有的索引的，在设计索引的时候，应该尽量确保热点查询都利用到了已有的索引(true | false)                                     
        &quot;indexOnly&quot; : false,   
        // 表示查询语句执行时写锁的等待时间  (ms)                                       
        &quot;nYields&quot; : 0,          
        // 在分片的时候可以通过这个字段看分片的效果 (number)                                       
        &quot;nChunkSkips&quot; : 0, 
        // 耗时 (ms)                                               
        &quot;millis&quot; : 65,    
        // 所使用的索引 (dict of dict)                                             
        &quot;indexBounds&quot; : {                                             

        },        
        // mongo所在的服务器地址                                            
        &quot;server&quot; : &quot;SHN1408GPVG612:27017&quot;                             
}                                                                
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;1.2 使用MongoDB自带的profile优化器查看查询性能&lt;/h3&gt;

&lt;p&gt;　　MongoDB Database Profiler是一种慢查询日志功能,可以作为我们优化数据库的依据.
开启Profiling功能,有2种方式可以控制Profiling的开关盒级别。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;启动MonggoDB时加上 &lt;code&gt;-profile=级别&lt;/code&gt; 即可&lt;/li&gt;
&lt;li&gt;在客户端调用db.setProfilingLevel(级别)命令来实时配置&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;　　Profiler信息保存在system.profile中.我们可以通过db.getProfilingLevel()命令来获取当前的Profile级别。profile的级别有4个，分别是-1、0、1、2，默认没有开启。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;-1: 返回当前设置的级别&lt;/li&gt;
&lt;li&gt;0： 表示不开启&lt;/li&gt;
&lt;li&gt;1： 表示记录慢命令(默认为&gt;100ms)&lt;/li&gt;
&lt;li&gt;2： 表示记录所有命令&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;　　下面是我运行profile的一个示例及各个字段的解释：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; db.system.profile.findOne()
{
    //操作类型：insert | query | update | remove | getmore | command
    &quot;op&quot; : &quot;query&quot;,
    //进行op操作的地方，比如现在是说在community数据库的system集合的indexes中进行了一次查询操作
    &quot;ns&quot; : &quot;community.system.indexes&quot;,
    &quot;query&quot; : {
            &quot;expireAfterSeconds&quot; : {
                    &quot;$exists&quot; : true
            }
    },
    &quot;ntoreturn&quot; : 0,
    &quot;ntoskip&quot; : 0,
    &quot;nscanned&quot; : 2,
    &quot;keyUpdates&quot; : 0,
    &quot;numYield&quot; : 0,
    // 此次查询花在处理锁上的时间；其中R/W代表全局读/写锁，r/w代表数据库层面的读/写锁；
    &quot;lockStats&quot; : {
            &quot;timeLockedMicros&quot; : {
                    &quot;r&quot; : NumberLong(79),
                    &quot;w&quot; : NumberLong(0)
            },
            &quot;timeAcquiringMicros&quot; : {
                    &quot;r&quot; : NumberLong(1),
                    &quot;w&quot; : NumberLong(2)
            }
    },
    // 返回的文档数量
    &quot;nreturned&quot; : 0,
    // 返回字节长度，如果这个数字很大，考虑值返回所需字段
    &quot;responseLength&quot; : 20,
    // 查询所耗时间，这个时间是在mongo服务器端，从这个查询开始到查询结束；类似于一般程序执行的CPU时间；
    &quot;millis&quot; : 0,
    &quot;ts&quot; : ISODate(&quot;2014-11-19T09:33:58.965Z&quot;),
    // 发起此次查询的远程地址
    &quot;client&quot; : &quot;0.0.0.0&quot;,
    &quot;allUsers&quot; : [ ],
    // 执行此查询语句的用户，不知道这里为什么是空的
    &quot;user&quot; : &quot;&quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;1.3 使用MongoDB自带的mongostat优化器查看查询性能&lt;/h3&gt;

&lt;p&gt;　　下面是我本地测试的一个mongostat示例即相关字段含义：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;D:\mongodb-2.4.10\bin
λ mongostat.exe
connected to: 127.0.0.1
insert  query update delete getmore command flushes mapped  vsize    res faults  locked db idx miss %     qr|qw   ar|aw  netIn netOut  conn       time
    *0     *0     *0     *0       0     1|0       0   608m   1.4g    14m      1  test:3.3%          0       0|0     0|0    62b     3k    60   10:42:44
    *0     *0     *0     *0       0     1|0       0   608m   1.4g    14m      0  test:0.0%          0       0|0     0|0    62b     3k    60   10:42:45
    *0     *0     *0     *0       0     1|0       0   608m   1.4g    14m      0  test:0.0%          0       0|0     0|0    62b     3k    60   10:42:46
    *0     *0     *0     *0       0     1|0       0   608m   1.4g    14m      0  test:0.0%          0       0|0     0|0    62b     3k    60   10:42:47
    *0     *0     *0     *0       0     3|0       0   608m   1.4g    14m      0  test:0.0%          0       0|0     0|0   174b     3k    60   10:42:48
    *0     *0     *0     *0       0     1|0       0   608m   1.4g    14m      0  test:0.0%          0       0|0     0|0    62b     3k    60   10:42:49
    *0     *0     *0     *0       0     3|0       0   608m   1.4g    14m      0 local:0.0%          0       0|0     0|0   178b     3k    60   10:42:50
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　mongostat各个字段解释：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;insert/query/update/delete/getmore：每秒执行这5个操作的次数；&lt;/li&gt;
&lt;li&gt;command：每秒执行指令的次数，在从数据库中，这个字段的值是一个以“|”分开的两个值，表示 local|replicated 数量；&lt;/li&gt;
&lt;li&gt;flushes：每秒fsync操作的次数；&lt;/li&gt;
&lt;li&gt;mapped：按照官方解释，这个字段表示上一次执行mongostat指令是所有数据的大小，应该是指所有数据占磁盘的大小，但似乎不是很对；&lt;/li&gt;
&lt;li&gt;vsize：mongod服务占用的虚拟内存大小；&lt;/li&gt;
&lt;li&gt;res：mongod所占用的物理内存；&lt;/li&gt;
&lt;li&gt;faluts：page faults次数；&lt;/li&gt;
&lt;li&gt;index miss：索引缺失的数量；&lt;/li&gt;
&lt;li&gt;qr/qw：表示在队列中等待的客户端，rw表示读写；&lt;/li&gt;
&lt;li&gt;ar/aw：表示正在进行请求的客户端；&lt;/li&gt;
&lt;li&gt;netIn/netOut表示网络流量，单位是字节；&lt;/li&gt;
&lt;li&gt;conn：表示连接数；&lt;/li&gt;
&lt;li&gt;repl：表示同步状态；&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;1.4 使用MongoDB自带的db.serverStatus查看服务器状态&lt;/h3&gt;

&lt;p&gt;　　这里输出的信息太多了，看一个Robomongo的截图吧，里面有几个字段也是很重要的。 &lt;img src=&quot;../../images/mongo-db-serverStats.jpg&quot; alt=&quot;mongo-db-serverStatus&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;1.5 使用MongoDB自带的db.stats查看服务器状态&lt;/h3&gt;

&lt;p&gt;　　说到这里，我突然发现监控MongoDB performance的工具真的挺多的，完全可以自己给予这些命令和工具来开发后台管理的工具啊。比如说stats这个命令，也提供了挺多的信息：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; db.stats()                                        
{                                                   
        &quot;db&quot; : &quot;community&quot;,                         
        &quot;collections&quot; : 7,   
        // 记录在数据库中的所有文档总数                       
        &quot;objects&quot; : 73,                
        // 数据库中所有文档的平均大小，等于 dataSize/objects             
        &quot;avgObjSize&quot; : 24965.424657534248,          
        // 数据库所有文档的总大小，以字节为单位
        &quot;dataSize&quot; : 1822476,                  
        // 分配给每一个文档的磁盘空间，奇怪这里为什么不是16MB     
        &quot;storageSize&quot; : 11943936,                   
        &quot;numExtents&quot; : 12,                          
        &quot;indexes&quot; : 4,                              
        &quot;indexSize&quot; : 32704,                        
        &quot;fileSize&quot; : 201326592,                     
        &quot;nsSizeMB&quot; : 16,                            
        &quot;dataFileVersion&quot; : {                       
                &quot;major&quot; : 4,                        
                &quot;minor&quot; : 5                         
        },                                          
        &quot;ok&quot; : 1                                    
}                                                   
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;2. 优化Schema&lt;/h2&gt;

&lt;p&gt;　　解决一个问题永远都有多种方法，且在产品的不同时期也会有不同的解决办法。但核心观点都不变：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;理解产品的核心应用；&lt;/li&gt;
&lt;li&gt;合理平衡数据库的读写，读写比例很大程度上决定你的Schema设计；&lt;/li&gt;
&lt;li&gt;避免随机性IO操作；&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;　　即使是NoSQL，也无法避免一些数据库字段在关系上的建立。所以在设计NoSQL Schema的时候不可避免地要进行一些关系的处理。在MongoDB方面，处理关系有以下三种方法：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;数据库引用&lt;/li&gt;
&lt;li&gt;集合间的应用&lt;/li&gt;
&lt;li&gt;文档嵌套&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;　　其中数据库引用很少用到，并且官方也不推荐这种用法。相对于集合间引用和文档嵌套，这个需要看具体设计。特别是在应用文档嵌套方式的时候，需要注意一个文档的最大容量是&lt;a href=&quot;http://docs.mongodb.org/manual/core/document/&quot;&gt;16MB&lt;/a&gt;。文章末尾的&lt;strong&gt;6 Rules of Thumb for MongoDB Schema Design&lt;/strong&gt;详细介绍、对比了一些集合引用和嵌套文档方面的案例，&lt;strong&gt;MongoDB Schema Design: Four Real-World Examples&lt;/strong&gt;介绍了MongoDB 4个真实的应用设计案例，有比较大的参考价值。&lt;/p&gt;

&lt;h2&gt;2.1 MongoDB 和 RDBMS的一些概念联系&lt;/h2&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt; RDBMS &lt;/th&gt;
&lt;th&gt; MongoDB &lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt; Database &lt;/td&gt;
&lt;td&gt; Database &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; Table &lt;/td&gt;
&lt;td&gt; Collection &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; Row &lt;/td&gt;
&lt;td&gt; Document &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; Index &lt;/td&gt;
&lt;td&gt; Index &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; Join &lt;/td&gt;
&lt;td&gt; Embedded Document &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; Foreign Key &lt;/td&gt;
&lt;td&gt; Reference &lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;


&lt;h2&gt;3. 查询优化&lt;/h2&gt;

&lt;h3&gt;3.1 建立索引&lt;/h3&gt;

&lt;p&gt;　　首先要明确的是，mongo里会自动根据&lt;em&gt;id来创建一个唯一性索引，所以如果你是以&lt;/em&gt;id为key来进行查询的话都会很快的。比如下面这个截图，nscanned为1。
&lt;img src=&quot;../../images/mongo-index-id.jpg&quot; alt=&quot;mongo-explain&quot; /&gt;&lt;br/&gt;
　　database.collection.ensureIndex( { key : 1 } , { background : true } ); &lt;br/&gt;
　　说明：在数据库database里，对collection中的字段key建立索引，按照升序方式建立索引background参数设置为true时表示后台创建索引【建立索引略耗时】。  &lt;br/&gt;
　　索引是一把双刃剑啊，用得好不好，全看也许需求和数据库设计了，在设计索引前最好参考参考&lt;a href=&quot;http://docs.mongodb.org/manual/core/indexes-introduction/&quot;&gt;官方文档&lt;/a&gt;，而且最好要有一个建索引前后的performance的一些对比，talk is cheap, show me the data.  &lt;br/&gt;
　　总而言之，当需要建立索引的时候，一定要仔细思考下面几个方面：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;热点查询的数据是什么，可否用缓存替代；&lt;/li&gt;
&lt;li&gt;建立索引的顺序也会影响查询速度，参考: 10gen工程师谈MongoDB组合索引的优化；&lt;/li&gt;
&lt;li&gt;索引的更新周期，更新索引是一件很tricky的事情；&lt;/li&gt;
&lt;li&gt;索引建立后，是否能跟上后期系统扩展的脚步；&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;3.2 限制返回数据&lt;/h3&gt;

&lt;p&gt;　　使用limit，skip方式返回查询数据，只返回需要返回的数据。&lt;/p&gt;

&lt;h2&gt;4. 总结&lt;/h2&gt;

&lt;p&gt;　　在记录这篇文章的过程中，我发现监控mongo性能的工具还真的挺多的。但是现在我参与的产品中数据量还很少，还没有涉及到数据库这方面的优化，所以上面提到的这些都是自己在官网和一些优秀博客收集的资料。在后续涉及到自己优化这些查询的时候，我会再把实际经验和优化对比记录下来。&lt;/p&gt;

&lt;h2&gt;5. 一些资料&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://caizi.blog.51cto.com/5234706/1542480&quot;&gt;mongodb性能优化&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://blog.mongodb.org/post/87200945828/6-rules-of-thumb-for-mongodb-schema-design-part-1&quot;&gt;6 Rules of Thumb for MongoDB Schema Design&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.slideshare.net/friedo/data-modeling-examples&quot;&gt;MongoDB Schema Design: Four Real-World Examples&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://docs.mongodb.org/v2.4/administration/optimization/&quot;&gt;Optimization Strategies for MongoDB&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://docs.mongodb.org/v2.4/reference/database-profiler/&quot;&gt;Database Profiler Output&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.csdn.net/article/2012-11-09/2811690-optimizing-mongodb-compound&quot;&gt;10gen工程师谈MongoDB组合索引的优化&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://docs.mongodb.org/v2.4/reference/command/dbStats/&quot;&gt;dbStats&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
   <entry>
     <title>［touch spark］5. spark RDD 之：RDD Transformation</title>
     <link href="/rdd-transformations"/>
     <updated>2014-12-20T00:00:00+08:00</updated>
     <id>/rdd-transformations</id>
     <content type="html">&lt;p&gt;&lt;img src=&quot;../../images/transfomers.jpg&quot; alt=&quot;transfomers&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;&lt;/h2&gt;

&lt;h2&gt;1. 什么是RDD&lt;/h2&gt;

&lt;p&gt;　　关于什么是RDD，可以参考上一篇 &lt;a href=&quot;../spark-what-is-rdd&quot;&gt;4. spark RDD 之：什么是RDD&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;2. RDD transfomation 一览&lt;/h2&gt;

&lt;p&gt;　　ok，了解了RDD的含义，我们可以来看看神马叫transformation了，中文叫转换。上一篇提到RDD可以由两种方式创建，而在实际应用中，一般第一种方式都是用于新建一个RDD的时候，大多数时候都是通过第二种方式来生成一个新的RDD。so，想想这里我们应该怎么来根据一个已存在的RDD来transform出另一个新的RDD呢？当然就是根据一些规则，比如说筛选，映射，分组等等，而用于支撑这些规则的函数，就叫做RDD的transformation。&lt;br/&gt;
　　下面我们通过下面这张表来看看RDD都支持哪些规则的transformation吧。 这些是&lt;a href=&quot;http://spark.apache.org/docs/latest/programming-guide.html#transformations&quot;&gt;官方&lt;/a&gt;列出的一些常用的transformation，我原本想把所有transformation列出来的，可考虑到2/8原则，想想下面这些在实际应用中应该足够了。如果真的需要其他transformation的时候，相比彼时你的功力应该已到阅读、贡献源码的级别了。也就不需要再参考我下面将要写的东西了。 &lt;br/&gt;
　　这些transformation中有一些是我不太熟悉的，所以这里记录一下我不太熟悉的那些转换函数的用法，仅供个人参考哦。&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt; No &lt;/th&gt;
&lt;th&gt; Transformation  &lt;/th&gt;
&lt;th&gt;  Meaning &lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt; 1 &lt;/td&gt;
&lt;td&gt; map(func) &lt;/td&gt;
&lt;td&gt; Return a new distributed dataset formed by passing each element of the source through a function func.  &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; 2 &lt;/td&gt;
&lt;td&gt; filter(func) &lt;/td&gt;
&lt;td&gt; Return a new dataset formed by selecting those elements of the source on which func returns true. &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; 3 &lt;/td&gt;
&lt;td&gt; flatMap(func) &lt;/td&gt;
&lt;td&gt; Similar to map, but each input item can be mapped to 0 or more output items (so func should return a Seq rather than a single item). &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; 4 &lt;/td&gt;
&lt;td&gt; mapPartitions(func) &lt;/td&gt;
&lt;td&gt; Similar to map, but runs separately on each partition (block) of the RDD, so func must be of type Iterator&lt;T&gt; =&gt; Iterator&lt;U&gt; when running on an RDD of type T. &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; 5 &lt;/td&gt;
&lt;td&gt; mapPartitionsWithIndex(func) &lt;/td&gt;
&lt;td&gt; Similar to mapPartitions, but also provides func with an integer value representing the index of the partition, so func must be of type (Int, Iterator&lt;T&gt;) =&gt; Iterator&lt;U&gt; when running on an RDD of type T. &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; 6 &lt;/td&gt;
&lt;td&gt; sample(withReplacement, fraction, seed) &lt;/td&gt;
&lt;td&gt; Sample a fraction fraction of the data, with or without replacement, using a given random number generator seed. &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; 7 &lt;/td&gt;
&lt;td&gt; union(otherDataset) &lt;/td&gt;
&lt;td&gt; Return a new dataset that contains the union of the elements in the source dataset and the argument. &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; 8 &lt;/td&gt;
&lt;td&gt; intersection(otherDataset) &lt;/td&gt;
&lt;td&gt; Return a new RDD that contains the intersection of elements in the source dataset and the argument. &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; 9 &lt;/td&gt;
&lt;td&gt; distinct([numTasks])) &lt;/td&gt;
&lt;td&gt; Return a new dataset that contains the distinct elements of the source dataset. &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; 10 &lt;/td&gt;
&lt;td&gt; groupByKey([numTasks]) &lt;/td&gt;
&lt;td&gt; When called on a dataset of (K, V) pairs, returns a dataset of (K, Iterable&lt;V&gt;) pairs.   &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; 11 &lt;/td&gt;
&lt;td&gt; reduceByKey(func, [numTasks]) &lt;/td&gt;
&lt;td&gt; When called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function func, which must be of type (V,V) =&gt; V. Like in groupByKey, the number of reduce tasks is configurable through an optional second argument. &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; 12 &lt;/td&gt;
&lt;td&gt; aggregateByKey(zeroValue)(seqOp, combOp, [numTasks]) &lt;/td&gt;
&lt;td&gt; When called on a dataset of (K, V) pairs, returns a dataset of (K, U) pairs where the values for each key are aggregated using the given combine functions and a neutral &quot;zero&quot; value. Allows an aggregated value type that is different than the input value type, while avoiding unnecessary allocations. Like in groupByKey, the number of reduce tasks is configurable through an optional second argument. &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; 13 &lt;/td&gt;
&lt;td&gt; sortByKey([ascending], [numTasks]) &lt;/td&gt;
&lt;td&gt; When called on a dataset of (K, V) pairs where K implements Ordered, returns a dataset of (K, V) pairs sorted by keys in ascending or descending order, as specified in the boolean ascending argument. &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; 14 &lt;/td&gt;
&lt;td&gt; join(otherDataset, [numTasks]) &lt;/td&gt;
&lt;td&gt; When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs with all pairs of elements for each key. Outer joins are also supported through leftOuterJoin and rightOuterJoin. &lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;


&lt;h2&gt;3. RDD transfomation&lt;/h2&gt;

&lt;p&gt;　　RDD 的transformation有几个和scala里的函数组合子一样，其他的我估计也是基于scala的组合子来写的。所以，为了方便起见，能用scala来展现的，我就用scala来写；不可以的，我再用spark来写示例。下面提到的transformation更新到&lt;a href=&quot;http://spark.apache.org/docs/latest/programming-guide.html#transformations&quot;&gt;spark 1.1.1版本&lt;/a&gt;。要想查看最新版本的，可以上&lt;a href=&quot;http://spark.apache.org/&quot;&gt;官网&lt;/a&gt;和&lt;a href=&quot;http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD&quot;&gt;官方API文档&lt;/a&gt;。这里顺带提一下，说到transformation，网上几乎每篇说spark的文章都会用到下面这张图。可是我真的想说，这张图可是Matei这哥们2012年发的&lt;a href=&quot;https://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf&quot;&gt;RDD论文&lt;/a&gt;里的啊，spark更新的速度也许比大多数人换对象的速度还快啊，好多东西已经变了。大家以后要是发这张图，就该说明出处和版本吧，免得大家误解transformation和actions就那几种类型啊。&lt;br/&gt;
　　以下都是基于官方&lt;a href=&quot;http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD&quot;&gt;API DOC&lt;/a&gt;，在我本机上的测试以及网络资料整理而来。所有参考过的资料我都会在最后列出来，供参考。
&lt;img src=&quot;../../images/trans-action-2012.png&quot; alt=&quot;trans-action-2012&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;3.1 RDD transfomation － flatMap&lt;/h2&gt;

&lt;p&gt;　　说明：&lt;br/&gt;
　　flatMap可以理解成map和flat的组合。他处理一个嵌套列表，对其中每个列表中的元素执行map，然后对每个列表执行flat，最后返回一个列表。 &lt;br/&gt;
　　所以，我们也可以先flatten一个列表，再对列表里的每个元素做mapping；当然也可以对嵌套列表里的每个元素做mapping，再对列表做flatten。看下面的例子就明白了：&lt;/p&gt;

&lt;p&gt;　　定义：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def
flatMap[U](f: (T) ⇒ TraversableOnce[U])(implicit arg0: ClassTag[U]): RDD[U]
Return a new RDD by first applying a function to all elements of this RDD, and then flattening the results.   
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　示例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// flatMap operation
scala&amp;gt; val test = List(List(1,2,3,4,5), List(10,20,30,40,50))
test: List[List[Int]] = List(List(1, 2, 3, 4, 5), List(10, 20, 30, 40, 50))

scala&amp;gt; val test1 = test.flatMap( x =&amp;gt; x.map(_ * 2))
test1: List[Int] = List(2, 4, 6, 8, 10, 20, 40, 60, 80, 100)

// flat first, then mapping
scala&amp;gt; val tset2 = test.flatten
tset2: List[Int] = List(1, 2, 3, 4, 5, 10, 20, 30, 40, 50)

scala&amp;gt; val test3 = test.flatten.map(_*2)
test3: List[Int] = List(2, 4, 6, 8, 10, 20, 40, 60, 80, 100)

// mapping first, then flat
scala&amp;gt; val test4 = test.map( x =&amp;gt; x.map(_*2))
test4: List[List[Int]] = List(List(2, 4, 6, 8, 10), List(20, 40, 60, 80, 100))

scala&amp;gt; val test5 = test.map(x =&amp;gt; x.map(_*2)).flatten
test5: List[Int] = List(2, 4, 6, 8, 10, 20, 40, 60, 80, 100)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;3.2 RDD transfomation － mapPartitions&lt;/h2&gt;

&lt;p&gt;　　说明：&lt;br/&gt;
　　mapPartitions是map的一个变种。map的输入函数是应用于RDD中每个元素，而mapPartitions的输入函数是应用于每个分区，也就是把每个分区中的内容作为整体来处理的。&lt;/p&gt;

&lt;p&gt;　　定义：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def  
mapPartitions[U](f: (Iterator[T]) ⇒ Iterator[U], preservesPartitioning: Boolean = false)(implicit arg0: ClassTag[U]): RDD[U]
Return a new RDD by applying a function to each partition of this RDD.

preservesPartitioning indicates whether the input function preserves the partitioner, which should be false unless this is a pair RDD and the input function doesn&#39;t modify the keys.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　f即为输入函数，它处理每个分区里面的内容。每个分区中的内容将以Iterator[T]传递给输入函数f，f的输出结果是Iterator[U]。最终的RDD由所有分区经过输入函数处理后的结果合并起来的。&lt;/p&gt;

&lt;h2&gt;3.3 RDD transfomation － mapPartitionsWithIndex | mapPartitionsWithContext&lt;/h2&gt;

&lt;p&gt;　　说明： &lt;br/&gt;
　　mapPartitions还有些变种，比如mapPartitionsWithContext，它能把处理过程中的一些状态信息传递给用户指定的输入函数。还有mapPartitionsWithIndex，它能把分区的index传递给用户指定的输入函数。&lt;/p&gt;

&lt;p&gt;　　定义：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def
mapPartitionsWithContext[U](f: (TaskContext, Iterator[T]) ⇒ Iterator[U], preservesPartitioning: Boolean = false)(implicit arg0: ClassTag[U]): RDD[U]
Return a new RDD by applying a function to each partition of this RDD. This is a variant of mapPartitions that also passes the TaskContext into the closure.

preservesPartitioning indicates whether the input function preserves the partitioner, which should be false unless this is a pair RDD and the input function does not modify the keys.
Annotations

def
mapPartitionsWithIndex[U](f: (Int, Iterator[T]) ⇒ Iterator[U], preservesPartitioning: Boolean = false)(implicit arg0: ClassTag[U]): RDD[U]
Return a new RDD by applying a function to each partition of this RDD, while tracking the index of the original partition.

preservesPartitioning indicates whether the input function preserves the partitioner, which should be false unless this is a pair RDD and the input function doesn&#39;t modify the keys.
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;3.3 RDD transfomation －  sample&lt;/h2&gt;

&lt;p&gt;　　说明： &lt;br/&gt;
　　根据fraction指定的比例，对数据进行采样，可以选择是否用随机数进行替换，seed用于指定随机数生成器种子。这里我一直有一个疑问，当我的数据集里有100个元素是，设置fraction为0.1，按理应该是返回10个随机数的，可是就返回了6个。似乎返回的随机数会少于数据集元素数量*随机数的比例，晚点继续研究。&lt;/p&gt;

&lt;p&gt;　　定义：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def  sample(withReplacement: Boolean, fraction: Double, seed: Long = Utils.random.nextLong): RDD[T]
Return a sampled subset of this RDD.  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　示例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; val a = sc.parallelize(1 to 100)
a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[17] at parallelize at &amp;lt;console&amp;gt;:12

scala&amp;gt; val sample = a.sample(false, 0.1, 0)
sample: org.apache.spark.rdd.RDD[Int] = PartitionwiseSampledRDD[18] at sample at &amp;lt;console&amp;gt;:14

scala&amp;gt; sample.count
.
.
.
14/12/12 10:39:07 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool 
14/12/12 10:39:07 INFO SparkContext: Job finished: count at &amp;lt;console&amp;gt;:17, took 3.491275316 s
res29: Long = 6

scala&amp;gt; sample.collect
.
.
.
14/12/12 10:39:11 INFO DAGScheduler: Stage 24 (collect at &amp;lt;console&amp;gt;:17) finished in 0.142 s
14/12/12 10:39:11 INFO SparkContext: Job finished: collect at &amp;lt;console&amp;gt;:17, took 0.151221638 s
res30: Array[Int] = Array(22, 46, 48, 80, 87, 97)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　后话：&lt;br/&gt;
　　在看API DOC的时候发现有一个和sample很像的transformation，takeSample，但准确的说这并不是一个transformation，应该算是一个action吧。这个函数可以从数据集里返回固定数量的随机数，弥补上上面我提到的sample那个问题。但是需要注意的是，这个函数是直接在RDD上计算，返回计算结果，并不是一个transformation。看下面的例子就明白了。&lt;/p&gt;

&lt;p&gt;　　示例2：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; val takeSample = a.takeSample(false, 10, 0)
.
.
.
14/12/12 10:43:24 INFO DAGScheduler: Stage 26 (takeSample at &amp;lt;console&amp;gt;:14) finished in 0.068 s
14/12/12 10:43:24 INFO SparkContext: Job finished: takeSample at &amp;lt;console&amp;gt;:14, took 0.074973222 s
takeSample: Array[Int] = Array(68, 18, 97, 26, 61, 33, 67, 10, 2, 1)

scala&amp;gt; takeSample
res33: Array[Int] = Array(68, 18, 97, 26, 61, 33, 67, 10, 2, 1)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;3.4 RDD transfomation － union&lt;/h2&gt;

&lt;p&gt;　　说明：&lt;br/&gt;
　　这个把两个RDD合并为一个，很简单，可以理解成计算并集。但值得说明的是，union和++运算符是等价的，至少在函数定义上是完全一致的。先了解下，以后有需要的时候再看是否有细节上的区别。&lt;/p&gt;

&lt;p&gt;　　定义：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def
++(other: RDD[T]): RDD[T]
Return the union of this RDD and another one. Any identical elements will appear multiple times (use .distinct() to eliminate them).
def
union(other: RDD[T]): RDD[T]
Return the union of this RDD and another one. Any identical elements will appear multiple times (use .distinct() to eliminate them).
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　示例：&lt;br/&gt;
    scala&gt; a
    res37: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[17] at parallelize at &lt;console&gt;:12&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; val a = sc.parallelize(1 to 10)
a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[20] at parallelize at &amp;lt;console&amp;gt;:12

scala&amp;gt; val b = sc.parallelize(11 to 20)
b: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[21] at parallelize at &amp;lt;console&amp;gt;:12

scala&amp;gt; val c1 = a++b
c1: org.apache.spark.rdd.RDD[Int] = UnionRDD[22] at $plus$plus at &amp;lt;console&amp;gt;:16

scala&amp;gt; val c2 = a.union(b)

scala&amp;gt; c1.collect
.
.
.
14/12/12 11:01:13 INFO SparkContext: Job finished: collect at &amp;lt;console&amp;gt;:19, took 0.3475174 s
res38: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20)

scala&amp;gt; c2.collect
.
.
.
14/12/12 11:01:17 INFO SparkContext: Job finished: collect at &amp;lt;console&amp;gt;:19, took 0.122094635 s
res39: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;3.5 RDD transfomation － intersection&lt;/h2&gt;

&lt;p&gt;　　说明： &lt;br/&gt;
　　对比上面的union就很好理解了，上面是计算并集，这里是计算交集。这个transformation也有需要注意的地方，就是其有三种形式，根据是否提供第二个参数以及第二个参数的类型。具体可以参考函数定义。c++里这叫多态，现在对scala才是初学阶段，我想既然scala也是OO，那应该也有多态类似的概念吧，这里晚点继续补上。&lt;/p&gt;

&lt;p&gt;　　定义：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def
intersection(other: RDD[T], numPartitions: Int): RDD[T]
Return the intersection of this RDD and another one. The output will not contain any duplicate elements, even if the input RDDs did. Performs a hash partition across the cluster

Note that this method performs a shuffle internally.
numPartitions
How many partitions to use in the resulting RDD

def
intersection(other: RDD[T], partitioner: Partitioner)(implicit ord: Ordering[T] = null): RDD[T]
Return the intersection of this RDD and another one. The output will not contain any duplicate elements, even if the input RDDs did.

Note that this method performs a shuffle internally.
partitioner
Partitioner to use for the resulting RDD

def
intersection(other: RDD[T]): RDD[T]
Return the intersection of this RDD and another one. The output will not contain any duplicate elements, even if the input RDDs did.

Note that this method performs a shuffle internally.  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　示例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;val x = sc.parallelize(1 to 20)
val y = sc.parallelize(10 to 30)
val z = x.intersection(y)

z.collect
res74: Array[Int] = Array(16, 12, 20, 13, 17, 14, 18, 10, 19, 15, 11)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;3.6 RDD transfomation － distinct&lt;/h2&gt;

&lt;p&gt;　　说明： &lt;br/&gt;
　　即去重，相当于python里面的set。&lt;/p&gt;

&lt;p&gt;　　定义：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def
distinct(): RDD[T]
Return a new RDD containing the distinct elements in this RDD.
def
distinct(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T]
Return a new RDD containing the distinct elements in this RDD.
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;3.7 RDD transfomation － groupByKey&lt;/h2&gt;

&lt;p&gt;　　说明： &lt;br/&gt;
　　这是一个针对键值对结构来进行操作的转换方法，即你的RDD的结构是(key, value)类型，而其中key不是唯一性的。此时我们可以对这个RDD进行groupByKey的转换得到新的RDD，新的RDD的结构同样也是键值对，只是值改变了，并且键是唯一性的，即(key, iterator(value))。&lt;/p&gt;

&lt;p&gt;　　定义：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def
groupByKey(): RDD[(K, Iterable[V])]
Group the values for each key in the RDD into a single sequence. Hash-partitions the resulting RDD with the existing partitioner/parallelism level.

Note: This operation may be very expensive. If you are grouping in order to perform an aggregation (such as a sum or average) over each key, using PairRDDFunctions.aggregateByKey or PairRDDFunctions.reduceByKey will provide much better performance.
def
groupByKey(numPartitions: Int): RDD[(K, Iterable[V])]
Group the values for each key in the RDD into a single sequence. Hash-partitions the resulting RDD with into numPartitions partitions.

Note: This operation may be very expensive. If you are grouping in order to perform an aggregation (such as a sum or average) over each key, using PairRDDFunctions.aggregateByKey or PairRDDFunctions.reduceByKey will provide much better performance.
def
groupByKey(partitioner: Partitioner): RDD[(K, Iterable[V])]
Group the values for each key in the RDD into a single sequence. Allows controlling the partitioning of the resulting key-value pair RDD by passing a Partitioner.

Note: This operation may be very expensive. If you are grouping in order to perform an aggregation (such as a sum or average) over each key, using PairRDDFunctions.aggregateByKey or PairRDDFunctions.reduceByKey will provide much better performance.
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;3.8 RDD transfomation － reduceByKey&lt;/h2&gt;

&lt;p&gt;　　说明：  &lt;br/&gt;
　　和上面的groupByKey一样，这也是一个针对(key, value)型结构的RDD的转换函数。只是上面的groupByKey是将相同key对应的value组合成一个可迭代的对象；而reduceByKey是将相同key对应的value通过传入的函数func计算成一个新的value。&lt;/p&gt;

&lt;p&gt;　　定义：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def
reduceByKey(func: (V, V) ⇒ V): RDD[(K, V)]
Merge the values for each key using an associative reduce function. This will also perform the merging locally on each mapper before sending results to a reducer, similarly to a &quot;combiner&quot; in MapReduce. Output will be hash-partitioned with the existing partitioner/ parallelism level.
def
reduceByKey(func: (V, V) ⇒ V, numPartitions: Int): RDD[(K, V)]
Merge the values for each key using an associative reduce function. This will also perform the merging locally on each mapper before sending results to a reducer, similarly to a &quot;combiner&quot; in MapReduce. Output will be hash-partitioned with numPartitions partitions.
def
reduceByKey(partitioner: Partitioner, func: (V, V) ⇒ V): RDD[(K, V)]
Merge the values for each key using an associative reduce function. This will also perform the merging locally on each mapper before sending results to a reducer, similarly to a &quot;combiner&quot; in MapReduce.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　示例：　&lt;/p&gt;

&lt;h2&gt;3.9 RDD transfomation － aggregateByKey&lt;/h2&gt;

&lt;p&gt;　　说明：&lt;/p&gt;

&lt;p&gt;　　定义：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def
aggregateByKey[U](zeroValue: U)(seqOp: (U, V) ⇒ U, combOp: (U, U) ⇒ U)(implicit arg0: ClassTag[U]): RDD[(K, U)]
Aggregate the values of each key, using given combine functions and a neutral &quot;zero value&quot;. This function can return a different result type, U, than the type of the values in this RDD, V. Thus, we need one operation for merging a V into a U and one operation for merging two U&#39;s, as in scala.TraversableOnce. The former operation is used for merging values within a partition, and the latter is used for merging values between partitions. To avoid memory allocation, both of these functions are allowed to modify and return their first argument instead of creating a new U.
def
aggregateByKey[U](zeroValue: U, numPartitions: Int)(seqOp: (U, V) ⇒ U, combOp: (U, U) ⇒ U)(implicit arg0: ClassTag[U]): RDD[(K, U)]
Aggregate the values of each key, using given combine functions and a neutral &quot;zero value&quot;. This function can return a different result type, U, than the type of the values in this RDD, V. Thus, we need one operation for merging a V into a U and one operation for merging two U&#39;s, as in scala.TraversableOnce. The former operation is used for merging values within a partition, and the latter is used for merging values between partitions. To avoid memory allocation, both of these functions are allowed to modify and return their first argument instead of creating a new U.
def
aggregateByKey[U](zeroValue: U, partitioner: Partitioner)(seqOp: (U, V) ⇒ U, combOp: (U, U) ⇒ U)(implicit arg0: ClassTag[U]): RDD[(K, U)]
Aggregate the values of each key, using given combine functions and a neutral &quot;zero value&quot;. This function can return a different result type, U, than the type of the values in this RDD, V. Thus, we need one operation for merging a V into a U and one operation for merging two U&#39;s, as in scala.TraversableOnce. The former operation is used for merging values within a partition, and the latter is used for merging values between partitions. To avoid memory allocation, both of these functions are allowed to modify and return their first argument instead of creating a new U.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　示例：　&lt;/p&gt;

&lt;h2&gt;3.10 RDD transfomation － sortByKey&lt;/h2&gt;

&lt;p&gt;　　说明：  &lt;br/&gt;
　　按key来排序。但是在一些情况下，当RDD是(key, value)类型时，如果想对value来排序应该怎么处理呢？很简单，就是先把原来的(key, value)转换成(value, key)结构，然后对(value, key)进行sortByKey操作，最后再把已经排序了的(value, key)转换回(key, value)形式。&lt;/p&gt;

&lt;p&gt;　　定义：   &lt;br/&gt;
　　奇怪了，我再官方API DOC里没有找到这个转换函数的定义，晚一点再看一下。&lt;/p&gt;

&lt;p&gt;　　示例：　&lt;/p&gt;

&lt;h2&gt;3.11 RDD transfomation － join&lt;/h2&gt;

&lt;p&gt;　　说明：  &lt;br/&gt;
　　把两个(key, value)结构的RDD合成一个新的(key, value)结构的RDD。即(k1, v1).join((k1, v2)) =&gt; (k1, (v1, v2))，需要注意的是，为了让join操作成功，必须保证key是可以比较的。&lt;/p&gt;

&lt;p&gt;　　定义：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def
join[W](other: RDD[(K, W)], numPartitions: Int): RDD[(K, (V, W))]
Return an RDD containing all pairs of elements with matching keys in this and other. Each pair of elements will be returned as a (k, (v1, v2)) tuple, where (k, v1) is in this and (k, v2) is in other. Performs a hash join across the cluster.
def
join[W](other: RDD[(K, W)]): RDD[(K, (V, W))]
Return an RDD containing all pairs of elements with matching keys in this and other. Each pair of elements will be returned as a (k, (v1, v2)) tuple, where (k, v1) is in this and (k, v2) is in other. Performs a hash join across the cluster.
def
join[W](other: RDD[(K, W)], partitioner: Partitioner): RDD[(K, (V, W))]
Return an RDD containing all pairs of elements with matching keys in this and other. Each pair of elements will be returned as a (k, (v1, v2)) tuple, where (k, v1) is in this and (k, v2) is in other. Uses the given Partitioner to partition the output RDD.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　示例：　&lt;br/&gt;
　　&lt;/p&gt;

&lt;h2&gt;4，一些资源&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://blog.csdn.net/anzhsoft/article/details/39851421&quot;&gt;RDD&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://twitter.github.io/scala_school/zh_cn/collections.html#flatMap&quot;&gt;scala school from twitter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.zybuluo.com/jewes/note/35032&quot;&gt;RDD Reference 1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://homepage.cs.latrobe.edu.au/zhe/ZhenHeSparkRDDAPIExamples.html&quot;&gt;RDD API Examples&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD&quot;&gt;RDD API Docs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://spark.apache.org/docs/1.1.1/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions&quot;&gt;pairRDD API Docs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
   <entry>
     <title>［touch spark］4. spark RDD 之：什么是RDD</title>
     <link href="/spark-what-is-rdd"/>
     <updated>2014-12-08T00:00:00+08:00</updated>
     <id>/spark-what-is-rdd</id>
     <content type="html">&lt;h2&gt;&lt;/h2&gt;

&lt;h2&gt;1. 什么是RDD&lt;/h2&gt;

&lt;p&gt;　　先看下源码里是怎么描述RDD的。&lt;/p&gt;

&lt;blockquote&gt;&lt;blockquote&gt;&lt;p&gt;Internally, each RDD is characterized by five main properties:&lt;br/&gt;
A list of partitions&lt;br/&gt;
A function for computing each split &lt;br/&gt;
A list of dependencies on other RDDs&lt;br/&gt;
Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned) &lt;br/&gt;
Optionally, a list of preferred locations to compute each split on (e.g. block locations for an HDFS file)&lt;/p&gt;&lt;/blockquote&gt;&lt;/blockquote&gt;

&lt;p&gt;　　每个RDD有5个主要的属性：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;一组分片（partition），即数据集的基本组成单位&lt;/li&gt;
&lt;li&gt;一个计算每个分片的函数&lt;/li&gt;
&lt;li&gt;对parent RDD的依赖，这个依赖描述了RDD之间的lineage&lt;/li&gt;
&lt;li&gt;对于key-value的RDD，一个Partitioner，这是可选择的&lt;/li&gt;
&lt;li&gt;一个列表，存储存取每个partition的preferred位置。对于一个HDFS文件来说，存储每个partition所在的块的位置。这也是可选择的&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;　　把上面这5个主要的属性总结一下，可以得出RDD的大致概念。首先要知道，RDD大概是这样一种表示数据集的东西，它具有以上列出的一些属性。是spark项目组设计用来表示数据集的一种数据结构。而spark项目组为了让RDD能handle更多的问题，又规定RDD应该是只读的，分区记录的一种数据集合中。可以通过两种方式来创建RDD：一种是基于物理存储中的数据，比如说磁盘上的文件；另一种，也是大多数创建RDD的方式，即通过其他RDD来创建【以后叫做转换】而成。而正因为RDD满足了这么多特性，所以spark把RDD叫做Resilient Distributed Datasets，中文叫做弹性分布式数据集。很多文章都是先讲RDD的定义，概念，再来说RDD的特性。我觉得其实也可以倒过来，通过RDD的特性反过来理解RDD的定义和概念，通过这种由果溯因的方式来理解RDD也未尝不可。反正对我个人而言这种方式是挺好的。&lt;/p&gt;

&lt;h2&gt;2. 理解RDD的几个关键概念&lt;/h2&gt;

&lt;p&gt;　　本来我是想参考RDD的论文和自己的理解来整理这篇文章的，可是后来想想这样是不是有点过于细致了。我想，认识一个新事物，在时间、资源有限的情况下，不必锱铢必较，可以先focus on几个关键点，到后期应用的时候再步步深入。&lt;br/&gt;
　　所以，按照我个人的理解，我认为想用好spark，必须要理解RDD，而为了理解RDD，我认为只要了解下面几个RDD的几个关键点就能handle很多情况下的问题了。所以，下面所有列到的点，都是在我个人看来很重要的，但也许有所欠缺，大家如果想继续深入，可以看第三部分列出的参考资料，谢谢。
　　&lt;/p&gt;

&lt;h3&gt;2.1 RDD的背景及解决的痛点问题&lt;/h3&gt;

&lt;p&gt;　　按照RDD的paper来讲，RDD的设计是为了充分利用分布式系统中的内存资源，使得提升一些特定的应用的效率。这里所谓的特定的应用没有明确定义，但可以理解为一类应用到迭代算法，图算法等需要重复利用数据的应用类型；除此之外，RDD还可以应用在交互式大数据处理方面。所以，我们这里需要明确一下：RDD并不是万能的，也不是什么带着纱巾的少女那样神奇。简单的理解，就是一群大牛为了解决一个问题而设计的一个特定的数据结构，that&#39;s all。&lt;/p&gt;

&lt;h3&gt;2.2 What is DAG - 趣说有向无环图&lt;/h3&gt;

&lt;p&gt;　　DAG - Direct Acyclic Graph，有向五无图，好久没看图片了，先发个图片来理解理解吧。
&lt;img src=&quot;../../images/dag.jpg&quot; alt=&quot;DAG&quot; /&gt;&lt;br/&gt;
　　要理解DAG，只需弄明白三个概念就可以毕业了，首先，我们假设上图图二中的A,B,C,D,E都代表spark里不同的RDD：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;图：图是表达RDD Lineage信息的一个结构，在spark中，大部分RDD都是通过其他RDD进行转换而来的，比如说上图图二中，B和D都是通过A转换而来的，而C是通过B转换而来，E的话是通过B和D一起转换来的。&lt;/li&gt;
&lt;li&gt;有向：有向就更容易理解了，简单来说就是linage是一个top-down的结构，而且是时间序列上的top-down结构，这里不是很好理解，我们在下面讲“无环”这个概念是一起说明。&lt;/li&gt;
&lt;li&gt;&lt;p&gt;无环：这里就是重点要理解的地方了，我猜想spark的优化器在这里也发挥了很大的作用。首先，我们先理解一下无环的概念，假设有图三中左下B,D,E这样一个RDD转换图，那当我们的需要执行D.collect操作的时候，就会引发一个死循环了。不过，仔细想过的话，就会知道，“无环”这个问题其实已经在“有向”这个概念中提现了，上面说的“有向”，其实更详细的说是一个时间上的先来后到，即祖先与子孙的关系，是不可逆的。举个例子，我们按照时间序列分析一下图下左下的B,D,E三个RDD：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt; B通过某种方式初始化了第一个RDD【这里我们抛却A,C不谈】；&lt;/li&gt;
&lt;li&gt; D通过某种转换从B生成第二个RDD；&lt;/li&gt;
&lt;li&gt; E通过某种转换从D生成第三个RDD；&lt;/li&gt;
&lt;li&gt; 现在B这个RDD已经存在了，所以根本无从说明B是从E通过转换生成的，为啥，因为B已经存在了；&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;　　够清楚了吗，啥，还不够清楚，好，那我告诉你，B是小明他爷爷，D是小明他爸爸，E是小明自己，你说小明他爷爷能是小明通过某种方式转换出现在这个世界上的吗？&lt;/p&gt;

&lt;h3&gt;2.3 What is Data Locality - RDD的位置可见性&lt;/h3&gt;

&lt;p&gt;　　这个问题就不重复造轮子了，直接引用Quora上的一个&lt;a href=&quot;https://www.quora.com/How-do-I-make-clear-the-concept-of-RDD-in-Spark&quot;&gt;问答了&lt;/a&gt;。&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;RDD is a dataset which is distributed, that is, it is divided into &quot;partitions&quot;. Each of these partitions can be present in the memory or disk of different machines. If you want Spark to process the RDD, then Spark needs to launch one task per partition of the RDD. Its best that each task be sent to the machine have the partition that task is supposed to process. In that case, the task will be able to read the data of the partition from the local machine. Otherwise, the task would have to pull the partition data over the network from a different machine, which is less efficient. This scheduling of tasks (that is, allocation of tasks to machines) such that the tasks can read data &quot;locally&quot; is known as &quot;locality aware scheduling&quot;.&lt;/p&gt;&lt;/blockquote&gt;

&lt;h3&gt;2.4 What is Lazy Evaluation - 神马叫惰性求值&lt;/h3&gt;

&lt;p&gt;　　本来不想叫“惰性求值”的，看到“惰”这个字实在是各种不爽，实际上，我觉得应该叫&quot;后续求值&quot;，&quot;按需计算&quot;，&quot;晚点搞&quot;这类似的，哈哈。这几天一直在想应该怎么简单易懂地来表达Lazy Evaluation这个概念，本来打算引用MongoDB的Cursor来类比一下的，可总觉得还是小题大做了。这个概念就懒得解释了，主要是觉得太简单了，没有必要把事情搞得这么复杂，哈哈。&lt;/p&gt;

&lt;h3&gt;2.5 What is Narrow/Wide Dependency - RDD的宽依赖和窄依赖&lt;/h3&gt;

&lt;p&gt;　　首先，先从原文看看宽依赖和窄依赖各自的定义。&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;　　&lt;strong&gt;narrow dependencies&lt;/strong&gt;: where each partition of the parent RDD is used by at most one partition of the child RDD, &lt;strong&gt;wide dependencis&lt;/strong&gt;, where multiple child partitions may depend on it.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;　　按照&lt;a href=&quot;http://shiyanjun.cn/archives/744.html&quot;&gt;这篇RDD论文中文译文&lt;/a&gt;的解释，窄依赖是指子RDD的每个分区依赖于常数个父分区（即与数据规模无关）；宽依赖指子RDD的每个分区依赖于所有父RDD分区。暂且不说这样理解是否有偏差，我们先来从两个方面了解下计算一个窄依赖的子RDD和一个宽依赖的RDD时具体都有什么区别，然后再回顾这个定义。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;计算方面：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;计算窄依赖的子RDD：可以在某一个计算节点上直接通过父RDD的某几块数据（通常是一块）计算得到子RDD某一块的数据；&lt;/li&gt;
&lt;li&gt;计算宽依赖的子RDD：子RDD某一块数据的计算必须等到它的父RDD所有数据都计算完成之后才可以进行，而且需要对父RDD的计算结果进行hash并传递到对应的节点之上；&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;容错恢复方面：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;窄依赖：当父RDD的某分片丢失时，只有丢失的那一块数据需要被重新计算；&lt;/li&gt;
&lt;li&gt;宽依赖：当父RDD的某分片丢失时，需要把父RDD的所有分区数据重新计算一次，计算量明显比窄依赖情况下大很多；&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;3. 尚未提到的一些重要概念&lt;/h2&gt;

&lt;p&gt;　　还有一些基本概念上面没有提到，一些是因为自己还没怎么弄清楚，一些是觉得重要但是容易理解的，所以就先不记录下来了。比如说：粗粒度、细粒度；序列化和反序列化等。
　　&lt;/p&gt;

&lt;h2&gt;4. 参考资料&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://blog.csdn.net/anzhsoft/article/details/39851421&quot;&gt;Spark技术内幕：究竟什么是RDD&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf&quot;&gt;Resilient Distributed Datasets: A Fault-Tolerant Abstraction for
In-Memory Cluster Computing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://shiyanjun.cn/archives/744.html&quot;&gt;RDD 论文中文版&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;　　&lt;/p&gt;
</content>
   </entry>
   
   <entry>
     <title>［touch spark］3. 使用Spark分析wikipedia流量数据</title>
     <link href="/using-amazon-aws-2"/>
     <updated>2014-12-02T00:00:00+08:00</updated>
     <id>/using-amazon-aws-2</id>
     <content type="html">&lt;p&gt;　　本文是接上一篇的，所以序号就延续下来了。上一篇主要记录一些EC2配置和启动的问题，有兴趣请移步&lt;a href=&quot;../using-amazon-aws-1&quot;&gt;Amazon AWS EC2 入门&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;&lt;/h2&gt;

&lt;h2&gt;&lt;/h2&gt;

&lt;h2&gt;4. 利用spark来分析wikipedia流量数据&lt;/h2&gt;

&lt;p&gt;　　启动spark shell。路径在/root/spark/spark-shell。&lt;/p&gt;

&lt;h3&gt;4.1  热身&lt;/h3&gt;

&lt;p&gt;　　创建一个RDD，在spark-shell中，可以用sc代替SparkContext来创建RDD。这里需要注意一点，在Scala中有两种变量类型var和val，其中var是variable的缩写，val是value的缩写。顾名思义，var是可变的，val是不可变的。简单的可以把val理解成C/C++里的常量，或者Erlang里的变量【Erlang里的变量具有单次赋值的特征】。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;&quot;&gt;scala&amp;gt; var a =&quot;aaa&quot;
a: java.lang.String = aaa

scala&amp;gt; a = &quot;a&quot;
a: java.lang.String = a

scala&amp;gt; val b = &quot;aaa&quot;
b: java.lang.String = aaa

scala&amp;gt; b = &quot;a&quot;
&amp;lt;console&amp;gt;:12: error: reassignment to val
       b = &quot;a&quot;
         ^
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　这里我们需要用val来指定一个新建的RDD，原因有2：第一，我们不需要对RDD做in place的改变，所以可以采用val来指定；其次，我们不应该对RDD做in place的改变，所以必须采用val来指定。下面，我们新建一个val型pagecounts变量，读取wikipedia 20GB的流量数据，并以两种方式打印前3条数据。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;&quot;&gt;scala&amp;gt; val pagecounts = sc.textFile(&quot;/wiki/pagecounts&quot;)
14/12/04 05:58:35 INFO mapred.FileInputFormat: Total input paths to process : 74
pagecounts: spark.RDD[String] = spark.MappedRDD@2fddef87

scala&amp;gt; pagecounts.take(3)
14/12/04 05:58:49 INFO spark.SparkContext: Starting job...
14/12/04 05:58:49 INFO spark.CacheTracker: Registering RDD ID 1 with cache
14/12/04 05:58:49 INFO spark.CacheTrackerActor: Registering RDD 1 with 177 partitions
14/12/04 05:58:49 INFO spark.CacheTracker: Registering RDD ID 0 with cache
14/12/04 05:58:49 INFO spark.CacheTrackerActor: Registering RDD 0 with 177 partitions
14/12/04 05:58:49 INFO spark.CacheTrackerActor: Asked for current cache locations
14/12/04 05:58:49 INFO spark.MesosScheduler: Final stage: Stage 0
14/12/04 05:58:49 INFO spark.MesosScheduler: Parents of final stage: List()
14/12/04 05:58:49 INFO spark.MesosScheduler: Missing parents: List()
14/12/04 05:58:49 INFO spark.MesosScheduler: Computing the requested partition locally
14/12/04 05:58:49 INFO spark.SparkContext: Job finished in 0.098193078 s
14/12/04 05:58:49 INFO spark.SparkContext: Starting job...
14/12/04 05:58:49 INFO spark.CacheTrackerActor: Asked for current cache locations
14/12/04 05:58:49 INFO spark.MesosScheduler: Final stage: Stage 1
14/12/04 05:58:49 INFO spark.MesosScheduler: Parents of final stage: List()
14/12/04 05:58:49 INFO spark.MesosScheduler: Missing parents: List()
14/12/04 05:58:49 INFO spark.MesosScheduler: Computing the requested partition locally
14/12/04 05:58:49 INFO spark.SparkContext: Job finished in 0.026119526 s
res1: Array[String] = Array(20090505-000000 aa.b ?71G4Bo1cAdWyg 1 14463, 20090505-000000 aa.b Special:Statistics 1 840, 20090505-000000 aa.b Special:Whatlinkshere/MediaWiki:Returnto 1 1019)

scala&amp;gt; pagecounts.take(3).foreach(println)
14/12/04 05:59:16 INFO spark.SparkContext: Starting job...
14/12/04 05:59:16 INFO spark.CacheTrackerActor: Asked for current cache locations
14/12/04 05:59:16 INFO spark.MesosScheduler: Final stage: Stage 2
14/12/04 05:59:16 INFO spark.MesosScheduler: Parents of final stage: List()
14/12/04 05:59:16 INFO spark.MesosScheduler: Missing parents: List()
14/12/04 05:59:16 INFO spark.MesosScheduler: Computing the requested partition locally
14/12/04 05:59:16 INFO spark.SparkContext: Job finished in 0.004355182 s
14/12/04 05:59:16 INFO spark.SparkContext: Starting job...
14/12/04 05:59:16 INFO spark.CacheTrackerActor: Asked for current cache locations
14/12/04 05:59:16 INFO spark.MesosScheduler: Final stage: Stage 3
14/12/04 05:59:16 INFO spark.MesosScheduler: Parents of final stage: List()
14/12/04 05:59:16 INFO spark.MesosScheduler: Missing parents: List()
14/12/04 05:59:16 INFO spark.MesosScheduler: Computing the requested partition locally
14/12/04 05:59:16 INFO spark.SparkContext: Job finished in 0.016392708 s
20090505-000000 aa.b ?71G4Bo1cAdWyg 1 14463
20090505-000000 aa.b Special:Statistics 1 840
20090505-000000 aa.b Special:Whatlinkshere/MediaWiki:Returnto 1 1019
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;4.2 初试RDD Transfomation 和 RDD Action&lt;/h3&gt;

&lt;p&gt;　　下面，我们来演示一个RDD Transformation的例子。关于RDD Transformation，这篇有详细介绍和示例&lt;a href=&quot;../spark-transformers/&quot;&gt;spark RDD transformation 学习&lt;/a&gt;。首先，我们先看看这20GB的文件里有多少条数据，然后查询一下看所有流量数据中，有多少条是浏览的英文wiki。&lt;br/&gt;
　　首先，执行pagecounts.count来查看有多少条数据。这个动作会产生177个spark任务，这里是从HDFS读书数据，所以这个任务的瓶颈实在I/O这块，整个任务执行下来大概2~3分钟。这里我执行了几次，大概花了2分钟左右的时间，执行结果如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; pagecounts.count
.
.
.
14/12/05 01:19:58 INFO spark.SimpleJob: Finished TID 173 (progress: 177/177)
14/12/05 01:19:58 INFO spark.MesosScheduler: Completed ResultTask(0, 174)
14/12/05 01:19:58 INFO spark.SparkContext: Job finished in 95.251659404 s
res0: Long = 329641466
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　在任务运行的时候，可以打开web窗口访问：http://&lt;master_node_hostname&gt;:8080 来实时观察执行进度。下面是我的一个截图示例：&lt;br/&gt;
&lt;img src=&quot;../../images/mesos-cluster.png&quot; alt=&quot;mesos-cluster&quot; /&gt;&lt;/p&gt;

&lt;p&gt;　　现在，我们来利用现在这个RDD来trasform出另外一个RDD，用于记录英文wiki的数据。也通过把英文wiki的流量数据写到内存里，来比较一下数据在内存中和不在内存中两种情况下一些操作的耗时。这个测试需要下面4步：&lt;br/&gt;
　　1. 通过trasformation生成一个RDD[enPages]，记录英文wiki流量数据，因为这个步骤也需要遍历一边所有数据，所以这个步骤耗时也应该和上一个 pagecounts.count 耗时相当。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;&quot;&gt;scala&amp;gt; val enPages = pagecounts.filter(_.split(&quot; &quot;)(1) == &quot;en&quot;)
enPages: spark.RDD[String] = spark.FilteredRDD@1b8f2e35

scala&amp;gt; enPages.count
.
.
.
14/12/05 01:51:01 INFO spark.SparkContext: Job finished in 114.035390332 s
res1: Long = 122352588
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　2. 把enPages缓存到内存中&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;&quot;&gt;scala&amp;gt; enPages.cache
res0: spark.RDD[String] = spark.FilteredRDD@78bf34f4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　3. 执行enPages.count，看看执行速度有神马区别，what happened? 按照原计划，现在不应该是神速吗？仔细看看下面的执行log，是不是有一种恍然大悟的赶脚啊？&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;&quot;&gt;scala&amp;gt; enPages.count
.
.
.
14/12/05 01:59:33 INFO spark.SimpleJob: Size of task 0:176 is 10680 bytes and took 4 ms to serialize by spark.JavaSerializerInstance
14/12/05 01:59:33 INFO spark.CacheTrackerActor: Cache entry added: (2, 176) on ip-172-31-25-137.ec2.internal (size added: 16.0B, available: 6.0GB)
14/12/05 01:59:33 INFO spark.SimpleJob: Finished TID 176 (progress: 172/177)
14/12/05 01:59:33 INFO spark.MesosScheduler: Completed ResultTask(0, 176)
14/12/05 01:59:34 INFO spark.CacheTrackerActor: Cache entry added: (2, 170) on ip-172-31-25-139.ec2.internal (size added: 10.3MB, available: 5.0GB)
14/12/05 01:59:34 INFO spark.SimpleJob: Finished TID 169 (progress: 173/177)
14/12/05 01:59:34 INFO spark.MesosScheduler: Completed ResultTask(0, 170)
14/12/05 01:59:35 INFO spark.CacheTrackerActor: Cache entry added: (2, 172) on ip-172-31-25-137.ec2.internal (size added: 183.3MB, available: 5.8GB)
14/12/05 01:59:35 INFO spark.SimpleJob: Finished TID 171 (progress: 174/177)
14/12/05 01:59:35 INFO spark.MesosScheduler: Completed ResultTask(0, 172)
14/12/05 01:59:35 INFO spark.CacheTrackerActor: Cache entry added: (2, 175) on ip-172-31-25-138.ec2.internal (size added: 16.0B, available: 6.1GB)
14/12/05 01:59:35 INFO spark.SimpleJob: Finished TID 174 (progress: 175/177)
14/12/05 01:59:35 INFO spark.MesosScheduler: Completed ResultTask(0, 175)
14/12/05 01:59:36 INFO spark.CacheTrackerActor: Cache entry added: (2, 173) on ip-172-31-25-138.ec2.internal (size added: 16.0B, available: 6.1GB)
14/12/05 01:59:36 INFO spark.SimpleJob: Finished TID 172 (progress: 176/177)
14/12/05 01:59:36 INFO spark.MesosScheduler: Completed ResultTask(0, 173)
14/12/05 01:59:36 INFO spark.CacheTrackerActor: Cache entry added: (2, 174) on ip-172-31-25-139.ec2.internal (size added: 178.3MB, available: 4.8GB)
14/12/05 01:59:36 INFO spark.SimpleJob: Finished TID 173 (progress: 177/177)
14/12/05 01:59:36 INFO spark.MesosScheduler: Completed ResultTask(0, 174)
14/12/05 01:59:36 INFO spark.SparkContext: Job finished in 130.727017576 s
res0: Long = 122352588
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　4. 好，现在我们再次执行enPages.count，看看是不是有神马神奇的事情发生了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; enPages.count
.
.
.
14/12/05 02:12:01 INFO spark.SparkContext: Job finished in 2.492567199 s
res2: Long = 122352588
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　
　　哇，130秒和2.5秒的对决，心算一下，52倍啊，如果visualize一下这个数据，估计会更让人吃惊吧。擅于YY的我不禁用echarts画了个图，感受一下内存计算的神速。画图代码如下，直接把代码粘贴到&lt;a href=&quot;http://echarts.baidu.com/doc/example/bar1.html#macarons&quot;&gt;echarts bar&lt;/a&gt;，在再点击刷新就可以看到图了。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;&quot;&gt;option = {
    title : {
        text: &#39;enPages.count&#39;,
        subtext: &#39;by taotao.li&#39;
    },
    tooltip : {
        trigger: &#39;axis&#39;
    },
    legend: {
        data:[&#39;no cache&#39;,&#39;cache&#39;]
    },
    toolbox: {
        show : true,
        feature : {
            mark : {show: true},
            dataView : {show: true, readOnly: false},
            magicType : {show: true, type: [&#39;line&#39;, &#39;bar&#39;]},
            restore : {show: true},
            saveAsImage : {show: true}
        }
    },
    calculable : true,
    xAxis : [
        {
            type : &#39;category&#39;,
            data : [&#39;one time&#39;]
        }
    ],
    yAxis : [
        {
            type : &#39;value&#39;
        }
    ],
    series : [
        {
            name:&#39;no cache&#39;,
            type:&#39;bar&#39;,
            data:[130.727017576]
        },
        {
            name:&#39;cache&#39;,
            type:&#39;bar&#39;,
            data:[2.492567199]
        }
    ]
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;../../images/enPages-pic.png&quot; alt=&quot;enPages-pic&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;4.3 分析wikipedia的每日PV&lt;/h3&gt;

&lt;p&gt;　　重新温习一下&lt;a href=&quot;../using-amazon-aws-1&quot;&gt;上一篇末尾&lt;/a&gt;分析的wiki流量数据的格式如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;date_time: 以YYYYMMDD-HHMMSS格式表示的访问时间，且以小时为单位；&lt;/li&gt;
&lt;li&gt;project_code：表示对应的页面所使用的语言；&lt;/li&gt;
&lt;li&gt;page_title：表示访问的wiki标题；&lt;/li&gt;
&lt;li&gt;num_hits：表示从date_time起一小时内的浏览量；&lt;/li&gt;
&lt;li&gt;page_size： 表示以字节为单位，这个页面的大小；&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;　　OK，现在我们如果想要分析wiki流量的日PV，在上面5个字段中应该最关注的是date_time和num_hits吧。所以这里我们针对每一行数据创建一个key-value对，其中key是date_time，value是num_hits，在相加上相同的key对应的value就可以了。 下面我们把这些步骤拆开，一步一步分析，其中有些输出我就省略了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; val enTuples = enPages.map(line =&amp;gt; line.split(&quot; &quot;))
enTuples: spark.RDD[Array[java.lang.String]] = spark.MappedRDD@34ba89c5

scala&amp;gt; enTuples.take(5)
.
.
.
14/12/08 03:05:26 INFO spark.SparkContext: Job finished in 7.956625757 s
res3: Array[Array[java.lang.String]] = Array(Array(20090505-000000, en, !, 4, 170494), Array(20090505-000000, en, !!!, 21, 306957), Array(20090505-000000, en, !!!Fuck_You!!!, 9, 87025), Array(20090505-000000, en, !!!Fuck_You!!!_And_Then_Some, 2, 18249), Array(20090505-000000, en, !!!Fuck_You!!!_and_Then_Some, 2, 17960))


scala&amp;gt; val enKeyValuePairs = enTuples.map(line =&amp;gt; (line(0).substring(0, 8), line(3).toInt))
enKeyValuePairs: spark.RDD[(java.lang.String, Int)] = spark.MappedRDD@5e62a8d2

scala&amp;gt; enKeyValuePairs.take(5).foreach(println)
.
.
.
14/12/08 03:07:58 INFO spark.SparkContext: Job finished in 0.001414429 s
(20090505,4)
(20090505,21)
(20090505,9)
(20090505,2)
(20090505,2)

scala&amp;gt; val dailyPv = enKeyValuePairs.reduceByKey(_+_, 1)
dailyPv: spark.RDD[(java.lang.String, Int)] = spark.ShuffledRDD@50a934ec

scala&amp;gt; dailyPv.take(5).foreach(println)
.
.
.
14/12/08 03:13:18 INFO spark.SparkContext: Job finished in 26.776559986 s
(20090506,204190442)
(20090507,202617618)
(20090505,207698578)

scala&amp;gt; dailyPv.collect
.
.
.
14/12/08 03:13:45 INFO spark.SparkContext: Job finished in 0.145675681 s
res8: Array[(java.lang.String, Int)] = Array((20090506,204190442), (20090507,202617618), (20090505,207698578))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　最后的collect方法会把RDD 转换成scala里的数组。take(n)方法是取出前n条，因为这里我们就分析3天的数据，所以最多也只能取钱3天的，这里take(5)是看看这样会不会有什么错误提示呢。&lt;br/&gt;
　　上面我们大概用了3-4行语句来完成这个统计，这已经很强大了。而spark更强大的地方是它提供的编程模型，即transformation和action，虽然这些行为也就寥寥数十个，但已经足够处理大多数常见的问题了。比如说上面这个统计日PV的查询，在spark里其实完全可以把上面3-4行语句组合成一行语句，也就是说，在spark里，只有一行语句就可以统计当前wiki数据集下的日PV了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; enPages.map(line =&amp;gt; line.split(&quot; &quot;)).map(line =&amp;gt; (line(0).substring(0,8), line(3).toInt)).reduceByKey(_+_, 1).collect
.
.
.
14/12/08 03:25:03 INFO spark.SparkContext: Job finished in 27.144072883 s
res12: Array[(java.lang.String, Int)] = Array((20090506,204190442), (20090507,202617618), (20090505,207698578))
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;&lt;p&gt;　　&lt;strong&gt;可是老湿，你上面不是说只用一行语句就可以统计当前wiki数据集下的日PV的吗？可你这里用的是enPages啊！enPages不也是结果转换的吗，得把前几句加上吧？老湿，你骗我！！！&lt;/strong&gt;&lt;br/&gt;
&lt;img src=&quot;../../images/wawawa.gif&quot; alt=&quot;wawawa&quot; /&gt;&lt;/p&gt;

&lt;p&gt;　　&lt;strong&gt;同学，你问这个问题是不是刚才又写情书去了？既然enPages也是由其他RDD转换而来的，那这里不也可以把enPages替换成其他的RDD与与对应的transformation吗？&lt;/strong&gt;&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;../../images/laoshi.gif&quot; alt=&quot;laoshi&quot; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; pagecounts.filter(_.split(&quot; &quot;)(1) == &quot;en&quot;).map(line =&amp;gt; line.split(&quot; &quot;)).map(line =&amp;gt; (line(0).substring(0,8), line(3).toInt)).reduceByKey(_+_, 1).collect
 .
 .
 .
 14/12/08 04:33:52 INFO spark.SparkContext: Job finished in 151.78660518 s
res14: Array[(java.lang.String, Int)] = Array((20090506,204190442), (20090507,202617618), (20090505,207698578))
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;4.4 做点有趣的事情，看看哪些网页浏览次数最多&lt;/h3&gt;

&lt;p&gt;　　OK，其实分析每日PV已经是一个很有用的分析案例了。特别是长时间段的，比如说一周，一月，一季等，这些数据会让公司在容灾容错方面有很大启发。同样有用的是分析热点数据，即哪些页面是用户最常访问的，这个在缓存系统建立方面是绝对的关键啊。想一想，要是你把一个用户很少访问的页面放到缓存系统里，是不是既浪费了昂贵的缓存空间，又费力不讨好，简直是事倍功半啊。所以，现在我们就来做一件事，根据wiki这几日的流量数据，分析一下用户最常访问的wiki页面。&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;　　&lt;strong&gt;要不，我们先预测一下。我个人觉得，怎么说至少也应该有主页，帮助页面吧。&lt;/strong&gt;&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;　　当然，首先还是需要继续温习一下数据流量的格式啊：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;date_time: 以YYYYMMDD-HHMMSS格式表示的访问时间，且以小时为单位；&lt;/li&gt;
&lt;li&gt;project_code：表示对应的页面所使用的语言；&lt;/li&gt;
&lt;li&gt;page_title：表示访问的wiki标题；&lt;/li&gt;
&lt;li&gt;num_hits：表示从date_time起一小时内的浏览量；&lt;/li&gt;
&lt;li&gt;page_size： 表示以字节为单位，这个页面的大小；&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;　　既然我们要找到最常访问的热点数据，那就应该关注page_title和num_hits了。so，用分析日PV同样的思路，我们来分析一下热点数据：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; enPages.take(5)
.
.
.
14/12/08 04:59:48 INFO spark.SparkContext: Job finished in 9.6166E-4 s
res15: Array[String] = Array(20090505-000000 en ! 4 170494, 20090505-000000 en !!! 21 306957, 20090505-000000 en !!!Fuck_You!!! 9 87025, 20090505-000000 en !!!Fuck_You!!!_And_Then_Some 2 18249, 20090505-000000 en !!!Fuck_You!!!_and_Then_Some 2 17960)

scala&amp;gt; val enPageArray = enPages.map( l=&amp;gt;l.split(&quot; &quot;))
enPageArray: spark.RDD[Array[java.lang.String]] = spark.MappedRDD@27174693

scala&amp;gt; enPageArray.take(5)
.
.
.
14/12/08 05:02:28 INFO spark.SparkContext: Job finished in 0.001180471 s
res16: Array[Array[java.lang.String]] = Array(Array(20090505-000000, en, !, 4, 170494), Array(20090505-000000, en, !!!, 21, 306957), Array(20090505-000000, en, !!!Fuck_You!!!, 9, 87025), Array(20090505-000000, en, !!!Fuck_You!!!_And_Then_Some, 2, 18249), Array(20090505-000000, en, !!!Fuck_You!!!_and_Then_Some, 2, 17960))

scala&amp;gt; val enPageKeyValue = enPageArray.map(l =&amp;gt;(l(2), l(3).toInt))
enPageKeyValue: spark.RDD[(java.lang.String, Int)] = spark.MappedRDD@5b68b32

scala&amp;gt; enPageKeyValue.take(5)
.
.
.
14/12/08 05:03:54 INFO spark.SparkContext: Job finished in 0.00113825 s
res17: Array[(java.lang.String, Int)] = Array((!,4), (!!!,21), (!!!Fuck_You!!!,9), (!!!Fuck_You!!!_And_Then_Some,2), (!!!Fuck_You!!!_and_Then_Some,2))

scala&amp;gt; val keyValueUnion = enPageKeyValue.reduceByKey(_+_, 40)
keyValueUnion: spark.RDD[(java.lang.String, Int)] = spark.ShuffledRDD@7843f53

scala&amp;gt; keyValueUnion.take(5).foreach(println)
.
.
.
14/12/08 05:17:35 INFO spark.SparkContext: Job finished in 98.416456363 s
(Einst%C3%83%C2%BCrzende_Neubauten,2)
(Maxemail,1)
(Michael_Carl,4)
(Boothe_Homestead,1)
(File:The_Photographer.jpg,20)

scala&amp;gt; val valueKey = keyValueUnion.map(x=&amp;gt;(x._2, x._1))
valueKey: spark.RDD[(Int, java.lang.String)] = spark.MappedRDD@47a82a6a

scala&amp;gt; valueKey.take(5).foreach(println)

14/12/08 05:19:42 INFO spark.SparkContext: Job finished in 4.196323691 s
(2,Einst%C3%83%C2%BCrzende_Neubauten)
(4,Michael_Carl)
(1,Maxemail)
(1,Boothe_Homestead)
(20,File:The_Photographer.jpg)

scala&amp;gt; valueKey.sortByKey(false).take(5).foreach(println)
.
.
.
14/12/08 05:21:59 INFO spark.SparkContext: Job finished in 41.025906283 s
(43822489,404_error/)
(18730347,Main_Page)
(17657352,Special:Search)
(5816953,Special:Random)
(3521336,Special:Randompage)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　思路依然和分析每日PV是一样的，当然也可以组织成一行语句：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;val hotPage = enPages.map(l =&amp;gt; l.split(&quot; &quot;)).map(l =&amp;gt; (l(2), l(3).toInt)).reduceByKey(_+_, 40).map(x =&amp;gt; (x._2, x._1)).sortByKey(false).take(10).foreach(println)
.
.
.
14/12/08 09:57:30 INFO spark.SparkContext: Job finished in 41.232081196 s
(43822489,404_error/)
(18730347,Main_Page)
(17657352,Special:Search)
(5816953,Special:Random)
(3521336,Special:Randompage)
(695817,Cinco_de_Mayo)
(534253,Swine_influenza)
(464935,Wiki)
(396776,Dom_DeLuise)
(382510,Deadpool_(comics))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　好，这篇我们就先实践到这里。接下来体会一下shark的power，有兴趣的同志请移步&lt;a href=&quot;../using-amazon-aws-3-shark&quot;&gt;Spark Shark使用&lt;/a&gt;。&lt;/p&gt;
</content>
   </entry>
   
   <entry>
     <title>［touch spark］2. Amazon AWS EC2 入门</title>
     <link href="/using-amazon-aws-1"/>
     <updated>2014-12-01T00:00:00+08:00</updated>
     <id>/using-amazon-aws-1</id>
     <content type="html">&lt;h2&gt;&lt;/h2&gt;

&lt;h2&gt;1. 申请Amazon AWS账号&lt;/h2&gt;

&lt;p&gt;　　申请Amazon AWS需要绑定信用卡，无奈兄弟我从来没用过信用卡，所以只能跑到&lt;a href=&quot;https://www.globalcash.hk/&quot;&gt;global cash&lt;/a&gt;申请一张虚拟信用卡了。有关申请虚拟信用卡的教程&lt;a href=&quot;http://www.freehao123.com/globalcash/&quot;&gt;这里&lt;/a&gt;已经有了，我就不重复了。&lt;/p&gt;

&lt;h2&gt;2. 在EC2上创建一个spark集群&lt;/h2&gt;

&lt;h3&gt;2.1 前期准备&lt;/h3&gt;

&lt;p&gt;　　本文中用到的所有脚本都是基于python 2.x写的，且在Linux和0S X上测试通过。&lt;/p&gt;

&lt;h3&gt;2.2 创建EC2 keys&lt;/h3&gt;

&lt;p&gt;　　首先确保你的地区是US EAST，在右上角可以选择区域，即帐号名右侧。还没找到的请看下图：&lt;br/&gt;
&lt;img src=&quot;../../images/choose_ec2_region.png&quot; alt=&quot;choose_ec2_region&quot; /&gt;&lt;/p&gt;

&lt;p&gt;　　然后在帐号名-&gt;Security Credentials-&gt;Dashboard 下的 Details-&gt;Security Status-&gt;Manage Security Credentials-&gt;Access Keys-&gt;Create New Access Key创建keys，这里最好把keys记录下来，以后好用。&lt;br/&gt;
　　设置变量，下面的KEY_ID, ACCESS_KEY是在你创建keys的时候产生的：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;&quot;&gt;export AWS_ACCESS_KEY_ID=&amp;lt;ACCESS_KEY_ID&amp;gt;
export AWS_SECRET_ACCESS_KEY=&amp;lt;SECRET_ACCESS_KEY&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;2.3 创建key pair&lt;/h3&gt;

&lt;p&gt;　　在EC2 Dashboard左侧边栏-&gt;Network &amp;amp; Security-&gt;Key Pairs-&gt;Create Key Pair。这里会需要你输入一个key pair name，最好搞一个简单好记的，因为以后也会用到。创建成功后会自动下载一个用于后期验证登录的文件，下载该文件把其复制到用户家目录下，确保其权限至少是600，保险起见执行 chmod 600 key_pair_file。&lt;/p&gt;

&lt;h3&gt;2.4 下载启动脚本&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;&quot;&gt;git clone git://github.com/amplab/ampcamp.git  
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;2.5 建立并启动集群&lt;/h3&gt;

&lt;p&gt;　　若上面的启动脚本下载成功后，本地会有一个ampcamp的文件夹，cd 到ampcamp文件夹里，执行下面命令启动集群。其中key_file是刚刚下载并复制到家目录下的验证文件，name_of_key_pair是你创建key_pair的时候自己命名的。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;&quot;&gt;./spark-ec2 -i &amp;lt;key_file&amp;gt; -k &amp;lt;name_of_key_pair&amp;gt; --copy launch ampcamp
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　上面这个过程大约会持续15-20分钟，耐心等待一下。如果期间出现下面这个问题，那是因为没有把key_pair文件复制到家目录下去。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rsync: connection unexpectedly closed (0 bytes received so far) [sender]
rsync error: unexplained error (code 255) at io.c(605) [sender=3.0.9]
Traceback (most recent call last):
  File &quot;./spark_ec2.py&quot;, line 759, in &amp;lt;module&amp;gt;
    main()
  File &quot;./spark_ec2.py&quot;, line 648, in main
    setup_cluster(conn, master_nodes, slave_nodes, zoo_nodes, opts, True)
  File &quot;./spark_ec2.py&quot;, line 363, in setup_cluster
    deploy_files(conn, &quot;deploy.generic&quot;, opts, master_nodes, slave_nodes, zoo_nodes)
  File &quot;./spark_ec2.py&quot;, line 604, in deploy_files
    subprocess.check_call(command, shell=True)
  File &quot;/root/anaconda/lib/python2.7/subprocess.py&quot;, line 540, in check_call
    raise CalledProcessError(retcode, cmd)
subprocess.CalledProcessError: Command &#39;rsync -rv -e &#39;ssh -o StrictHostKeyChecking=no -i ../company.pem&#39; &#39;/tmp/tmp6YpLzV/&#39; &#39;root@ec2-54-172-219-206.compute-1.amazonaws.com:/&#39;&#39; returned non-zero exit status 255
root@ubuntu2:~/Desktop/spark/ampcamp# cp ../company.pem .
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　如果一切顺利（但愿），最后会有消息提示创建成功：SUCCESS: Cluster successfully launched! You can login to the master at ***&lt;/p&gt;

&lt;h3&gt;2.6 其他相关命令&lt;/h3&gt;

&lt;p&gt;　　第一个命令获取ampcamp集群的master节点，这个需要在集群启动成功后执行一次，因为后续也要用到这个节点地址，所以最好把master 节点地址记录下来。第二个命令是删除集群。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;&quot;&gt;./spark-ec2 -i &amp;lt;key_file&amp;gt; -k &amp;lt;key_pair&amp;gt; get-master ampcamp   
./spark-ec2 -i &amp;lt;key_file&amp;gt; -k &amp;lt;key_pair&amp;gt; destroy ampcamp  
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;3. 查看集群设置和数据准备&lt;/h2&gt;

&lt;h3&gt;3.1 获取master节点地址&lt;/h3&gt;

&lt;p&gt;　　在这个练习中，我们会用从&lt;a href=&quot;http://aws.amazon.com/datasets/4182&quot;&gt;http://aws.amazon.com/datasets/4182&lt;/a&gt;拿到的wikipedia的流量数据来做分析。&lt;br/&gt;
　　方便起见，AMP Camp已经提前把(May 5 to May 7, 2009; roughly 20G and 329 million entries)的数据准备好，并且预加载到集群里一个HDFS机器上了。这样我们就不用准备数据了，可以专注在体验spark特性的这件事上。&lt;/p&gt;

&lt;h3&gt;3.1 获取master节点地址&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;&quot;&gt;./spark-ec2 -i &amp;lt;key_file&amp;gt; -k &amp;lt;key_pair&amp;gt; get-master ampcamp  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　此时成功的话应该会提示你当前有一个master，3个slave，0个ZooKeeper。&lt;/p&gt;

&lt;h3&gt;3.2 使用ssh登录master节点&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;&quot;&gt;ssh -i &amp;lt;key_file&amp;gt; -l root &amp;lt;master_node_hostname&amp;gt;
or
ssh -i &amp;lt;key_file&amp;gt; root &amp;lt;master_node_hostname&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　需要注意的是，这里虽然你是登录到一个机器上，但实际是一个集群中。集群里有一个master节点，3个slave节点。其中你登录的地方是master节点，master节点主要负责任务分配和管理HDFS的元数据。其他的3个slave节点是计算节点，也就是真正运行任务的节点。&lt;br/&gt;
　　在master里，执行ls可以看到以下几个文件夹，下面列出比较重要的几个文件夹：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ephemeral-hdfs: Hadoop installation&lt;/li&gt;
&lt;li&gt;hive: Hive installation&lt;/li&gt;
&lt;li&gt;java-app-template: Some stand-alone Spark programs in java&lt;/li&gt;
&lt;li&gt;mesos: Mesos installation&lt;/li&gt;
&lt;li&gt;mesos-ec2: A suite of scripts to manage Mesos on EC2&lt;/li&gt;
&lt;li&gt;scala-2.9.1.final: Scala installation&lt;/li&gt;
&lt;li&gt;scala-app-template: Some stand-alone Spark programs in scala&lt;/li&gt;
&lt;li&gt;spark: Spark installation&lt;/li&gt;
&lt;li&gt;shark: Shark installation&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;　　可以在mesos-ec2/slaves文件里看到自己的3个slave节点地址：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@ip-172-31-22-240 ~]# cat mesos-ec2/slaves
ec2-54-174-175-127.compute-1.amazonaws.com
ec2-54-174-183-88.compute-1.amazonaws.com
ec2-54-174-124-52.compute-1.amazonaws.com
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　你的HDFS集群应该已经提前载入20GB的wikipedia数据文件了，可以到ephemeral-hdfs/bin/下执行hadoop fs -ls /wiki/pagecounts查看，这里应该是有74个文件，其中2个是空的。其中每一个文件是以小时为单位来保存的。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;&quot;&gt;[root@ip-172-31-22-240 ~]# ephemeral-hdfs/bin/hadoop fs -ls /wiki/pagecounts
Found 74 items
-rw-r--r--   3 root supergroup          0 2014-12-03 02:18 /wiki/pagecounts/part-00095
-rw-r--r--   3 root supergroup  244236879 2014-12-03 02:18 /wiki/pagecounts/part-00096
-rw-r--r--   3 root supergroup  233905016 2014-12-03 02:18 /wiki/pagecounts/part-00097
-rw-r--r--   3 root supergroup  225825888 2014-12-03 02:19 /wiki/pagecounts/part-00098
-rw-r--r--   3 root supergroup  225164279 2014-12-03 02:18 /wiki/pagecounts/part-00099
-rw-r--r--   3 root supergroup  228145848 2014-12-03 02:19 /wiki/pagecounts/part-00100
.            
.
.
-rw-r--r--   3 root supergroup  327382691 2014-12-03 02:26 /wiki/pagecounts/part-00163
-rw-r--r--   3 root supergroup  325471268 2014-12-03 02:27 /wiki/pagecounts/part-00164
-rw-r--r--   3 root supergroup  288288841 2014-12-03 02:27 /wiki/pagecounts/part-00165
-rw-r--r--   3 root supergroup  266179174 2014-12-03 02:29 /wiki/pagecounts/part-00166
-rw-r--r--   3 root supergroup  243451716 2014-12-03 02:18 /wiki/pagecounts/part-00167
-rw-r--r--   3 root supergroup          0 2014-12-03 02:19 /wiki/pagecounts/part-00168
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　其中，每个文件都以一行为单位记录，每行都符合模式：&lt;code&gt;&amp;lt;date_time&amp;gt; &amp;lt;project_code&amp;gt; &amp;lt;page_title&amp;gt; &amp;lt;num_hits&amp;gt; &amp;lt;page_size&amp;gt;&lt;/code&gt;。其中&lt;code&gt;&amp;lt;date_time&amp;gt;&lt;/code&gt;字段以YYYYMMDD-HHMMSS为时间格式，表示访问时间，且以小时为单位，所以只有YYYYMMDD-HH为有效数据，MMSS都为0，&lt;code&gt;&amp;lt;project_code&amp;gt;&lt;/code&gt;字段表示对应的页面所使用的语言，如&quot;en&quot;则表示英文；&lt;code&gt;&amp;lt;page_title&amp;gt;&lt;/code&gt;字段表示该页面在wiki上的标题，&lt;code&gt;&amp;lt;num_hits&amp;gt;&lt;/code&gt;表示从&lt;code&gt;&amp;lt;date_time&amp;gt;&lt;/code&gt;时间起一小时内的浏览量，&lt;code&gt;&amp;lt;page_size&amp;gt;&lt;/code&gt;表示以字节为单位，这个页面的大小。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;&quot;&gt;20090507-040000 aa Main_Page 7 51309
20090507-040000 aa Special:Boardvote 1 11631
20090507-040000 aa Special:Imagelist 1 931
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　下一篇会记录在EC2上用spark分析wikipedia流量的过程，请移步&lt;a href=&quot;../using-amazon-aws-2&quot;&gt;使用Spark分析wikipedia流量数据&lt;/a&gt;&lt;/p&gt;
</content>
   </entry>
   
   <entry>
     <title>［touch spark］1. 准备和初次体验spark</title>
     <link href="/learning-spark-step-1"/>
     <updated>2014-11-30T00:00:00+08:00</updated>
     <id>/learning-spark-step-1</id>
     <content type="html">&lt;h2&gt;&lt;/h2&gt;

&lt;h2&gt;1. 下载，安装，运行spark&lt;/h2&gt;

&lt;p&gt;　　下载最新版的&lt;a href=&quot;http://spark.apache.org/downloads.html&quot;&gt;spark&lt;/a&gt;. 我选择的是spark-1.1.1-bin-hadoop2.4.tgz。因为是预编译过的，所以没有所谓的“安装”环境，直接解压即可运行了。&lt;br/&gt;
　　运行路径是：spark-1.1.1-bin-hadoop2.4/bin，如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;chenshan@mac007 spark$ls
spark-1.1.1-bin-hadoop2.4.tgz
chenshan@mac007 spark$tar zxf spark-1.1.1-bin-hadoop2.4.tgz 
chenshan@mac007 spark$ls
chenshan@mac007 spark$cd spark-1.1.1-bin-hadoop2.4
chenshan@mac007 spark-1.1.1-bin-hadoop2.4$ls
CHANGES.txt NOTICE      RELEASE     conf        examples    python
LICENSE     README.md   bin         ec2         lib         sbin
chenshan@mac007 spark-1.1.1-bin-hadoop2.4$cd bin
chenshan@mac007 bin$ls
beeline               pyspark               run-example.cmd       spark-class2.cmd      spark-submit
compute-classpath.cmd pyspark.cmd           run-example2.cmd      spark-shell           spark-submit.cmd
compute-classpath.sh  pyspark2.cmd          spark-class           spark-shell.cmd       utils.sh
load-spark-env.sh     run-example           spark-class.cmd       spark-sql
chenshan@mac007 bin$./pyspark 
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;2. 设置环境变量&lt;/h2&gt;

&lt;p&gt;　　为了方便以后使用，最好把bin文件夹加入环境变量，关于环境变量的设置可以参考&lt;a href=&quot;http://www.tuicool.com/articles/7nu2E3R&quot;&gt;这里&lt;/a&gt;。编辑～/.bash_profile，在最后加上一句：export PATH=~/Desktop/spark/spark-1.1.1-bin-hadoop2.4/bin:$PATH，保存文件后退出，在执行 source ～/.bash_profile 或 . ～/.bash_profile 即可生效。&lt;br/&gt;
　　ok，现在可以随时随地在任何路径下执行pyspark启动spark了。因为默认是用python的console来启动的，这样也可以用，但在python console里编码多多少少有很多不便，对我个人而言就是一些ls/cd啊这些常用命令，以及查看每个对象但方法和属性和自动补全了。所以这里如果可以在ipython里启动spark的话不就完美了。先google了下，没什么结果。那就来看看pyspark这个脚本里是怎么写的吧，果不其然，在最后几行还真找到东西了：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  if [[ &quot;$IPYTHON&quot; = &quot;1&quot; ]]; then
    exec ipython $IPYTHON_OPTS
  else
    exec &quot;$PYSPARK_PYTHON&quot;
  fi
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　好，神秘钥匙找到了，看来我们需要设置一个IPYTHON的变量，且把该变量设置成1。ok，试一下:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;&quot;&gt;chenshan@mac007 bin$$IPTYHON
chenshan@mac007 bin$export IPYTHON=&quot;1&quot;
chenshan@mac007 bin$$IPTYHON
-bash: 1: command not found
chenshan@mac007 bin$pyspark 
Python 2.7.5 (default, Mar  9 2014, 22:15:05) 
Type &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.

IPython 2.1.0 -- An enhanced Interactive Python.
?         -&amp;gt; Introduction and overview of IPython&#39;s features.
%quickref -&amp;gt; Quick reference.
help      -&amp;gt; Python&#39;s own help system.
object?   -&amp;gt; Details about &#39;object&#39;, use &#39;object??&#39; for extra details.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　成功，我想要的效果达到了，看下面。后来才发现原来官方上面已经有这个说明了，只是感觉也说得不是很明确，看&lt;a href=&quot;http://spark.apache.org/docs/latest/programming-guide.html&quot;&gt;这里&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;In [1]: ls
README.md

In [2]: t = sc.textFile(&quot;R
README.md       ReferenceError  RuntimeError    RuntimeWarning  

In [2]: t = sc.textFile(&quot;README.md&quot;)
14/11/30 12:48:12 INFO MemoryStore: ensureFreeSpace(172851) called with curMem=0, maxMem=286300569
14/11/30 12:48:12 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 168.8 KB, free 272.9 MB)

In [3]: t.
t.aggregate                  t.foreachPartition           t.max                        t.setName
t.aggregateByKey             t.getCheckpointFile          t.mean                       t.sortBy
t.cache                      t.getNumPartitions           t.min                        t.sortByKey
t.cartesian                  t.getStorageLevel            t.name                       t.stats
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;3. 单机版的spark应用&lt;/h2&gt;

&lt;p&gt;　　官方的&lt;a href=&quot;http://spark.apache.org/docs/latest/quick-start.html&quot;&gt;quick start&lt;/a&gt;里已经有了一个单机版的spark应用的示例。很简单的例子，但我觉得还是有一个地方需要理解下才行。那就是运行python写但spark应用但方法，需要使用spark安装目录里bin可执行文件目录下但spark-submit来运行。我第一次运行这个示例的时候，很不知所谓地就python SimpleApp.py，然后自然而然地提示我没有pyspark这个包，我心想，没有这个包还不好办吗，直接pip install pyspark，可pip居然提示也没有这个包。我想难道是pip抽风了，那好，我就google下pyspark，按理说第一项应该就是pyspark在pip的网页了，我靠，可连google也没搜出什么东西来啊。这下我觉得应该是我的问题了。想了下，先启动pyspark来看看有没有这个包，居然是有的，这？执行pyspark.&lt;strong&gt;file&lt;/strong&gt;来看看这个包的路径，我才恍然大悟。原来是我短路了，spark官方都说了，人家自己提供了scala，java，python都接口，全部都在源码里了，还提供了一些基于这三种语言都examples。只是这个包没有放到pip里去，而是随spark一起发布的。我想是因为spark现在还不够成熟，基于python的接口也会跟着每一个小版本的spark发布而改变，所以暂时没把pyspark发布到pip上去吧。&lt;br/&gt;
　　所以，这里值得注意的是，一定要用spark-submit来运行python写的spark应用。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;&quot;&gt;In [13]: import pyspark

In [14]: pyspark.__file__
Out[14]: &#39;/Users/chenshan/Desktop/spark/spark-1.1.1-bin-hadoop2.4/python/pyspark/__init__.pyc&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　&lt;/p&gt;
</content>
   </entry>
   
   <entry>
     <title>SVN 中的branches, tags和trunk</title>
     <link href="/svn-branch-trunk-tags"/>
     <updated>2014-11-14T00:00:00+08:00</updated>
     <id>/svn-branch-trunk-tags</id>
     <content type="html">&lt;h2&gt;&lt;/h2&gt;

&lt;h2&gt;1. 直接上正文&lt;/h2&gt;

&lt;p&gt;　　正文就是，关于这点，&lt;a href=&quot;http://stackoverflow.com/questions/16142/what-do-branch-tag-and-trunk-mean-in-subversion-repositories&quot;&gt;SO&lt;/a&gt;已经有多答案了，下面我引用一下投票最多的答案。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Trunk&lt;/strong&gt;：would be the main body of development, originating from the start of the project until the present.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Branch&lt;/strong&gt;： will be a copy of code derived from a certain point in the trunk that is used for applying major changes to the code while preserving the integrity of the code in the trunk. If the major changes work according to plan, they are usually merged back into the trunk.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tag&lt;/strong&gt;： will be a point in time on the trunk or a branch that you wish to preserve. The two main reasons for preservation would be that either this is a major release of the software, whether alpha, beta, RC or RTM, or this is the most stable point of the software before major revisions on the trunk were applied.&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;2. 我是怎么理解的&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Trunk&lt;/strong&gt;: 你的现女友，你会付出很多东西在她身上，期待美好的结果，但时不时也会吵吵闹闹，出现一些意外情况；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Branches&lt;/strong&gt;：你的前女友们，按时间排序，她们都有很多相似的东西，那就是隐藏在你最深处的东西，你的Kernel，可以说是你的爱情观、人生观、事业观等等啦；&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tags&lt;/strong&gt;：让你记忆尤深的前女友们，她们可不像你这样善变，分手后依然爱你，甚至当你有一天被现女友携款潜逃将你抛弃后，你还能回去找她们，kiss她们的香唇。&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;3. 一些资源&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://tortoisesvn.net/docs/release/TortoiseSVN_zh_CN/index.html&quot;&gt;TortoiseSVN 1.8 中文文档&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://marklodato.github.io/visual-git-guide/index-zh-cn.html&quot;&gt;图解git命令&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
   <entry>
     <title>一个完整的 OTP 应用</title>
     <link href="/an-integrated-otp-application"/>
     <updated>2014-11-10T00:00:00+08:00</updated>
     <id>/an-integrated-otp-application</id>
     <content type="html">&lt;h2&gt;&lt;/h2&gt;

&lt;h2&gt;1. 写在前面&lt;/h2&gt;

&lt;p&gt;　　在实践 &lt;em&gt;Erlang and OTP in Action&lt;/em&gt; 的第六章的时候，确实遇到了不少问题，后来慢慢多看了几遍书，多阅读了几次源码，特别是阅读了 application 的源码后，才慢慢理解了这章所讲的这个应用。这个总结不会很长，原本就想简单记在书上的，但想到以后也许会有一些更新，还有一些流程图的绘制需要经过多次修改，就还是放到github上来了，说实话，真挺感谢github的，让我学习和管理代码如此方便。&lt;/p&gt;

&lt;h2&gt;2. 程序运行流程&lt;/h2&gt;

&lt;p&gt;　　下图是整个simple_cache应用运行的流程，我们先上图，然后在后面再说说重要的地方。
&lt;img src=&quot;../../images/simple_cache.jpg&quot; alt=&quot;simple_cache运行流程图&quot; /&gt;
　　
　　下面我们再说明一下各个模块各自的职责：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;sc_app: 应用行为模式的实现模块&lt;/li&gt;
&lt;li&gt;simple_cache: 用户API，应用的外部接口&lt;/li&gt;
&lt;li&gt;sc_store: 用于封装键和pid之间映射关系的模块&lt;/li&gt;
&lt;li&gt;sc_element: 缓存数据存储进程&lt;/li&gt;
&lt;li&gt;sc_sup: 根监督者实现模块&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;　　其中sc_app并没有出现在流程图中，因为流程图是展示应用已经被启动后的运行流程，而sc_app是用于启动应用的，这点会单独在下一节讨论。现在先focus在应用成功启动后的运行流程。下面我们分别以insert，lookup，delete过程单独看一下流程。&lt;/p&gt;

&lt;h3&gt;2.1 insert流程&lt;/h3&gt;

&lt;p&gt;　　假设需要缓存的键为 K，值为 V。用户在API接口调用 &lt;code&gt;simple_cache:insert(K, V)&lt;/code&gt;；simple_cache先调用sc_store:lookup(K) 检查这个键是否已经缓存，若缓存则更新键值为V，否则新建一个缓存进程。因为我们是第一次insert，所以会调用sc_element来新建一个进程，并将值 V 放到这个新建进程的状态里，这个状态被定义为一个记录&lt;code&gt;-record(state, {value, lease_time, start_time}).&lt;/code&gt;；即sc_element新建的进程状态state中的value字段存储的就是我们需要缓存的键值 v。sc_element新建的进程PID会被返回，且返回后会被组合成(K, PID), 然后调用sc_store:insert(K, PID)来存储键K和进程的PID。(K, PID) 是存储到ets表里的。
　　从上面可以看到，其实使用进程做了一次中转类似的操作。我们原本要缓存一对键值k－v，现在是将k和进程pid单一映射，存储在ets表中，然后将值v存储到进程到状态state里。这样，当要拿到一个缓存的键值k－v时，先到ets表中查找k对应到进程pid，然后根据再读取这个进程到状态state即可。这个过程可以参考下图：
&lt;img src=&quot;../../images/key-value-map.jpg&quot; alt=&quot;key-value-mapping&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;2.2 lookup流程&lt;/h3&gt;

&lt;p&gt;　　其实lookup流程上面也提到了，即先根据k查找对应到pid，然后在以模块到方式调用sc_element来查找pid对应到进程到状态。这里可能大家很容易混淆标注1中的sc_element和标注2中supervisor监督的一系列sc_element自进程。其实一开始我也很纳闷，supervisor下有这么多sc_element自进程，可当调用sc_element来查找进程pid当状态时，是调用哪个自进程呢？这里其实是一个比较容易犯的低级错误，当调用sc_element来读取进程pid的状态时，是将sc_element当作一个模块来调用，和supervisor监督下的sc_element自进程毛线关系都没有，根本就不是一个概念呀。&lt;/p&gt;

&lt;h3&gt;2.3 delete流程&lt;/h3&gt;

&lt;p&gt;　　至于delete流程，则非常简单了，调用流程如下：simple_cache:delete/1 -&gt; sc_store:lookup/1 -&gt; sc_element:delete/1 -&gt; gen_server:cast/2 -&gt; handle_cast/2.&lt;/p&gt;

&lt;h2&gt;3. OTP 应用的标准组织结构&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;doc  用于存放文档。如果文档时用EDoc生成，请将overview.edoc文件放在此处，其余的文件将会自动生成；&lt;/li&gt;
&lt;li&gt;ebin  用于存放编译后的代码(.beam文件)，包含应用元数据的.app文件也应放在此处；&lt;/li&gt;
&lt;li&gt;include  用于存放公共头文件。所有作为公共API的一部分的.hrl文件都应该放在这个目录中。仅用于你自己的代码之中且不打算公开的私有.hrl文件则应该与其它源码文件一起放；&lt;/li&gt;
&lt;li&gt;priv  用于存放各种需要随应用一起发布的其他内容。定位priv目录的方法很简单：调用code:priv_dir(&lt;application-name&gt;)，便会以字符串形式得到priv目录完整路径；&lt;/li&gt;
&lt;li&gt;src  存放应用源代码；&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;4. 应用元数据描述文件说明&lt;/h2&gt;

&lt;p&gt;　　&lt;a href=&quot;http://www.erlang.org/doc/man/app.html&quot;&gt;官方关于应用元数据描述文件的文档&lt;/a&gt;
　　我们的应用元数据如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{application, simple_cache,
    [{description, &quot;A simple caching system&quot;},
     {vsn, &quot;0.1.0&quot;},
     {modules, [sc_app,
                sc_sup]},
     {registered, [sc_sup]},
     {applications, [kernel, sasl, stdlib]},
     {mod, {sc_app, []}}}.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　应用元数据文件里其实是定义了一个Erlang项式，这里真想吐槽一下，感觉这样写好麻烦，Python 里的配置文件多方面啊，简单易用。而Erlang应用元数据里的这个Erlang项式，是一个三元组{application, ApplicationName, ApplicationConfigureList}. application表示用application来启动，ApplicationName代表这个应用的名字，应该和应用元数据文件的文件名是一样的，ApplicationConfigureList是应用描述应用配置的信息，是一个列表类型。下面我们看看这个配置列表里的一些信息说明：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;description：应用描述；&lt;/li&gt;
&lt;li&gt;vsn：应用版本，推荐&amp;lt;主版本号&gt;.&amp;lt;次版本号&gt;.&amp;lt;修正版本号&gt;的格式；&lt;/li&gt;
&lt;li&gt;modules：应用中的模块列表，erlang的systools会用这个列表中的模块来制作启动脚本和应用程序包；&lt;/li&gt;
&lt;li&gt;registered：需要在系统中注册的模块名字，常用于系统服务等场合，erlang的systools会检查这个列表里的模块是否有命名冲突；&lt;/li&gt;
&lt;li&gt;application：必须在应用启动前先行启动的所有应用。主动应用要求自己所依赖的所有应用在自己的生命周期开始之前先行启动并就绪，这个列表中的各个应用的顺序无关紧要；&lt;/li&gt;
&lt;li&gt;mod：告知OTP系统应该如何启动应用，该参数的值是一个元组，其内容为一个模块名和一些可选的启动参数；&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;　　还有其他很多东西，今天就先不讲了，过几天再补上。&lt;/p&gt;

&lt;h2&gt;5. 感想&lt;/h2&gt;

&lt;p&gt;　　理解这个OTP应用的关键是监督树的建立和sc_element作为模块来调用的方式。最近看了一些公司在实践erlang后又转向其他语言来实施项目的文章分享，心里确实对Erlang对前途不是很明确，但凡事都得多面对待，不能别人说不行你也说不行，不能过去说不行现在，将来也说不行。&lt;br/&gt;
　　下面附上最近了解的小米和Facebook在Erlang的实践中的一些探索，仅供参考：&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.infoq.com/cn/news/2014/11/xiaomi-1111-pushservice&quot;&gt;1. 小米推送服务从Erlang转到java&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;开发语言从Erlang 转为Java。 小米原来的消息系统是使用Erlang开发的，所以推送系统的第一版也是基于Erlang；但是Erlang的社区不够活跃，开发人员很难找，学习曲线陡，支持工具和类库少，所以后来开发团队选择了使用Java重新开发；迁移到Java后，对开发人员的要求降低，各种工具和类库较多，大大提高了开发效率。&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;http://www.quora.com/Why-was-Erlang-chosen-for-use-in-Facebook-chat&quot;&gt;2. 介绍FB一开始选用Erlang来开发聊天服务的原因&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.quora.com/When-did-Facebook-switch-away-from-using-Erlang-for-Facebook-Chat&quot;&gt;3. 介绍FB聊天服务从Erlang转向C++的原因&lt;/a&gt;&lt;/p&gt;
</content>
   </entry>
   
   <entry>
     <title>book-1. 50 Tips and Tricks for MongoDB Developers</title>
     <link href="/50-tips-and-tricks-for-mongodb-developer"/>
     <updated>2014-11-09T00:00:00+08:00</updated>
     <id>/50-tips-and-tricks-for-mongodb-developer</id>
     <content type="html">&lt;h2&gt;&lt;/h2&gt;

&lt;h2&gt;写在前面&lt;/h2&gt;

&lt;p&gt;　　book 开头的文章都是记录我个人的读书记录的，里面分情况会记录下面这些东西：目录，读书感想，技术分享和推荐等。如果只是写下目录，那大多是留给自己以后看的。我不会把书长篇大论地翻译成中文，只会写下能对我个人以后真正有用的东西。
　　这是一本讲mongodb实践的经验书籍，就66页，除去前面目录什么的，也就50来页，对我个人还是挺有用的，所以我写下其目录，供以后复习时用。没必要写什么读书笔记，毕竟大脑才是最好用的u盘。&lt;/p&gt;

&lt;h2&gt;目录&lt;/h2&gt;

&lt;blockquote&gt;&lt;ol&gt;
&lt;li&gt;Duplicate data for speed, reference data for integrity&lt;/li&gt;
&lt;li&gt;Normalize if you need to future-proof data&lt;/li&gt;
&lt;li&gt;Try to fetech data in a single query&lt;/li&gt;
&lt;li&gt;Embed dependent fields&lt;/li&gt;
&lt;li&gt;Embed &quot;point-in-time&quot; data&lt;/li&gt;
&lt;li&gt;Do not embed fields that have unbound growth&lt;/li&gt;
&lt;li&gt;Pre-populate anything you can&lt;/li&gt;
&lt;li&gt;Preallocate space, whenever possible&lt;/li&gt;
&lt;li&gt;Store embedded information in arrays for anonymous access&lt;/li&gt;
&lt;li&gt;Desigin documents to be self-sufficient&lt;/li&gt;
&lt;li&gt;Prefer $-operators to JavaScript&lt;/li&gt;
&lt;li&gt;Compute aggregations as you go&lt;/li&gt;
&lt;li&gt;Write code to handle data integrity issues&lt;/li&gt;
&lt;li&gt;Use the correct types&lt;/li&gt;
&lt;li&gt;Override _id when you have your own simple, unique id&lt;/li&gt;
&lt;li&gt;Avoid using a document for _id&lt;/li&gt;
&lt;li&gt;Do not use database references&lt;/li&gt;
&lt;li&gt;Don&#39;t use GridFS for small binary data&lt;/li&gt;
&lt;li&gt;Handle &quot;seamless&quot; failover&lt;/li&gt;
&lt;li&gt;Handle replica set failure and failover&lt;/li&gt;
&lt;li&gt;Minimize disk access&lt;/li&gt;
&lt;li&gt;Use indexes to do more with less memory&lt;/li&gt;
&lt;li&gt;Don&#39;t always use an index&lt;/li&gt;
&lt;li&gt;Create indexes that cover your queries&lt;/li&gt;
&lt;li&gt;Use compound indexes to make multiple queries fast&lt;/li&gt;
&lt;li&gt;Create hierarchical documents for faster scans&lt;/li&gt;
&lt;li&gt;AND-queries should match as little as possible as fast as possible&lt;/li&gt;
&lt;li&gt;OR-queries should match as much as possible as soon as possible&lt;/li&gt;
&lt;li&gt;Write to the journal for single server, replicas for multiserver&lt;/li&gt;
&lt;li&gt;Always use replication, journaling, or both&lt;/li&gt;
&lt;li&gt;Do not depend on repair to recover data&lt;/li&gt;
&lt;li&gt;Understand getlasterror&lt;/li&gt;
&lt;li&gt;Always use safe writes in development&lt;/li&gt;
&lt;li&gt;Use w with replication&lt;/li&gt;
&lt;li&gt;Always use wtimeout with w&lt;/li&gt;
&lt;li&gt;Don&#39;s use fsync on every write&lt;/li&gt;
&lt;li&gt;Start up normally after a crash&lt;/li&gt;
&lt;li&gt;Take instant-in-time backups of durable servers&lt;/li&gt;
&lt;li&gt;Manually clean up your chunks collections&lt;/li&gt;
&lt;li&gt;Compact database with repair&lt;/li&gt;
&lt;li&gt;Don&#39;t change the number of votes for members of a replic set&lt;/li&gt;
&lt;li&gt;Replica sets can be reconfigured without a master up&lt;/li&gt;
&lt;li&gt;--shardsvr and --configsvr aren&#39;t required&lt;/li&gt;
&lt;li&gt;Only use --notablescan in development&lt;/li&gt;
&lt;li&gt;Learn some JavaScript&lt;/li&gt;
&lt;li&gt;Manage all of your servers and databases from one shell&lt;/li&gt;
&lt;li&gt;Get &quot;help&quot; for any function&lt;/li&gt;
&lt;li&gt;Create startup files&lt;/li&gt;
&lt;li&gt;Add your own functions&lt;/li&gt;
&lt;li&gt;Use a single connection to read your own writes&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;

&lt;h2&gt;推荐资料&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://blog.51cto.com/zt/107&quot;&gt;中文翻译&lt;/a&gt;&lt;/p&gt;
</content>
   </entry>
   

</feed>


</body>
</html>
