<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8" />
  <title>Taotao's Zone</title>
  <meta name="renderer" content="webkit">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="mobile-web-app-capable" content="yes">
  <link rel="stylesheet" href="/css/font-awesome/css/font-awesome.min.css" type="text/css" />
  <!-- <link rel="stylesheet" href="/css/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="/css/default.css" type="text/css" />
  <link rel="stylesheet" href="/css/desktop.css" type="text/css" />
  <link rel="stylesheet" href="/css/mobile.css" type="text/css" />
  <link rel="shortcut icon" href="/css/favicon.ico" type="image/x-icon" />
  <link rel="icon" href="/css/favicon.ico" mce_href="/favicon.ico" type="image/x-icon">
  <link rel="alternate" type="application/atom+xml" title="Recent Entries" href="/atom.xml" />
  <script src="/js/jquery-1.11.0.min.js" type="text/javascript"></script>
  <script src="/js/jquery-migrate-1.2.1.js" type="text/javascript"></script>
  <script src="/js/jquery.transit.min.js" type="text/javascript"></script>
  <script src="/js/common.js" type="text/javascript"></script>
</head>
<body>
  <?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

   <title>Taotao's Zone</title>
   <link href="http://litaotao.github.io/atom.xml" rel="self" type="application/atom+xml"/>
   <link href="http://litaotao.github.io" rel="alternate" type="text/html" />
   <updated>2016-04-12T10:58:11+08:00</updated>
   <id>http://litaotao.github.io</id>
   <author>
     <name></name>
     <email></email>
   </author>

   
   <entry>
     <title>『 Spark 』7. 使用 Spark DataFrame 进行大数据分析</title>
     <link href="/spark-dataframe-introduction"/>
     <updated>2016-03-30T00:00:00+08:00</updated>
     <id>/spark-dataframe-introduction</id>
     <content type="html">&lt;h2 id=&quot;section&quot;&gt;写在前面&lt;/h2&gt;

&lt;p&gt;本系列是综合了自己在学习spark过程中的理解记录 ＋ 对参考文章中的一些理解 ＋ 个人实践spark过程中的一些心得而来。写这样一个系列仅仅是为了梳理个人学习spark的笔记记录，所以一切以能够理解为主，没有必要的细节就不会记录了，而且文中有时候会出现英文原版文档，只要不影响理解，都不翻译了。若想深入了解，最好阅读参考文章和官方文档。&lt;/p&gt;

&lt;p&gt;其次，本系列是基于目前最新的 spark 1.6.0 系列开始的，spark 目前的更新速度很快，记录一下版本好还是必要的。 &lt;br /&gt;
最后，如果各位觉得内容有误，欢迎留言备注，所有留言 24 小时内必定回复，非常感谢。   &lt;br /&gt;
Tips: 如果插图看起来不明显，可以：1. 放大网页；2. 新标签中打开图片，查看原图哦。&lt;/p&gt;

&lt;h2 id=&quot;spark-dataframe&quot;&gt;1. 什么是 spark dataframe&lt;/h2&gt;

&lt;p&gt;先来看看官方原汁原味的文档是怎么介绍的：&lt;/p&gt;

&lt;p&gt;A DataFrame is &lt;code class=&quot;highlighter-rouge&quot;&gt;a distributed collection of data&lt;/code&gt; organized into named columns. It is conceptually equivalent to a &lt;code class=&quot;highlighter-rouge&quot;&gt;table in a relational database&lt;/code&gt; or a data frame in R/Python, but with &lt;code class=&quot;highlighter-rouge&quot;&gt;richer optimizations&lt;/code&gt; under the hood. DataFrames can be constructed from a wide array of sources such as: &lt;code class=&quot;highlighter-rouge&quot;&gt;structured data files, tables in Hive, external databases, or existing RDDs&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;我们可以看到 spark dataframe 的几个关键点：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;分布式的数据集&lt;/li&gt;
  &lt;li&gt;类似关系型数据库中的table，或者 excel 里的一张 sheet，或者 python/R 里的 dataframe&lt;/li&gt;
  &lt;li&gt;拥有丰富的操作函数，类似于 rdd 中的算子&lt;/li&gt;
  &lt;li&gt;一个 dataframe 可以被注册成一张数据表，然后用 sql 语言在上面操作&lt;/li&gt;
  &lt;li&gt;丰富的创建方式
    &lt;ul&gt;
      &lt;li&gt;已有的RDD&lt;/li&gt;
      &lt;li&gt;结构化数据文件&lt;/li&gt;
      &lt;li&gt;JSON数据集&lt;/li&gt;
      &lt;li&gt;Hive表&lt;/li&gt;
      &lt;li&gt;外部数据库&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;spark-dataframe-1&quot;&gt;2. 为什么要用 spark dataframe&lt;/h2&gt;

&lt;p&gt;DataFrame API 是在 R 和 Python data frame 的设计灵感之上设计的，具有以下功能特性：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;从KB到PB级的数据量支持；&lt;/li&gt;
  &lt;li&gt;多种数据格式和多种存储系统支持；&lt;/li&gt;
  &lt;li&gt;通过Spark SQL 的 Catalyst优化器进行先进的优化，生成代码；&lt;/li&gt;
  &lt;li&gt;通过Spark无缝集成所有大数据工具与基础设施；&lt;/li&gt;
  &lt;li&gt;为Python、Java、Scala和R语言（SparkR）API；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;简单来说，dataframe 能够更方便的操作数据集，而且因为其底层是通过 spark sql 的 Catalyst优化器生成优化后的执行代码，所以其执行速度会更快。总结下来就是，使用 spark dataframe 来构建 spark app，能：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;write less : 写更少的代码&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;do more : 做更多的事情&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;faster : 以更快的速度&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;dataframe&quot;&gt;3. 创建 dataframe&lt;/h2&gt;

&lt;p&gt;因为 spark sql，dataframe，datasets 都是共用 spark sql 这个库的，三者共享同样的代码优化，生成以及执行流程，所以 sql，dataframe，datasets 的入口都是 sqlContext。可用于创建 spark dataframe 的数据源有很多，我们就讲最简单的从结构化文件创建 dataframe。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/spark-dataframe-3.jpg&quot; alt=&quot;spark-dataframe-3.jpg&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;step 1 : 创建 sqlContext&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;下面是我自己创建 spark sc 都模版：&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;sc_conf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SparkConf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sc_conf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setAppName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;03-DataFrame-01&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sc_conf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setMaster&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SPARK_MASTER&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sc_conf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;spark.executor.memory&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;2g&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sc_conf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;spark.logConf&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sc_conf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getAll&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sleep&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;except&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SparkContext&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sc_conf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sqlContext&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SQLContext&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;step 2 : 创建 dataframe，从 json 文件&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;数据文件说明：中国 A 股上市公司基本信息，可以在这里取到：&lt;a href=&quot;http://pan.baidu.com/s/1pLxN851&quot;&gt;stock_5.json&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/spark-dataframe-1.jpg&quot; alt=&quot;spark-dataframe-1.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;注：这里的 json 文件并不是标准的 json 文件，spark 目前也不支持读取标准的 json 文件。你需要预先把标准的 json 文件处理成 spark 支持的格式: 每一行是一个 json 对象。&lt;/p&gt;

&lt;p&gt;比如说，官网的 &lt;code class=&quot;highlighter-rouge&quot;&gt;people.json&lt;/code&gt; 这个例子，它要求的格式是：&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Yin&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;address&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:{&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;city&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Columbus&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;state&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Ohio&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Michael&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;address&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:{&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;city&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;null&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;state&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;California&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;但对这个文件来看，标准的 json 格式只有下面两种：&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Yin&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Michael&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
 &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;address&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;city&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Columbus&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;state&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Ohio&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
    &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;city&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;null&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;state&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;California&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
  &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;###&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;err&quot;&gt;或者&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Yin&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;address&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:{&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;city&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Columbus&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;state&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Ohio&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}},&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Michael&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;address&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:{&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;city&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;null&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;state&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;California&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;所以在用 spark sql 来读取一个 json 文件的时候，务必要提前处理好 json 的文件格式，这里我们已经提前处理好了，文件如下所示：&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-json&quot; data-lang=&quot;json&quot;&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;ticker&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;000001&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;tradeDate&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;2016-03-30&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;exchangeCD&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;XSHE&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;secShortName&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;\u5e73\u5b89\u94f6\u884c&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;preClosePrice&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;10.43&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;openPrice&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;10.48&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;dealAmount&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;19661&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;turnoverValue&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;572627417.1299999952&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;highestPrice&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;10.7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;lowestPrice&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;10.47&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;closePrice&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;10.7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;negMarketValue&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;126303384220.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;marketValue&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;153102835340.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;isOpen&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;secID&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;000001.XSHE&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;listDate&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;1991-04-03&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;ListSector&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;\u4e3b\u677f&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;totalShares&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;14308676200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;ticker&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;000002&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;tradeDate&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;2016-03-30&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;exchangeCD&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;XSHE&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;secShortName&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;\u4e07\u79d1A&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;preClosePrice&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;24.43&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;openPrice&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;dealAmount&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;turnoverValue&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;highestPrice&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;lowestPrice&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;closePrice&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;24.43&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;negMarketValue&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;237174448154.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;marketValue&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;269685994760.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;isOpen&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;secID&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;000002.XSHE&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;listDate&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;1991-01-29&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;ListSector&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;\u4e3b\u677f&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&quot;totalShares&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;11039132000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;### df is short for dataframe&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sqlContext&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;json&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;hdfs://10.21.208.21:8020/user/mercury/stock_5.json&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;printSchema&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;select&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;ticker&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;secID&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;tradeDate&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;listDate&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;openPrice&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;closePrice&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                 &lt;span class=&quot;s&quot;&gt;&#39;highestPrice&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;lowestPrice&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;isOpen&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;../images/spark-dataframe-2.jpg&quot; alt=&quot;spark-dataframe-2.jpg&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;dataframe-1&quot;&gt;4. 操作 dataframe&lt;/h2&gt;

&lt;p&gt;同 rdd 一样，dataframe 也有很多专属于自己的算子，用于操作整个 dataframe 数据集，我们以后都简称为 dataframe api 吧，用 &lt;code class=&quot;highlighter-rouge&quot;&gt;算子&lt;/code&gt;， &lt;code class=&quot;highlighter-rouge&quot;&gt;DSL&lt;/code&gt; 这类的称呼对不熟悉的人来说不易理解，下面这里是完整的 api 列表：&lt;a href=&quot;http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame&quot;&gt;spark dataframe api&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;dataframe--sql-&quot;&gt;4.1 在 dataframe 上执行 sql 语句&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;../images/spark-dataframe-4.jpg&quot; alt=&quot;spark-dataframe-4.jpg&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;spark-dataframe--pandas-dataframe-&quot;&gt;4.2 spark dataframe 与 pandas dataframe 转换&lt;/h3&gt;

&lt;p&gt;一图胜千言啊：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/spark-dataframe-6.jpg&quot; alt=&quot;spark-dataframe-6.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;纵观 spark 的诞生和发展，我觉得 spark 有一点做得非常明智：&lt;em&gt;对同类产品的兼容&lt;/em&gt;。从大的方面来说，就像 spark 官网的这段话一样: &lt;em&gt;Runs Everywhere: Spark runs on Hadoop, Mesos, standalone, or in the cloud. It can access diverse data sources including HDFS, Cassandra, HBase, and S3.&lt;/em&gt;，spark 对 hadoop 系产品的兼容，让 hadoop 系的开发人员可以轻松的从 hadoop 转到 spark；从小的方面来说，spark 对一些细分工具也照顾 [兼容] 得很好，比如说 spark 推出了 dataframe，人家就可以支持 spark dataframe 和 pandas dataframe 的转换。&lt;/p&gt;

&lt;p&gt;熟悉 pandas dataframe 的都了解，pandas 里的 dataframe 可以做很多事情，比如说画图，保存为各种类型的文件，做数据分析什么的。我觉得，可以在 spark 的 dataframe 里做数据处理，分析的整个逻辑，然后可以把最后的结果转化成 pandas 的 dataframe 来展示。当然，如果你的数据量小，也可以直接用 pandas dataframe 来做。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/spark-dataframe-7.jpg&quot; alt=&quot;spark-dataframe-7.jpg&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;5. 一些经验&lt;/h2&gt;

&lt;h3 id=&quot;spark-json-&quot;&gt;5.1 spark json 格式问题&lt;/h3&gt;

&lt;p&gt;spark 目前也不支持读取标准的 json 文件。你需要预先把标准的 json 文件处理成 spark 支持的格式: 每一行是一个 json 对象。&lt;/p&gt;

&lt;h3 id=&quot;spark-dataframe--pandas-dataframe--1&quot;&gt;5.2 spark dataframe 和 pandas dataframe 选择问题&lt;/h3&gt;

&lt;p&gt;如果数据量小，结构简单，可以直接用 pandas dataframe 来做分析；如果数据量大，结构复杂 [嵌套结构]，那么推荐用 spark dataframe 来做数据分析，然后把结果转成 pandas dataframe，用 pandas dataframe 来做展示和报告。&lt;/p&gt;

&lt;h2 id=&quot;next&quot;&gt;6. Next&lt;/h2&gt;

&lt;p&gt;ok，dataframe 简单的也说了几句了。我们先缓一缓，上个例子，再接着讲起他的，例子的话就用一个我正在实践的：用 spark 来做量化投资。&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;7. 打开微信，扫一扫，点一点，棒棒的，^_^&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;../images/wechat_pay_6-6.png&quot; alt=&quot;wechat_pay_6-6.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;参考文章&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://spark.apache.org/docs/latest/sql-programming-guide.html#dataframes&quot;&gt;Spark SQL, DataFrames and Datasets Guide&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html&quot;&gt;Introducing DataFrames in Spark for Large Scale Data Science&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://forums.databricks.com/questions/7257/from-webinar-spark-dataframes-what-is-the-differen-1.html&quot;&gt;From Webinar Apache Spark 1.5: What is the difference between a DataFrame and a RDD?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.infoq.com/cn/articles/apache-spark-sql&quot;&gt;用Apache Spark进行大数据处理——第二部分：Spark SQL&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://databricks.com/blog/2015/02/02/an-introduction-to-json-support-in-spark-sql.html&quot;&gt;An introduction to JSON support in Spark SQL&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.csdn.net/article/2015-02-18/2823997&quot;&gt;Spark新年福音：一个用于大规模数据科学的API——DataFrame&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://databricks.com/blog/2015/02/02/an-introduction-to-json-support-in-spark-sql.html&quot;&gt;An introduction to JSON support in Spark SQL&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-4&quot;&gt;本系列文章链接&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;../introduction-to-spark&quot;&gt;『 Spark 』1. spark 简介 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;../spark-questions-concepts&quot;&gt;『 Spark 』2. spark 基本概念解析 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;../spark-programming-model&quot;&gt;『 Spark 』3. spark 编程模式 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;../spark-what-is-rdd&quot;&gt;『 Spark 』4. spark 之 RDD &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;../spark-resouces-blogs-paper&quot;&gt;『 Spark 』5. 这些年，你不能错过的 spark 学习资源 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;../deep-into-spark-exection-model&quot;&gt;『 Spark 』6. 深入研究 spark 运行原理之 job, stage, task&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;../spark-dataframe-introduction&quot;&gt;『 Spark 』7. 使用 Spark DataFrame 进行大数据分析&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
   <entry>
     <title>『 读书笔记 』3月读书总结和推荐</title>
     <link href="/books-recommend-and-summarize-on-mar-2016"/>
     <updated>2016-03-26T00:00:00+08:00</updated>
     <id>/books-recommend-and-summarize-on-mar-2016</id>
     <content type="html">&lt;h2 id=&quot;section&quot;&gt;写在前面&lt;/h2&gt;

&lt;p&gt;计划是每月读 5-10 本书，书籍类型大概是三个方面的：金融，技术，管理。之所以选择这三个方面，一方面是因为自己对这三个方面都很有兴趣，其次是被 linkedin 创始人 Hoffman 的 &lt;a href=&quot;http://techcrunch.com/2012/02/14/in-startups-and-life-you-need-plan-a-b-and-z/&quot;&gt;ABZ 理论&lt;/a&gt; 深度影响。建议大家都看看 abz 理论那篇文章，如果我有空，也会整理一些常用的这类理论模型到博客里的。&lt;/p&gt;

&lt;p&gt;月底读书总结的形式都很简单，只是简单的一个列表和简单的书评，对觉得比较好的书会有单独的读书笔记。另外推荐大家用 excel 来做一些简单的工作管理，我现在就用 google docs 来做工作安排和读书计划，个人感觉比一些常用的神马协同软件强大太多了，简单，够用，就行了。工作中见过太多人把时间都花到使用那些协同软件上去，不得不说避重就轻了，适得其反，哈哈。&lt;/p&gt;

&lt;p&gt;下面是一张我用 google docs 来做读书安排的截图，不同颜色代表不同类别的数据，清晰明了实用。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/book-reading.jpg&quot; alt=&quot;book-reading.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;本月看了 7 本书，其中的电子书链接都放到亲爱的&lt;a href=&quot;http://pan.baidu.com/s/1pL26FZd&quot;&gt;度娘云&lt;/a&gt;里了，个人觉得不错的书都是纸板的，不知道有没有电子版的，推荐好书都看纸版的。&lt;/p&gt;

&lt;p&gt;ps: 我对好书的定义很简单：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;给自己有所启发的&lt;/li&gt;
  &lt;li&gt;高质量的，专业的教程类书籍&lt;/li&gt;
  &lt;li&gt;后期会再度回首的书&lt;/li&gt;
  &lt;li&gt;看完后会打算赠送给盆友看的书&lt;/li&gt;
  &lt;li&gt;留着给儿子看的书 [好吧，目前我只有个宝贝侄儿，哈哈]&lt;/li&gt;
  &lt;li&gt;最后一条，印刷质量要好&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;httppanbaiducoms1pl26fzd&quot;&gt;1. &lt;a href=&quot;http://pan.baidu.com/s/1pL26FZd&quot;&gt;中国顶尖技术团队访谈录 - 电子版&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;这是 infoq 出的一系列电子书中的一本，总共有 4 季访谈录，都是对一些公司技术领导人的访谈，虽然访谈都讲得很粗，但是在遇到相关问题时也可以参考参考别人是怎么处理的，比如说当你要搭建一个大型 docker 集群时，可以参考参考第二季访谈录中这篇 &lt;em&gt;腾讯罗韩梅 :万台规模的 Docker 应用实践&lt;/em&gt; ，虽然说肯定不能解决你的所有问题，但是你肯定知道在腾讯有这样一个牛人有这个经验啊，去 linkedin 什么的找找这个人，邮件或者微信或者通过其他方式请教人家也行啊，是吧，哈哈。&lt;/p&gt;

&lt;p&gt;总结：不要奢求能从这系列访谈里学到降龙十八掌，但是对于一个 tech leader 来说，看看这些书是应该的，&lt;em&gt;书中自有颜如玉，书外自有黄金屋&lt;/em&gt;。btw，第三季和第四季做得没前两季好，页数都少了很多，估计是 infoq 不想做这个访谈了吧，anyway。&lt;/p&gt;

&lt;p&gt;推荐指数：&lt;code class=&quot;highlighter-rouge&quot;&gt;* * * *&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;python-for-finance---httppanbaiducoms1pl26fzd&quot;&gt;2. &lt;a href=&quot;http://pan.baidu.com/s/1pL26FZd&quot;&gt;python for finance - 电子版&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;通读下来，这本书更应该叫 &lt;code class=&quot;highlighter-rouge&quot;&gt;python for finance - python tutoial and introduction to some basic financial theories&lt;/code&gt;，干货不多，大多数篇幅都去讲 &lt;code class=&quot;highlighter-rouge&quot;&gt;python&lt;/code&gt; 了，也讲了一些基础的金融理论，比如说蒙特卡罗模拟，期权定价原理什么的。如果你会 python，会用 pandas，懂一些基础的金融知识，可以不看这本书了。读下来对这本书没有什么大的感触，就不发表太多看法了。&lt;/p&gt;

&lt;p&gt;总结：如果你会 python，会用 pandas，懂一些基础的金融知识，可以不看这本书了；如果你不懂 python，不会 pandas，那也不推荐用这本书来学 python。&lt;/p&gt;

&lt;p&gt;推荐指数：&lt;code class=&quot;highlighter-rouge&quot;&gt;*&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;etf----httppanbaiducoms1pl26fzd&quot;&gt;3. &lt;a href=&quot;http://pan.baidu.com/s/1pL26FZd&quot;&gt;etf 投资，从入门到精通 - 电子版&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;之所以想看这本书，是一位大神觉得股票市场波动太大，去玩 etf 风险低，手续费也便宜，推荐我玩玩 etf。因为自己对 etf 一点不通，就买了这本书来看，上交所出版的，很专业，也讲得挺细致，对想玩，喜欢玩 etf 的人来说应该算是本好的手册。&lt;/p&gt;

&lt;p&gt;总结：etf 基础书籍里比较好的，对 etf 感兴趣的人可以看看哦。&lt;/p&gt;

&lt;p&gt;推荐指数：&lt;code class=&quot;highlighter-rouge&quot;&gt;* * * *&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;httpwwwamazoncne7bd97e8be91e6809de7bbb4-e69c89e7a78d-e69c89e8b6a3-e69c89e69699-e7bd97e68cafe5ae87dpb00fvha2f0refsr11ieutf8qid1459046888sr8-1keywordse980bbe8be91e6809de7bbb4&quot;&gt;4. &lt;a href=&quot;http://www.amazon.cn/%E7%BD%97%E8%BE%91%E6%80%9D%E7%BB%B4-%E6%9C%89%E7%A7%8D-%E6%9C%89%E8%B6%A3-%E6%9C%89%E6%96%99-%E7%BD%97%E6%8C%AF%E5%AE%87/dp/B00FVHA2F0/ref=sr_1_1?ie=UTF8&amp;amp;qid=1459046888&amp;amp;sr=8-1&amp;amp;keywords=%E9%80%BB%E8%BE%91%E6%80%9D%E7%BB%B4&quot;&gt;罗辑思维:有种、有趣、有料 - 纸版&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;罗辑思维出了几本书了，我看的是第一本，很有意思。不仅是观点上新颖独到，老罗还把网络上的一些评论也放到书里去了，甚至还放了一些&lt;code class=&quot;highlighter-rouge&quot;&gt;负面&lt;/code&gt;的评论，对读者来说这样很不错。读这本书，能让人在看待问题，处理问题时的思路更开阔，更宽容一些，学会从更多方面，更多角度去挖掘一个问题的根本原因。这本书还有个比较让我喜欢的地方，每章都会有一些推荐的书，其中不乏好书。经常听到人说很想看书，但是不知道看什么书，对此我的回答的 “随便挑本书来看，看着看着就知道该看什么书了”。还准备看看之后的版本，虽然同事说后面的版本没有前面的有意思了，不过打算先去书店翻看翻看，如果后续的版本不是换汤不换药，仍然满足上面我对好书的定义，那我也会毫不犹豫的买纸版来看的。&lt;/p&gt;

&lt;p&gt;总结：比较适合学生，职场人士读的书，尝试学会从更多的方面去待人待物。&lt;/p&gt;

&lt;p&gt;推荐指数：&lt;code class=&quot;highlighter-rouge&quot;&gt;* * * * *&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;httppanbaiducoms1pl26fzd-1&quot;&gt;5. &lt;a href=&quot;http://pan.baidu.com/s/1pL26FZd&quot;&gt;从0到1 - 电子版&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;这本书曾经很火，还记得当时公司群里时常都在讨论。虽然我看的是电子版的，不过我也觉得这本书值得买纸版的，如果能容忍那外强中干的印刷质量的话。这本书单独有总结帖的：&lt;/p&gt;

&lt;p&gt;总结：很适合工作 3 年以上的人看，特别是想创业，创业中的，在创业公司上班的人，以创业心态工作的人看。或者再宽泛一点，适合想把事情做好的人看。&lt;/p&gt;

&lt;p&gt;推荐指数：&lt;code class=&quot;highlighter-rouge&quot;&gt;* * * *&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;httpwwwamazoncne69cbae599a8e5ada6e4b9a0-e591a8e5bf97e58d8edpb01arkev1grefsr11ieutf8qid1459046918sr8-1keywordse69cbae599a8e5ada6e4b9a0&quot;&gt;6. &lt;a href=&quot;http://www.amazon.cn/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%91%A8%E5%BF%97%E5%8D%8E/dp/B01ARKEV1G/ref=sr_1_1?ie=UTF8&amp;amp;qid=1459046918&amp;amp;sr=8-1&amp;amp;keywords=%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0&quot;&gt;机器学习（周志华）- 纸版&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;啊哈，这本书怎么说呢？之所以买他全是因为同事朋友圈里的一篇转发，说这个似乎是泰斗级的教授写了一本机器学习的书，当时也看了下这个教授的介绍 &lt;em&gt;[&lt;/em&gt; 哈哈，对天朝的老师们没什么好感。按照我的理解，所谓 &lt;code class=&quot;highlighter-rouge&quot;&gt;师者，传道，授业，解惑也&lt;/code&gt;，不知道天朝有几个老师敢读了韩愈的这段话还敢自称师者的 &lt;em&gt;]&lt;/em&gt;，觉得还行，amazon 上的书评也还可以，就剁手买了下来。&lt;/p&gt;

&lt;p&gt;读下来，只能说还可以吧，just so so，但是这本书有种很浓烈的味道 －－ 书生味。也许是工作的原因，对这类有太多书生味的书没太大感觉。还是更喜欢实在一些的书，比如 Mitchell 的 《Machine Learning》, 图灵出版的《Machine Learning in Action》，或者细分下来的《推荐系统实战》这类书。&lt;/p&gt;

&lt;p&gt;总结：书生味太浓，内容倒是也不差。&lt;/p&gt;

&lt;p&gt;推荐指数：&lt;code class=&quot;highlighter-rouge&quot;&gt;* * *&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;httpwwwamazoncne4bcb0e580bce79a84e889bae69caf-110e4b8aae8a7a3e8afbbe6a188e4be8b-e5b0bce58fa4e68b89e696afc2b7e696afe5af86e5beb7e69e97dpb014d1mc5wrefsr11ieutf8qid1459046933sr8-1keywordse4bcb0e580bce79a84e889bae69caf&quot;&gt;7. &lt;a href=&quot;http://www.amazon.cn/%E4%BC%B0%E5%80%BC%E7%9A%84%E8%89%BA%E6%9C%AF-110%E4%B8%AA%E8%A7%A3%E8%AF%BB%E6%A1%88%E4%BE%8B-%E5%B0%BC%E5%8F%A4%E6%8B%89%E6%96%AF%C2%B7%E6%96%AF%E5%AF%86%E5%BE%B7%E6%9E%97/dp/B014D1MC5W/ref=sr_1_1?ie=UTF8&amp;amp;qid=1459046933&amp;amp;sr=8-1&amp;amp;keywords=%E4%BC%B0%E5%80%BC%E7%9A%84%E8%89%BA%E6%9C%AF&quot;&gt;估值的艺术:110个解读案例 - 纸版&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;和这本书的第一次相遇是在陆家嘴正大广场的书店里看到的，当时我很想找一本公司基本面的书来看看，准备在自己的投资模型里多加一些公司基本面的因子。当时第一次看到这本书，翻看了十来分钟，知道这就是我想要的，简单，够用，还有翔实的例子，比《公司财务原理》这类书要来得痛快干脆，btw，我并不是说《公司财务原理》这本书不好，我也在看这本书的，只是《估值的艺术》这本书更适合当时的需求。而且，从小的方面来说，这本书能教你一些公司基本面的东西，对投资有所帮助；从大到方面来说，这本书教你怎么挖掘一个潜力公司，或者教你怎么管理自己的公司，或者说教你当你有了自己的公司的时候，应该从哪些方面实时查看自己公司的发展情况。很有价值。&lt;/p&gt;

&lt;p&gt;总结：如果你做股票投资，这本书值得一看；如果你有自己的公司，或者以后想要有自己的公司，那这本书更值得反复品读。&lt;/p&gt;

&lt;p&gt;推荐指数：&lt;code class=&quot;highlighter-rouge&quot;&gt;* * * * *&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;master-apache-sparkhttpswwwgitbookcombookjaceklaskowskimastering-apache-sparkdetails&quot;&gt;8. &lt;a href=&quot;https://www.gitbook.com/book/jaceklaskowski/mastering-apache-spark/details&quot;&gt;Master Apache Spark&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;这本书是一个国外咨询师写的开源书籍，居然还有一本纸质版的 &lt;a href=&quot;http://shop.oreilly.com/product/9781783987146.do&quot;&gt;Master Apache Spark&lt;/a&gt;，不过和这本开源书籍应该没什么关系。之所以想先看这本书是因为 spark 更新得很快，作者应该会及时更新相关内容到最新的 spark 版本。看下来感觉还行，都是作者根据相关文档，相关书籍，以及自己的理解和实践来写的。但是里面还是有一些问题，也有的地方没有写。不推荐作为第一本学习spark的书籍，可以在有一定经验后翻翻看。下月还是准备看 Matei 合写的 &lt;a href=&quot;http://shop.oreilly.com/product/0636920028512.do&quot;&gt;Learning Spark&lt;/a&gt;，虽然出版时间很早，但是毕竟是 spark 的作者参与的，内容应该更清晰，深入，等待下个月我的读书笔记吧。&lt;/p&gt;

&lt;p&gt;总结：内容还行，作者更新也挺频繁的，但是不推荐作为第一本学习 spark 的书，有一定经验后可以看看。&lt;/p&gt;

&lt;p&gt;推荐指数：&lt;code class=&quot;highlighter-rouge&quot;&gt;* * * *&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;参考文章&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://techcrunch.com/2012/02/14/in-startups-and-life-you-need-plan-a-b-and-z/&quot;&gt;In Startups And Life, You Need Plan A, B, And Z&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
   <entry>
     <title>『 Spark 』6. 深入研究 spark 运行原理之 job, stage, task</title>
     <link href="/deep-into-spark-exection-model"/>
     <updated>2016-03-18T00:00:00+08:00</updated>
     <id>/deep-into-spark-exection-model</id>
     <content type="html">&lt;h2 id=&quot;section&quot;&gt;写在前面&lt;/h2&gt;

&lt;p&gt;本系列是综合了自己在学习spark过程中的理解记录 ＋ 对参考文章中的一些理解 ＋ 个人实践spark过程中的一些心得而来。写这样一个系列仅仅是为了梳理个人学习spark的笔记记录，所以一切以能够理解为主，没有必要的细节就不会记录了，而且文中有时候会出现英文原版文档，只要不影响理解，都不翻译了。若想深入了解，最好阅读参考文章和官方文档。&lt;/p&gt;

&lt;p&gt;其次，本系列是基于目前最新的 spark 1.6.0 系列开始的，spark 目前的更新速度很快，记录一下版本好还是必要的。 &lt;br /&gt;
最后，如果各位觉得内容有误，欢迎留言备注，所有留言 24 小时内必定回复，非常感谢。   &lt;br /&gt;
Tips: 如果插图看起来不明显，可以：1. 放大网页；2. 新标签中打开图片，查看原图哦。&lt;/p&gt;

&lt;h2 id=&quot;spark-&quot;&gt;1. spark 运行原理&lt;/h2&gt;

&lt;p&gt;这一节是本文的核心，我们可以先抛出一个问题，如果看完这一节，或者这一章之后，你能理解你的整个 spark 应用的执行流程，那就可以关掉这个网页了［对了，关掉网页之前记得分享一下哦，哈哈］&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Problem: How does user program get translated into units of physical execution ?&lt;/code&gt;&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;我们用一个例子来说明，结合例子和运行截图来理解。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;1.1 例子，美国 1880 － 2014 年新生婴儿数据统计&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;目标&lt;/code&gt;：用美国 1880 － 2014 年新生婴儿的数据来做做简单的统计&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;数据源&lt;/code&gt;：&lt;a href=&quot;https://catalog.data.gov/dataset/baby-names-from-social-security-card-applications-national-level-data&quot;&gt; https://catalog.data.gov&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;数据格式&lt;/code&gt;：
    &lt;ul&gt;
      &lt;li&gt;每年的新生婴儿数据在一个文件里面&lt;/li&gt;
      &lt;li&gt;每个文件的每一条数据格式：&lt;code class=&quot;highlighter-rouge&quot;&gt;姓名,性别,新生人数&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../images/baby-data-format.jpg&quot; alt=&quot;baby-data-format.jpg&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;代码和结果展示&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;### packages&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pd&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;### spark UDF (User Defined Functions)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;map_extract&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;element&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;file_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;content&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;element&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;year&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;file_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;year&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;content&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\r\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;### spark logic&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;res&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;wholeTextFiles&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;hdfs://10.21.208.21:8020/user/mercury/names&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;minPartitions&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;40&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  \
        &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;map_extract&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
        &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flatMap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
        &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;,&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])))&lt;/span&gt; \
        &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reduceByKey&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;operator&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
        &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;collect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;### result displaying&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_records&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;year&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;birth&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sort&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;year&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ascending&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;year&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;birth&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;figsize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;US Baby Birth Data from 1897 to 2014&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;linewidth&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_axis_bgcolor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;white&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;gray&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;y&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;../images/baby-name-1.jpg&quot; alt=&quot;baby-name-1.jpg&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;1.2 运行流程概览&lt;/h2&gt;

&lt;p&gt;还记得我们在  &lt;a href=&quot;../spark-programming-model&quot;&gt;『 Spark 』3. spark 编程模式 &lt;/a&gt; 讲到的构建一个 spark application 的过程吗：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;加载数据集&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;处理数据&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;结果展示&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;上面的 22 行代码，就已经把构建一个 spark app 的三大步骤完成了，amazing, right? 今天我们主要讲 spark 的运行逻辑，所以我们就以核心的 11 － 16 ，这六行代码来作为今天的主线，了解了解 spark 的原理。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/baby-name-2.jpg&quot; alt=&quot;baby-name-2.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;可以看到，整个逻辑实际上就用了 sparkContext 的一个函数，rdd 的 3 个 transformation 和 1 个 action。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/baby-name-job.jpg&quot; alt=&quot;baby-name-job.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;现在让我们从 WEB UI 上来看看，当我们运行这段代码的时候，后台都发生了什么。
可以看到，执行这段代码的时候，spark 通过分析，优化代码，知道这段代码需要一个 job 来完成，所以 web ui 上只有一个 job。值得深究的是，这个 job 由两个 stage 完成，这两个 state 一共有 66 个 task。&lt;/p&gt;

&lt;p&gt;所以，这里我们就再次理解下 spark 里，job，stage，task 的概念：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;job&lt;/em&gt; : A job is triggered by an action, like count() or saveAsTextFile(). Click on a job to see information about the stages of tasks inside it. 理解了吗，所谓一个 job，就是由一个 rdd 的 action 触发的动作，可以简单的理解为，当你需要执行一个 rdd 的 action 的时候，会生成一个 job。&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;stage&lt;/em&gt; : stage 是一个 job 的组成单位，就是说，一个 job 会被切分成 1 个或 1 个以上的 stage，然后各个 stage 会按照执行顺序依次执行。至于 job 根据什么标准来切分 stage，可以回顾第二篇博文：&lt;a href=&quot;../spark-questions-concepts&quot;&gt;『 Spark 』2. spark 基本概念解析 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;task&lt;/em&gt; : A unit of work within a stage, corresponding to one RDD partition。即 stage 下的一个任务执行单元，一般来说，一个 rdd 有多少个 partition，就会有多少个 task，因为每一个 task 只是处理一个 partition 上的数据。从 web ui 截图上我们可以看到，这个 job 一共有 2 个 stage，66 个 task，平均下来每个 stage 有 33 个 task，相当于每个 stage 的数据都有 33 个 partition [注意：这里是平均下来的哦，并不都是每个 stage 有 33 个 task，有时候也会有一个 stage 多，另外一个 stage 少的情况，就看你有没有在不同的 stage 进行 repartition 类似的操作了。]&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../images/baby-name-ui-1.jpg&quot; alt=&quot;baby-name-ui-1.jpg&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;job&quot;&gt;1.3 运行流程之 : job&lt;/h2&gt;

&lt;p&gt;根据上面的截图和再次重温，我们知道这个 spark 应用里只有一个 job，那就是因为我们执行了一个 &lt;code class=&quot;highlighter-rouge&quot;&gt;collect&lt;/code&gt; 操作，即把处理后的数据全部返回到我们的 driver 上，进行后续的画图，返回的数据如下图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/baby-name-3.jpg&quot; alt=&quot;baby-name-3.jpg&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;stage&quot;&gt;1.4 运行流程之 : stage&lt;/h2&gt;

&lt;p&gt;我们这个 spark 应用，生成了一个 job，这个 job 由 2 个 stage 组成，并且每个 stage 都有 33 个task，说明每个 stage 的数据都在 33 个 partition 上，这下我们就来看看，这两个 stage 的情况。&lt;/p&gt;

&lt;p&gt;首先，我们先看看为什么这里会有两个 stage，根据 &lt;a href=&quot;../spark-questions-concepts&quot;&gt;『 Spark 』2. spark 基本概念解析 &lt;/a&gt; 中对 stage 的描述，目前有两个划分 stage 的标准：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;当触发 rdd 的 action 时 : 在我们的应用中就是最后的 &lt;code class=&quot;highlighter-rouge&quot;&gt;collect&lt;/code&gt; 操作，关于这个操作的说明，可以看官方文档: &lt;a href=&quot;https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.collect&quot;&gt;rdd.collect&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;当触发 rdd 的 shuffle 操作时 : 在我们的应用中就是 &lt;code class=&quot;highlighter-rouge&quot;&gt;reduceByKey&lt;/code&gt; 这个操作，官方文档: &lt;a href=&quot;https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduceByKey&quot;&gt;rdd.reduceByKey&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../images/baby-name-4.jpg&quot; alt=&quot;baby-name-4.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;再次回顾上面那张图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/baby-name-job.jpg&quot; alt=&quot;baby-name-job.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这下应该就明了了，关于两个 stage 的情况：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/baby-name-5.jpg&quot; alt=&quot;baby-name-5.jpg&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;第一个 stage，即截图中 stage id 为 0 的 stage，其执行了 &lt;code class=&quot;highlighter-rouge&quot;&gt;sc.wholeTextFiles().map().flatMap().map().reduceByKey()&lt;/code&gt; 这几个步骤，因为这是一个 &lt;code class=&quot;highlighter-rouge&quot;&gt;Shuffle&lt;/code&gt; 操作，所以后面会有 &lt;code class=&quot;highlighter-rouge&quot;&gt;Shuffle Read&lt;/code&gt; 和 &lt;code class=&quot;highlighter-rouge&quot;&gt;Shuffle Write&lt;/code&gt;。具体来说，就是在 stage 0 这个 stage 中，发生了一个 Shuffle 操作，这个操作读入 22.5 MB 的数据，生成 41.7 KB 的数据，并把生成的数据写在了硬盘上。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;第二个 stage，即截图中 stage id 为 1 到 stage，其执行了 &lt;code class=&quot;highlighter-rouge&quot;&gt;collect()&lt;/code&gt; 这个操作，因为这是一个 &lt;code class=&quot;highlighter-rouge&quot;&gt;action&lt;/code&gt; 操作，并且它上一步是一个 Shuffle 操作，且没有后续操作，所以这里 &lt;code class=&quot;highlighter-rouge&quot;&gt;collect()&lt;/code&gt; 这个操作被独立成一个 stage 了。这里它把上一个 Shuffle 写下的数据读取进来，然后一起返回到 driver 端，所以这里可以看到他的 &lt;code class=&quot;highlighter-rouge&quot;&gt;Shuffle Read&lt;/code&gt; 这里刚好读取了上一个 stage 写下的数据。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;task&quot;&gt;1.5 运行流程之 : task&lt;/h2&gt;

&lt;p&gt;其实到这里应该都理解得差不多了，至于为什么每个 stage 会有 33 个 task [即我们的数据文件存放到 33 个partition 上，可是明明 &lt;code class=&quot;highlighter-rouge&quot;&gt;sc.wholeTextFiles(&#39;hdfs://10.21.208.21:8020/user/mercury/names&#39;, minPartitions=40)&lt;/code&gt; 这里指定了最小要 40 个partition 到啊]，这个问题我们留到以后说，在后面我们会有一篇讲怎么调试，优化 spark app 的博文，到时候我们会继续回到这里，解答这里的问题。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/baby-name-7.jpg&quot; alt=&quot;baby-name-7.jpg&quot; /&gt;
&lt;img src=&quot;../images/baby-name-8.jpg&quot; alt=&quot;baby-name-8.jpg&quot; /&gt;
&lt;img src=&quot;../images/baby-name-9.jpg&quot; alt=&quot;baby-name-9.jpg&quot; /&gt;
&lt;img src=&quot;../images/baby-name-10.jpg&quot; alt=&quot;baby-name-10.jpg&quot; /&gt;
&lt;img src=&quot;../images/baby-name-11.jpg&quot; alt=&quot;baby-name-11.jpg&quot; /&gt;
&lt;img src=&quot;../images/baby-name-12.jpg&quot; alt=&quot;baby-name-12.jpg&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;next&quot;&gt;2. Next&lt;/h2&gt;

&lt;p&gt;既然我们都慢慢开始深入理解 spark 的执行原理了，那下次我们就来说说 spark 的一些配置吧，然后再说说 spark 应用的优化。&lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;7. 打开微信，扫一扫，点一点，棒棒的，^_^&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;../images/wechat_pay_6-6.png&quot; alt=&quot;wechat_pay_6-6.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-4&quot;&gt;参考文章&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.slideshare.net/pwendell/tuning-and-debugging-in-apache-spark&quot;&gt;Tuning and Debugging in Apache Spark&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.amazon.com/Learning-Spark-Lightning-Fast-Data-Analysis/dp/1449358624/ref=sr_1_1?ie=UTF8&amp;amp;qid=1458293667&amp;amp;sr=8-1&amp;amp;keywords=learning+spark&quot;&gt;learning spark&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://aiyanbo.gitbooks.io/spark-programming-guide-zh-cn/content/more/spark-configuration.html&quot;&gt;Spark配置&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://colobu.com/2014/12/10/spark-configuration/&quot;&gt;Spark 配置指南&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-5&quot;&gt;本系列文章链接&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;../introduction-to-spark&quot;&gt;『 Spark 』1. spark 简介 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;../spark-questions-concepts&quot;&gt;『 Spark 』2. spark 基本概念解析 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;../spark-programming-model&quot;&gt;『 Spark 』3. spark 编程模式 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;../spark-what-is-rdd&quot;&gt;『 Spark 』4. spark 之 RDD &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;../spark-resouces-blogs-paper&quot;&gt;『 Spark 』5. 这些年，你不能错过的 spark 学习资源 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;../deep-into-spark-exection-model&quot;&gt;『 Spark 』6. 深入研究 spark 运行原理之 job, stage, task&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;../spark-dataframe-introduction&quot;&gt;『 Spark 』7. 使用 Spark DataFrame 进行大数据分析&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
   <entry>
     <title>『 Spark 』5. 这些年，你不能错过的 spark 学习资源</title>
     <link href="/spark-resouces-blogs-paper"/>
     <updated>2016-03-10T00:00:00+08:00</updated>
     <id>/spark-resouces-blogs-paper</id>
     <content type="html">&lt;h2 id=&quot;section&quot;&gt;写在前面&lt;/h2&gt;

&lt;p&gt;本系列是综合了自己在学习spark过程中的理解记录 ＋ 对参考文章中的一些理解 ＋ 个人实践spark过程中的一些心得而来。写这样一个系列仅仅是为了梳理个人学习spark的笔记记录，所以一切以能够理解为主，没有必要的细节就不会记录了，而且文中有时候会出现英文原版文档，只要不影响理解，都不翻译了。若想深入了解，最好阅读参考文章和官方文档。&lt;/p&gt;

&lt;p&gt;其次，本系列是基于目前最新的 spark 1.6.0 系列开始的，spark 目前的更新速度很快，记录一下版本好还是必要的。 &lt;br /&gt;
最后，如果各位觉得内容有误，欢迎留言备注，所有留言 24 小时内必定回复，非常感谢。   &lt;br /&gt;
Tips: 如果插图看起来不明显，可以：1. 放大网页；2. 新标签中打开图片，查看原图哦。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;1. 文章，博客&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf&quot;&gt;RDD论文英文版&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://code.csdn.net/CODE_Translation/spark_matei_phd&quot;&gt;RDD论文中文版&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-12.pdf&quot;&gt;An Architecture for Fast and General Data Processing
on Large Clusters&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-1/&quot;&gt;How-to: Tune Your Apache Spark Jobs (Part 1)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://dataunion.org/22985.html&quot;&gt;借助 Redis ，让 Spark 提速 45 倍！&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.csdn.net/article/2015-10-06/2825849&quot;&gt;量化派基于Hadoop、Spark、Storm的大数据风控架构&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://geek.csdn.net/news/detail/58867&quot;&gt;基于Spark的异构分布式深度学习平台&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.36dsj.com/archives/40723&quot;&gt;你对Hadoop和Spark生态圈了解有几许？&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.yuntoutiao.com/dongtai/5389.html&quot;&gt;Hadoop vs Spark&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://geek.csdn.net/news/detail/57656&quot;&gt;雅虎开源CaffeOnSpark：基于Hadoop/Spark的分布式深度学习&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-2&quot;&gt;2. 视频&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=cs3_3LdCny8&quot;&gt;YouTube: what is apache spark&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=65aV15uDKgA&quot;&gt;Introduction to Spark Architecture&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=WyfHUNnMutg&quot;&gt;Top 5 Mistakes When Writing Spark Applications&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.slideshare.net/hadooparchbook/top-5-mistakes-when-writing-spark-applications&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;slide&lt;/code&gt; Top 5 mistakes when writing Spark applications&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=kkOG_aJ9KjQ&quot;&gt;Tuning and Debugging Apache Spark&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.slideshare.net/pwendell/tuning-and-debugging-in-apache-spark&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;slide&lt;/code&gt; Tuning and Debugging Apache Spark&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=dmL0N3qfSc8&quot;&gt;A Deeper Understanding of Spark Internals - Aaron Davidson (Databricks)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://spark-summit.org/2014/wp-content/uploads/2014/07/A-Deeper-Understanding-of-Spark-Internals-Aaron-Davidson.pdf&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;slide&lt;/code&gt; A Deeper Understanding of Spark Internals - Aaron Davidson (Databricks)&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=OednhGRp938&quot;&gt;Building, Debugging, and Tuning Spark Machine Learning Pipelines - Joseph Bradley (Databricks)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.slideshare.net/SparkSummit/building-debugging-and-tuning-spark-machine-leaning-pipelinesjoseph-bradley&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;slide&lt;/code&gt; Building, Debugging, and Tuning Spark Machine Learning Pipelines&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=xWkJCUcD55w&quot;&gt;Spark DataFrames Simple and Fast Analysis of Structured Data - Michael Armbrust (Databricks)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.slideshare.net/databricks/spark-dataframes-simple-and-fast-analytics-on-structured-data-at-spark-summit-2015&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;slide&lt;/code&gt; Spark DataFrames Simple and Fast Analysis of Structured Data - Michael Armbrust (Databricks)&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=HBZuB3pPri0&amp;amp;feature=youtu.be&quot;&gt;Spark Tuning for Enterprise System Administrators&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.slideshare.net/AnyaBida/bida-sse2016final-58237248&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;slide&lt;/code&gt; Spark Tuning for Enterprise System Administrators&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=i7l3JQRx7Qw&quot;&gt;Structuring Spark: DataFrames, Datasets, and Streaming&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.slideshare.net/databricks/structuring-spark-dataframes-datasets-and-streaming&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;slide&lt;/code&gt; Structuring Spark: DataFrames, Datasets, and Streaming&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=GzG9RTRTFck&quot;&gt;Spark in Production: Lessons from 100+ Production Users&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.slideshare.net/databricks/spark-summit-eu-2015-lessons-from-300-production-users&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;slide&lt;/code&gt; Spark in Production: Lessons from 100+ Production Users&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=rBrsxM091KA&quot;&gt;Production Spark and Tachyon use Cases&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.slideshare.net/SparkSummit/using-spark-with-tachyon-by-gene-pang&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;slide&lt;/code&gt; Production Spark and Tachyon use Cases&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=VQOKk9jJGcw&amp;amp;index=5&amp;amp;list=PL-x35fyliRwif48cPXQ1nFM85_7e200Jp&quot;&gt;SparkUI Visualization&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.slideshare.net/databricks/spark-summit-eu-2015-sparkui-visualization-a-lens-into-your-application&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;slide&lt;/code&gt; SparkUI Visualization&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=Wg2boMqLjCg&quot;&gt;
Everyday I’m Shuffling - Tips for Writing Better Spark Programs, Strata San Jose 2015&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.slideshare.net/databricks/strata-sj-everyday-im-shuffling-tips-for-writing-better-spark-programs&quot;&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;slide&lt;/code&gt; 
Everyday I’m Shuffling - Tips for Writing Better Spark Programs, Strata San Jose 2015&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=FA3ArTyXNoo&quot;&gt;Large Scale Distributed Machine Learning on Apache Spark&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=Aups6UcGiQQ&amp;amp;list=PL-x35fyliRwif48cPXQ1nFM85_7e200Jp&amp;amp;index=1&quot;&gt;Securing your Spark Applications&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.slideshare.net/cloudera/securing-your-apache-spark-applications&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;slide&lt;/code&gt; Securing your Spark Applications&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=AHYq91i-ohI&quot;&gt;Building a REST Job Server for Interactive Spark as a Service&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.slideshare.net/SparkSummit/building-a-rest-job-server-for-interactive-spark-as-a-service-by-romain-rigaux-and-erick-tryzelaar&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;slide&lt;/code&gt; Building a REST Job Server for Interactive Spark as a Service&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=PPQRi484bNo&amp;amp;list=PL-x35fyliRwif48cPXQ1nFM85_7e200Jp&amp;amp;index=2&quot;&gt;Exploiting GPUs for Columnar DataFrame Operations&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;http://www.slideshare.net/SparkSummit/exploiting-gpus-for-columnar-datafrrames-by-kiran-lonikar&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;slide&lt;/code&gt; Exploiting GPUs for Columnar DataFrame Operations&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=fBWLzB0FMX4&amp;amp;index=3&amp;amp;list=PL-x35fyliRwif48cPXQ1nFM85_7e200Jp&quot;&gt;Time Series Stream Processing with Spark and Cassandra&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=MFSUAkDBSdQ&quot;&gt;Easy JSON Data Manipulation in Spark - Yin Huai (Databricks)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://spark-summit.org/2014/wp-content/uploads/2014/07/Easy-json-Data-Manipulation-Yin-Huai.pdf&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;slide&lt;/code&gt; Easy JSON Data Manipulation in Spark - Yin Huai (Databricks)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-3&quot;&gt;3. 网站&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://spark.apache.org/&quot;&gt;official site&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://apache-spark-user-list.1001560.n3.nabble.com/&quot;&gt;user mailing list&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/user/TheApacheSpark&quot;&gt;spark channel on youtube&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://spark-summit.org/&quot;&gt;spark summit&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.meetup.com/&quot;&gt;meetup&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://spark-packages.org/&quot;&gt;spark third party packages&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://databricks.com/blog&quot;&gt;databricks blog&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.cloud.databricks.com/docs/latest/sample_applications/index.html#Introduction%20(Readme).html&quot;&gt;databricks docs&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.cloudera.com/blog/category/spark/&quot;&gt;cloudera blog about spark&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://techsuppdiva.github.io/&quot;&gt;http://techsuppdiva.github.io/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.iteblog.com/archives/category/spark&quot;&gt;过往记忆&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;next&quot;&gt;4. next&lt;/h2&gt;

&lt;p&gt;上面的资源我都会不断更新的，里面 80% 以上的都是我亲自看过并且觉得有价值的，可不是胡乱收集一通的，推荐欣赏哦。&lt;/p&gt;

&lt;h2 id=&quot;section-4&quot;&gt;5. 打开微信，扫一扫，点一点，棒棒的，^_^&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;../images/wechat_pay.png&quot; alt=&quot;wechat_pay.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-5&quot;&gt;本系列文章链接&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;../introduction-to-spark&quot;&gt;『 Spark 』1. spark 简介 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;../spark-questions-concepts&quot;&gt;『 Spark 』2. spark 基本概念解析 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;../spark-programming-model&quot;&gt;『 Spark 』3. spark 编程模式 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;../spark-what-is-rdd&quot;&gt;『 Spark 』4. spark 之 RDD &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;../spark-resouces-blogs-paper&quot;&gt;『 Spark 』5. 这些年，你不能错过的 spark 学习资源 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;../deep-into-spark-exection-model&quot;&gt;『 Spark 』6. 深入研究 spark 运行原理之 job, stage, task&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;../spark-dataframe-introduction&quot;&gt;『 Spark 』7. 使用 Spark DataFrame 进行大数据分析&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
   <entry>
     <title>『 Spark 』4. spark 之 RDD</title>
     <link href="/spark-what-is-rdd"/>
     <updated>2016-03-08T00:00:00+08:00</updated>
     <id>/spark-what-is-rdd</id>
     <content type="html">&lt;h2 id=&quot;section&quot;&gt;写在前面&lt;/h2&gt;

&lt;p&gt;本系列是综合了自己在学习spark过程中的理解记录 ＋ 对参考文章中的一些理解 ＋ 个人实践spark过程中的一些心得而来。写这样一个系列仅仅是为了梳理个人学习spark的笔记记录，所以一切以能够理解为主，没有必要的细节就不会记录了，而且文中有时候会出现英文原版文档，只要不影响理解，都不翻译了。若想深入了解，最好阅读参考文章和官方文档。&lt;/p&gt;

&lt;p&gt;其次，本系列是基于目前最新的 spark 1.6.0 系列开始的，spark 目前的更新速度很快，记录一下版本好还是必要的。 &lt;br /&gt;
最后，如果各位觉得内容有误，欢迎留言备注，所有留言 24 小时内必定回复，非常感谢。   &lt;br /&gt;
Tips: 如果插图看起来不明显，可以：1. 放大网页；2. 新标签中打开图片，查看原图哦。&lt;/p&gt;

&lt;h2 id=&quot;rdd&quot;&gt;1. 什么是RDD&lt;/h2&gt;
&lt;p&gt;先看下源码里是怎么描述RDD的。&lt;/p&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;

    &lt;p&gt;Internally, each RDD is characterized by five main properties:&lt;br /&gt;
A list of partitions&lt;br /&gt;
A function for computing each split &lt;br /&gt;
A list of dependencies on other RDDs&lt;br /&gt;
Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned) &lt;br /&gt;
Optionally, a list of preferred locations to compute each split on (e.g. block locations for an HDFS file)&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;p&gt;可以知道，每个 RDD 有以下5个主要的属性：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;

    &lt;ul&gt;
      &lt;li&gt;一组分片（partition），即数据集的基本组成单位&lt;/li&gt;
      &lt;li&gt;一个计算每个分片的函数&lt;/li&gt;
      &lt;li&gt;对parent RDD的依赖，这个依赖描述了RDD之间的 &lt;code class=&quot;highlighter-rouge&quot;&gt;lineage&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;对于key-value的RDD，一个Partitioner，这是可选择的&lt;/li&gt;
      &lt;li&gt;一个列表，存储存取每个partition的preferred位置。对于一个HDFS文件来说，存储每个partition所在的块的位置。这也是可选择的&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;p&gt;把上面这5个主要的属性总结一下，可以得出RDD的大致概念：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;blockquote&gt;

    &lt;p&gt;首先，RDD 大概是这样一种表示数据集的东西，它具有以上列出的一些属性。是设计用来表示数据集的一种数据结构。为了让 RDD 能 handle 更多的问题，规定 RDD 应该是只读的，分区记录的一种数据集合。可以通过两种方式来创建 RDD：一种是基于物理存储中的数据，比如说磁盘上的文件；另一种，也是大多数创建 RDD 的方式，即通过其他 RDD 来创建【以后叫做转换】而成。而正因为 RDD 满足了这么多特性，所以 spark 把 RDD 叫做 &lt;code class=&quot;highlighter-rouge&quot;&gt;Resilient Distributed Datasets&lt;/code&gt;，中文叫做弹性分布式数据集。很多文章都是先讲 RDD 的定义，概念，再来说 RDD 的特性。我觉得其实也可以倒过来，通过 RDD 的特性反过来理解 RDD 的定义和概念，通过这种由果溯因的方式来理解 RDD 也未尝不可，至少对我个人而言这种方式是挺好的。&lt;/p&gt;
  &lt;/blockquote&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;rdd-1&quot;&gt;2. 理解RDD的几个关键概念&lt;/h2&gt;

&lt;p&gt;本来我是想参考 RDD 的论文和自己的理解来整理这篇文章的，可是后来想想这样是不是有点过于细致了。我想，认识一个新事物，在时间、资源有限的情况下，不必锱铢必较，可以先 focus on 几个关键点，到后期应用的时候再步步深入。&lt;/p&gt;

&lt;p&gt;所以，按照我个人的理解，我认为想用好 spark，必须要理解 RDD ，而为了理解 RDD ，我认为只要了解下面几个 RDD 的几个关键点就能 handle 很多情况下的问题了。所以，下面所有列到的点，都是在我个人看来很重要的，但也许有所欠缺，大家如果想继续深入，可以看第三部分列出的参考资料，或者直接联系我，互相交流。&lt;/p&gt;

&lt;h3 id=&quot;rdd-2&quot;&gt;2.1 RDD的背景及解决的痛点问题&lt;/h3&gt;

&lt;p&gt;RDD 的设计是为了充分利用分布式系统中的内存资源，使得提升一些特定的应用的效率。这里所谓的特定的应用没有明确定义，但可以理解为一类应用到迭代算法，图算法等需要重复利用数据的应用类型；除此之外，RDD 还可以应用在交互式大数据处理方面。所以，我们这里需要明确一下：&lt;code class=&quot;highlighter-rouge&quot;&gt;RDD并不是万能的，也不是什么带着纱巾的少女那样神奇。简单的理解，就是一群大牛为了解决一个问题而设计的一个特定的数据结构，that&#39;s all&lt;/code&gt;。&lt;/p&gt;

&lt;h3 id=&quot;what-is-dag---&quot;&gt;2.2 What is DAG - 趣说有向无环图&lt;/h3&gt;

&lt;p&gt;DAG - Direct Acyclic Graph，有向无环图，好久没看图片了，先发个图片来理解理解吧。
&lt;img src=&quot;../../images/dag.jpg&quot; alt=&quot;DAG&quot; /&gt;&lt;br /&gt;
要理解DAG，只需弄明白三个概念就可以毕业了，首先，我们假设上图图二中的A,B,C,D,E都代表spark里不同的RDD：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;图：图是表达&lt;code class=&quot;highlighter-rouge&quot;&gt;RDD Lineage&lt;/code&gt;信息的一个结构，在 spark 中，大部分 RDD 都是通过其他 RDD 进行转换而来的，比如说上图图二中，B和D都是通过A转换而来的，而C是通过B转换而来，E的话是通过B和D一起转换来的。&lt;/li&gt;
  &lt;li&gt;有向：有向就更容易理解了，简单来说就是 linage 是一个 top-down 的结构，而且是时间序列上的 top-down 结构，这里如果没有理解的话，我们在下面讲“无环”这个概念时一起说明。&lt;/li&gt;
  &lt;li&gt;无环：这里就是重点要理解的地方了，spark 的优化器在这里也发挥了很大的作用。首先，我们先理解一下无环的概念，假设有图三中左下 B,D,E 这样一个 RDD 转换图，那当我们的需要执行 D.collect 操作的时候，就会引发一个死循环了。不过，仔细想过的话，就会知道，“无环”这个问题其实已经在“有向”这个概念中提现了，上面说的“有向”，其实更详细的说是一个时间上的先来后到，即祖先与子孙的关系，是不可逆的。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;what-is-data-locality---rdd&quot;&gt;2.3 What is Data Locality - RDD的位置可见性&lt;/h3&gt;
&lt;p&gt;这个问题就不重复造轮子了，直接引用Quora上的一个&lt;a href=&quot;https://www.quora.com/How-do-I-make-clear-the-concept-of-RDD-in-Spark&quot;&gt;问答了&lt;/a&gt;:&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;RDD is a dataset which is &lt;code class=&quot;highlighter-rouge&quot;&gt;distributed&lt;/code&gt;, that is, it is divided into &lt;code class=&quot;highlighter-rouge&quot;&gt;&quot;partitions&quot;&lt;/code&gt;. Each of these partitions can be present in the memory or disk of different machines. If you want Spark to process the RDD, then Spark needs to &lt;code class=&quot;highlighter-rouge&quot;&gt;launch one task per partition of the RDD&lt;/code&gt;. It’s best that each task be sent to the machine have the partition that task is supposed to process. In that case, the task will be able to read the data of the partition from the local machine. Otherwise, the task would have to pull the partition data over the network from a different machine, which is less efficient. This scheduling of tasks (that is, allocation of tasks to machines) such that the tasks can read data “locally” is known as “&lt;code class=&quot;highlighter-rouge&quot;&gt;locality aware scheduling&lt;/code&gt;”.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;what-is-lazy-evaluation---&quot;&gt;2.4 What is Lazy Evaluation - 神马叫惰性求值&lt;/h3&gt;
&lt;p&gt;本来不想叫“惰性求值”的，看到“惰”这个字实在是各种不爽，实际上，我觉得应该叫”后续求值”，”按需计算”，”晚点搞”这类似的，哈哈。这几天一直在想应该怎么简单易懂地来表达Lazy Evaluation这个概念，本来打算引用MongoDB的Cursor来类比一下的，可总觉得还是小题大做了。这个概念就懒得解释了，主要是觉得太简单了，没有必要把事情搞得这么复杂，哈哈。&lt;/p&gt;

&lt;h3 id=&quot;what-is-narrowwide-dependency---rdd&quot;&gt;2.5 What is Narrow/Wide Dependency - RDD的宽依赖和窄依赖&lt;/h3&gt;
&lt;p&gt;首先，先从原文看看宽依赖和窄依赖各自的定义。&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;narrow dependencies&lt;/code&gt;: where each partition of the parent RDD is used by at most one partition of the child RDD, &lt;code class=&quot;highlighter-rouge&quot;&gt;wide dependencis&lt;/code&gt;, where multiple child partitions may depend on it.&lt;/p&gt;

&lt;p&gt;按照&lt;a href=&quot;http://shiyanjun.cn/archives/744.html&quot;&gt;这篇RDD论文中文译文&lt;/a&gt;的解释，窄依赖是指子RDD的每个分区依赖于常数个父分区（即与数据规模无关）；宽依赖指子RDD的每个分区依赖于所有父RDD分区。暂且不说这样理解是否有偏差，我们先来从两个方面了解下计算一个窄依赖的子RDD和一个宽依赖的RDD时具体都有什么区别，然后再回顾这个定义。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;计算方面：
    &lt;ul&gt;
      &lt;li&gt;计算窄依赖的子RDD：可以在某一个计算节点上直接通过父RDD的某几块数据（通常是一块）计算得到子RDD某一块的数据；&lt;/li&gt;
      &lt;li&gt;计算宽依赖的子RDD：子RDD某一块数据的计算必须等到它的父RDD所有数据都计算完成之后才可以进行，而且需要对父RDD的计算结果进行hash并传递到对应的节点之上；&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;容错恢复方面：
    &lt;ul&gt;
      &lt;li&gt;窄依赖：当父RDD的某分片丢失时，只有丢失的那一块数据需要被重新计算；&lt;/li&gt;
      &lt;li&gt;宽依赖：当父RDD的某分片丢失时，需要把父RDD的所有分区数据重新计算一次，计算量明显比窄依赖情况下大很多；&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-1&quot;&gt;3. 尚未提到的一些重要概念&lt;/h2&gt;
&lt;p&gt;还有一些基本概念上面没有提到，一些是因为自己还没怎么弄清楚，一些是觉得重要但是容易理解的，所以就先不记录下来了。比如说：粗粒度、细粒度；序列化和反序列化等。&lt;/p&gt;

&lt;h2 id=&quot;next&quot;&gt;4. Next&lt;/h2&gt;

&lt;p&gt;基础的概念和理论都讲得差不多了，该小试牛刀了，哈哈。&lt;/p&gt;

&lt;p&gt;下几篇的安排：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;列一些学习 spark 比较好的&lt;code class=&quot;highlighter-rouge&quot;&gt;资源&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;详细从 job，stage，task 的定义来谈谈 spark 的运行原理&lt;/li&gt;
  &lt;li&gt;准备几个稍稍复杂一点的&lt;code class=&quot;highlighter-rouge&quot;&gt;例子&lt;/code&gt;, 例子个数根据时间安排发布
    &lt;ul&gt;
      &lt;li&gt;spark 在金融领域的应用之 指数相似度计算&lt;/li&gt;
      &lt;li&gt;spark 在搜索领域的应用之 pagerank&lt;/li&gt;
      &lt;li&gt;spark 在社交领域的应用之 评分计算&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;开始讲 &lt;code class=&quot;highlighter-rouge&quot;&gt;dataframe 和 datasets&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-2&quot;&gt;5. 打开微信，扫一扫，点一点，棒棒的，^_^&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;../images/wechat_pay.png&quot; alt=&quot;wechat_pay.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;参考文章&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.csdn.net/anzhsoft/article/details/39851421&quot;&gt;Spark技术内幕：究竟什么是RDD&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf&quot;&gt;Resilient Distributed Datasets: A Fault-Tolerant Abstraction for
In-Memory Cluster Computing&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://shiyanjun.cn/archives/744.html&quot;&gt;RDD 论文中文版&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-4&quot;&gt;本系列文章链接&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;../introduction-to-spark&quot;&gt;『 Spark 』1. spark 简介 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;../spark-questions-concepts&quot;&gt;『 Spark 』2. spark 基本概念解析 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;../spark-programming-model&quot;&gt;『 Spark 』3. spark 编程模式 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;../spark-what-is-rdd&quot;&gt;『 Spark 』4. spark 之 RDD &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;../spark-resouces-blogs-paper&quot;&gt;『 Spark 』5. 这些年，你不能错过的 spark 学习资源 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;../deep-into-spark-exection-model&quot;&gt;『 Spark 』6. 深入研究 spark 运行原理之 job, stage, task&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;../spark-dataframe-introduction&quot;&gt;『 Spark 』7. 使用 Spark DataFrame 进行大数据分析&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
   <entry>
     <title>『 Spark 』3. spark 编程模式</title>
     <link href="/spark-programming-model"/>
     <updated>2016-03-04T00:00:00+08:00</updated>
     <id>/spark-programming-model</id>
     <content type="html">&lt;h2 id=&quot;section&quot;&gt;写在前面&lt;/h2&gt;

&lt;p&gt;本系列是综合了自己在学习spark过程中的理解记录 ＋ 对参考文章中的一些理解 ＋ 个人实践spark过程中的一些心得而来。写这样一个系列仅仅是为了梳理个人学习spark的笔记记录，所以一切以能够理解为主，没有必要的细节就不会记录了，而且文中有时候会出现英文原版文档，只要不影响理解，都不翻译了。若想深入了解，最好阅读参考文章和官方文档。&lt;/p&gt;

&lt;p&gt;其次，本系列是基于目前最新的 spark 1.6.0 系列开始的，spark 目前的更新速度很快，记录一下版本好还是必要的。 &lt;br /&gt;
最后，如果各位觉得内容有误，欢迎留言备注，所有留言 24 小时内必定回复，非常感谢。   &lt;br /&gt;
Tips: 如果插图看起来不明显，可以：1. 放大网页；2. 新标签中打开图片，查看原图哦。&lt;/p&gt;

&lt;h2 id=&quot;spark-&quot;&gt;1. spark 基本编程模式&lt;/h2&gt;

&lt;p&gt;spark 里有两个很重要的概念：SparkContext [一般简称为 sc] 和 RDD，在上一篇文章中 &lt;a href=&quot;../spark-questions-concepts&quot;&gt;『 Spark 』2. spark 基本概念解析 &lt;/a&gt; 有讲到。可以说，sc 和 RDD 贯穿了 spark app 的大部分生命周期，从 app 的初始化，到数据的清洗，计算，到最后获取，展示结果。&lt;/p&gt;

&lt;p&gt;为了更加深入的了解 RDD 和基于 RDD 的编程模型，我们先把 RDD 的属性简单的分一个类，然后再通过一张流程图来理解。&lt;/p&gt;

&lt;h3 id=&quot;rdd-&quot;&gt;1.1 RDD 的属性&lt;/h3&gt;

&lt;p&gt;接触过 RDD 的人肯定都知道 &lt;em&gt;transform&lt;/em&gt; 和 &lt;em&gt;action&lt;/em&gt; 这两个核心概念，甚至很多人都认为 RDD 仅仅有 &lt;em&gt;transform&lt;/em&gt; 和 &lt;em&gt;action&lt;/em&gt; 这两个概念。殊不知其实 RDD 里面还有很多其他方法，下面我们来简单的分个类，在看这里的时候最好参考一下官方的 &lt;a href=&quot;http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD&quot;&gt;api 文档&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;RDD
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;action&lt;/code&gt;     : count, take, sample, first, collect  …&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;transform&lt;/code&gt;  : foreach, glom, map …&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;method&lt;/code&gt;     : cache, checkpoint, id, isCheckpointed, isEmpty, keys, lookup, max, mean, name, setName …&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;property&lt;/code&gt;   : context&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;看到了吗，这里其实 RDD 其实有很多既不是 &lt;em&gt;transform&lt;/em&gt; 也不是 &lt;em&gt;action&lt;/em&gt; 的函数和属性，在编写 spark app 的时候，其实很多时候我们都会用到那些 method，这样在开发调试过程中都会更加方便。比如说 &lt;code class=&quot;highlighter-rouge&quot;&gt;cache&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;setName&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;lookup&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;id&lt;/code&gt; 这些，在开发过程中都很有用。&lt;/p&gt;

&lt;h3 id=&quot;spark--1&quot;&gt;1.2 spark 编程模式图&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;../images/spark-programming-model.jpg&quot; alt=&quot;spark-programming-model.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;如图所示，我们构建 spark app，一般都是三个步骤:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;加载数据集&lt;/em&gt;，这里的数据集大概分为两组:
    &lt;ul&gt;
      &lt;li&gt;一种是不变的，静态数据集，大多数场景都是从数据库，文件系统上面加载进来&lt;/li&gt;
      &lt;li&gt;另一种是动态的数据集，一般做 streaming 应用的时候用到，大多数场景是通过 socket 来加载数据，复杂场景可以通过文件系统，akka actors，kafka，kinesis 和 一些第三方提供的 streaming api [twitter 等] 来作为数据源加载数据&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;处理数据&lt;/em&gt;，这是重点中的重点，不过不外乎都是从三个方面来完成这里的数据清理，逻辑运算等:
    &lt;ul&gt;
      &lt;li&gt;自定义的一些复杂处理函数或者第三方包 [下面我们称为函数集]&lt;/li&gt;
      &lt;li&gt;通过 RDD 的 transform，action 和函数集来完成整个处理，计算流程&lt;/li&gt;
      &lt;li&gt;通过 RDD 提供的 cache，persist，checkpoint 方法把一些处理流程中的重要处理节点和常用数据缓存和备份，以加速处理，计算速度&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;em&gt;结果展示&lt;/em&gt;，这里一般情况都是使用 RDD 的 collect，take，first，top 等方法把结果取出来，更常用的是先把结果取出来，放到一个数据库或文件系统上，然后再提供给专门展示结果的另一个 application 使用。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;mc-monte-carlo&quot;&gt;2. 例子：MC [Monte Carlo]&lt;/h2&gt;

&lt;p&gt;下面我将从几个方面来介绍这个例子：首先是介绍蒙特卡罗方法的基本概念和应用，然后是介绍如何用蒙特卡罗方法来估算 pi 的值，最后是看在 spark 集群中如何用多种方法来实现一个蒙特卡洛应用来计算 pi 的值。&lt;/p&gt;

&lt;h3 id=&quot;section-1&quot;&gt;2.1 蒙特卡罗方法介绍&lt;/h3&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;## from wiki 
Monte Carlo methods (or Monte Carlo experiments) are a broad class of 
computational algorithms that rely on repeated random sampling to obtain 
numerical results. They are often used in physical and mathematical problems 
and are most useful when it is difficult or impossible to use other mathematical 
methods. Monte Carlo methods are mainly used in three distinct problem 
classes:[1] optimization, numerical integration, and generating draws from 
a probability distribution.

## 
总的来说，蒙特卡罗是一种基于随机样本实验来进行估值的一种计算方法。&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3 id=&quot;pi-&quot;&gt;2.2 蒙特卡罗方法估算 pi 值原理&lt;/h3&gt;

&lt;p&gt;用蒙特卡罗方法估算 pi 值，核心方法是利用正方形和圆形面积的比例：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;首先，我们在坐标轴上构造一个边长为 1 的正方形&lt;/li&gt;
  &lt;li&gt;其次，我们以 (0, 0) 为圆心，构造一个半径为 1 的圆形&lt;/li&gt;
  &lt;li&gt;此时我们知道这个圆形有 1/4 是在正方形中的，正方形的面积和这 1/4 圆的面积分别是：1 和 pi/4，即 1/4 圆的面积和正方形面积之比刚好是 pi/4&lt;/li&gt;
  &lt;li&gt;然后通过蒙特卡罗模拟，看看这个比例大概是多少，模拟方法如下：
    &lt;ul&gt;
      &lt;li&gt;随机扔 n 个点 (x, y)，其中 x, y 都在 0 和 1 之间&lt;/li&gt;
      &lt;li&gt;如果 x^2 + y^2 &amp;lt; 0，则把这个点标注为红色，表示这个点落在圆内&lt;/li&gt;
      &lt;li&gt;最后数数有 n 个点中有多少点是红点，即落在圆内，假设点数为 m&lt;/li&gt;
      &lt;li&gt;则这个 1/4 圆的面积和正方形面积的比例应该是：m/n，即 m/n = pi/4 =&amp;gt; pi = 4*m/n&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../images/mc.gif&quot; alt=&quot;mc.gif&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;python--pi-&quot;&gt;2.3 Python 实现蒙特卡罗方法估算 pi 值&lt;/h3&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;import numpy as np

def mc_pi(n=100):
    &quot;&quot;&quot;Use Monte Calo Method to estimate pi.
    &quot;&quot;&quot;
    m = 0
    i = 0
    while i &amp;lt; n:
        x, y = np.random.rand(2)
        if x**2 + y**2 &amp;lt; 1:
            m += 1
        i += 1

    pi = 4. * m / n
    res = {&#39;total_point&#39;: n, &#39;point_in_circle&#39;: m, &#39;estimated_pi&#39;: pi}
    
    return res&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;../images/spark-programming-model-11.jpg&quot; alt=&quot;spark-programming-model-11.jpg&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;spark--2&quot;&gt;2.4 在 spark 集群中实现蒙特卡罗方法&lt;/h3&gt;

&lt;p&gt;我们按照上面写的三大步骤来写这个 spark 应用：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;加载数据集&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;### iterate number&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;total&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;local_collection&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;xrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;total&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;### parallelize a data set into the cluster&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;rdd&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parallelize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;local_collection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;       \
        &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;parallelized_data&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;        \
        &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;处理数据&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;### randomly generate points&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;map_func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;element&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;       &lt;span class=&quot;c&quot;&gt;## [0, 1)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;       &lt;span class=&quot;c&quot;&gt;## [0, 1)&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;             &lt;span class=&quot;c&quot;&gt;## random point&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;map_func_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;element&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;element&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;rdd2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rdd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;map_func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;            \
          &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;random_point&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  \
          &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;### calculate the number of points in and out the circle&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;rdd3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rdd2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;map_func_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;                 \
           &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;points_in_out_circle&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; \
           &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cache&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;结果展示&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;### how many points are in the circle&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;in_circle&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rdd3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;reduce&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;operator&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;4.&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;in_circle&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;total&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;iterate {} times&#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;total&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;estimated pi : {}&#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3 id=&quot;seems-a-little-complex-really&quot;&gt;2.5 Seems a little complex, really?&lt;/h3&gt;

&lt;p&gt;上面这个例子，可能会让一些初步接触 spark 的人很困惑，”明明几行代码就能解决的问题在 spark 里还有按照这些步骤写这么多代码？难道是老湿又骗我了吗？”。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/wawawa.gif&quot; alt=&quot;wawawa.gif&quot; /&gt;&lt;/p&gt;

&lt;p&gt;其实，就从上面这个例子看起来，似乎 spark 真的没有什么优势，但是，上面这个例子的目的是表明 spark 的编程模式，如果你还不相信，可以把模拟次数加到千万或者亿次以上看看效果。&lt;/p&gt;

&lt;p&gt;如果，如果你还是纠结于 “我骗了你，spark 没有梦想中的那么好” 的话，那看下面这一行代码吧，它也完成了同样的事情：&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;### version 1&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parallelize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;xrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;total&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;                                 \
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()))&lt;/span&gt;        \
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;         \
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;reduce&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;                               \
    &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;total&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;### version 2&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parallelize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;xrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;total&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;                                  \
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  \
    &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;reduce&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;                                \
    &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;total&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;next&quot;&gt;3. Next&lt;/h2&gt;

&lt;p&gt;下一篇，介绍 spark 的 RDD，之后会单独介绍 spark 的 dataframe 和 datasets。&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;4. 打开微信，扫一扫，点一点，棒棒的，^_^&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;../images/wechat_pay.png&quot; alt=&quot;wechat_pay.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;参考文章&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;../files/spark-rdd-paper.pdf&quot;&gt;spark-rdd-paper : Resilient Distributed Datasets: A Fault-Tolerant Abstraction for
In-Memory Cluster Computing&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://spark.apache.org/docs/latest/api/python/pyspark.html&quot;&gt;spark python API&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext&quot;&gt;spark context API&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://snap.stanford.edu/data/&quot;&gt;机器学习相关数据集-斯坦福&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/apache/spark/blob/master/examples/src/main/python/pagerank.py&quot;&gt;spark pagerank example&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://latex.91maths.com/&quot;&gt;latex online editor 在线latex公式编辑器&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.ruanyifeng.com/blog/2015/07/monte-carlo-method.html&quot;&gt;阮一峰：蒙特卡罗&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Monte_Carlo_method&quot;&gt;蒙特卡罗，wikipedia&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://blog.sciencenet.cn/blog-324394-292355.html&quot;&gt;科学网：蒙特卡罗&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-4&quot;&gt;本系列文章链接&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;../introduction-to-spark&quot;&gt;『 Spark 』1. spark 简介 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;../spark-questions-concepts&quot;&gt;『 Spark 』2. spark 基本概念解析 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;../spark-programming-model&quot;&gt;『 Spark 』3. spark 编程模式 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;../spark-what-is-rdd&quot;&gt;『 Spark 』4. spark 之 RDD &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;../spark-resouces-blogs-paper&quot;&gt;『 Spark 』5. 这些年，你不能错过的 spark 学习资源 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;../deep-into-spark-exection-model&quot;&gt;『 Spark 』6. 深入研究 spark 运行原理之 job, stage, task&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;../spark-dataframe-introduction&quot;&gt;『 Spark 』7. 使用 Spark DataFrame 进行大数据分析&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
   <entry>
     <title>如何做好一个 presentation</title>
     <link href="/how-to-be-a-amzing-speaker"/>
     <updated>2016-02-25T00:00:00+08:00</updated>
     <id>/how-to-be-a-amzing-speaker</id>
     <content type="html">&lt;h2 id=&quot;section&quot;&gt;写在前面&lt;/h2&gt;

&lt;p&gt;从 14.04 至今，已经工作将近 2 年了，这两年参加过很多次分享会，自己也当过十来次 speaker，有讲得不差的时候，也有讲得最差的时候：&lt;a href=&quot;../the-badest-thing-in-my-life&quot;&gt;今天发生了一件坏事，也发生了一件好事&lt;/a&gt;。参加这么多次分享会后，给我印象最深的其实并不是某某技术有多厉害，某某产品有多实用，给我感触最深的恰恰是主讲人。我发现，其实一个分享会是否成功，其实很少部分在于分享的主题，而在于主讲人。很多时候，在 speaker 开讲的3分钟内，我就会毫不犹豫的拿起手机；很少时候，我会提笔在笔记本上记下准备提问的问题。&lt;/p&gt;

&lt;p&gt;本篇是记录我这两年来对分享会的一些感触，目的是总结一些在分享会方面的经验，希望以后自己做 speaker 的时候，能一次比一次高效，成功。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;1. 我常遇见的问题&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;主讲人迟到 or 踩点到&lt;/code&gt;   &lt;br /&gt;
  如果你足够大牌，或者真的有事耽误，那请务必准时到，而且最好提前通知 audience。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;ppt，demo 字体太小，颜色太难为人&lt;/code&gt;  &lt;br /&gt;
  时刻谨记，你做的 ppt，demo 是给参会者看的，而不是给你自己看的。所以，务必注意字体，也不要用那些逆天的颜色搭配。
  你别跟我说你不懂颜色搭配，因为不懂不是你的错，但不懂去 google 就是你的错了，可以看看我后面的参考文档。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;钻牛角尖&lt;/code&gt;  &lt;br /&gt;
  这个问题经常在技术型分享会中遇到，几个工程师容易沉溺到一两个特别细节的问题里去争论。我是很赞成积极甚至是激烈的讨论的，但是
  这个讨论的时间地点都不对，耽误了大多数参会者的时间，甚至还会影响 speaker 的进度。这种情况我推荐把问题简单记录下来，私下讨论，并且讨论完成后给所有参会者发邮件说明就可以了。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;讲完整个 slide 才有 Q&amp;amp;A&lt;/code&gt; &lt;br /&gt;
  这个其实问题也不大，但我更推荐在中途有一次 Q&amp;amp;A 或者每讲完一个 agenda 就来一次 Q&amp;amp;A。这样的好处是能保证参会者跟住你的脚步，
  也能保证会议的活跃度。难点是 speaker 需要控制住每一次 Q&amp;amp;A 的时间，保证能按时完成分享。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;presentation&quot;&gt;3. 如何做好一个 presentation&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;提前准备&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;ppt, demo: 字体，颜色，如果有程序 demo 的话最好先完整运行一次，然后备份一下运行结果，以防在 present 的时候运行
  出意外，得不到理想的结果；&lt;/li&gt;
      &lt;li&gt;ppt 简洁最好，不要花哨，如果是技术性会议，我比较推荐 markdown 写的文档，简洁明了够用；&lt;/li&gt;
      &lt;li&gt;演示设备：提前10分钟到场，测试会场的电脑，音效等是否ok；如果自己带电脑的话，最好再提前几分钟到场，把自己的电脑和外接屏幕，投影仪
  等调试好，电脑网络设置好，slide 打开看看外屏显示效果是否ok，demo程序是否 ready；网络这里我是有惨痛经历的，所以我特别注重网络这块；&lt;/li&gt;
      &lt;li&gt;会议资料：可以提前把会议资料，ppt，demo 等放到网上去，这样对远程参会，坐得比较远的听众比较有好处；&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;表达清晰&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;语速放慢，概念，问题，说清楚&lt;/li&gt;
      &lt;li&gt;说自己明确知道的，弱有人问到自己不懂的，可以记下来，放到会后纪要里&lt;/li&gt;
      &lt;li&gt;有人提问的话，在回答前先确认自己知道别人的问题，然后向大众复述一遍问题，再做解答 [这个在听国外的分享时经常遇到，非常好]&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;介绍与会议有直接关系的参会者&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;这个很有必要，比如说你来介绍一个产品，但是这个产品依赖于某个部门的服务，那在开讲的时候最好介绍一下那个部门，那个服务。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;会后总结会议内容，给参会者发送会议回顾和总结[depends]&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;这个不一定做，视情况而定。如果是公司内部的，我推荐在会议完成后，自己在 slide 的基础上再总结一下，把 Q&amp;amp;A 环节的问题也总结进去，
  然后群发给参会者。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-2&quot;&gt;4. 细节，及亲身经验&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;关于技术分享之现场跑代码
    &lt;ul&gt;
      &lt;li&gt;提前去会场跑一次，确保网络，环境，代码无问题；&lt;/li&gt;
      &lt;li&gt;例子 1-2 个就行了，最好就举一个例子，但是这个例子里面要涵盖 简单－升级－高级 部分，并且把你想表达的功能点都表达到；&lt;/li&gt;
      &lt;li&gt;跑代码的时候，最重要的是代码背后的事情和结果展示，每运行一行代码，一个函数，一定要把代码背后的事情表达清楚，甚至是
  import 一个 package 的时候都最好简单介绍一些；比如说这个 &lt;a href=&quot;https://vimeo.com/155716963&quot;&gt;demo&lt;/a&gt;，主讲人就只举了
  一个例子，但是需要注意的是他只讲了20分钟左右；&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;关于背景介绍
    &lt;ul&gt;
      &lt;li&gt;开始介绍背景前先互动一下，问问现场有多少人了解，接触过相关技术，来决定花多少时间来讲介绍这部分；比如说你要讲的主题是
  spark，可以提前问问听众对spark，hadoop，map reduce，iterative computing，distributed computing 的了解情况；&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;presentation-&quot;&gt;4. 如何判断你这个 presentation 是否达到效果&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;会议中低头玩手机的人多吗&lt;/code&gt; &lt;br /&gt;
  偶尔有一两个人玩很正常的，因为也许是人家正在处理正事。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Q&amp;amp;A 是否活跃&lt;/code&gt; &lt;br /&gt;
  这个没办法量化，但是一个1小时的分享会下来，怎么说至少也应该要有3个问题才算活跃吧。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Q&amp;amp;A 的问题是否都是你一个人回答的&lt;/code&gt; &lt;br /&gt;
  如果有参会者主动回答别人提的问题，那就最好了，哈哈。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-3&quot;&gt;参考文档&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.wikihow.com/Do-a-Presentation-in-Class&quot;&gt;How to Do a Presentation in Class&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.zhihu.com/question/30709836&quot;&gt;怎样的 PPT 配色会让人觉得舒服&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=dEDcc0aCjaA&quot;&gt;How to Do a Presentation - 5 Steps to a Killer Opener&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
   <entry>
     <title>『 Spark 』2. spark 基本概念解析</title>
     <link href="/spark-questions-concepts"/>
     <updated>2016-02-05T00:00:00+08:00</updated>
     <id>/spark-questions-concepts</id>
     <content type="html">&lt;h2 id=&quot;section&quot;&gt;写在前面&lt;/h2&gt;

&lt;p&gt;本系列是综合了自己在学习spark过程中的理解记录 ＋ 对参考文章中的一些理解 ＋ 个人实践spark过程中的一些心得而来。写这样一个系列仅仅是为了梳理个人学习spark的笔记记录，所以一切以能够理解为主，没有必要的细节就不会记录了，而且文中有时候会出现英文原版文档，只要不影响理解，都不翻译了。若想深入了解，最好阅读参考文章和官方文档。&lt;/p&gt;

&lt;p&gt;其次，本系列是基于目前最新的 spark 1.6.0 系列开始的，spark 目前的更新速度很快，记录一下版本好还是必要的。 &lt;br /&gt;
最后，如果各位觉得内容有误，欢迎留言备注，所有留言 24 小时内必定回复，非常感谢。   &lt;br /&gt;
Tips: 如果插图看起来不明显，可以：1. 放大网页；2. 新标签中打开图片，查看原图哦。&lt;/p&gt;

&lt;h2 id=&quot;application&quot;&gt;1. Application&lt;/h2&gt;

&lt;p&gt;用户在 spark 上构建的程序，包含了 driver 程序以及在集群上运行的程序代码，物理机器上涉及了 driver，master，worker 三个节点.&lt;/p&gt;

&lt;h2 id=&quot;driver-program&quot;&gt;2. Driver Program&lt;/h2&gt;

&lt;p&gt;创建 sc ，定义 udf 函数，定义一个 spark 应用程序所需要的三大步骤的逻辑：加载数据集，处理数据，结果展示。&lt;/p&gt;

&lt;h2 id=&quot;cluster-manager&quot;&gt;3. Cluster Manager&lt;/h2&gt;

&lt;p&gt;集群的资源管理器，在集群上获取资源的外部服务。
拿 Yarn 举例，客户端程序会向 Yarn 申请计算我这个任务需要多少的 memory，多少 CPU，etc。
然后 Cluster Manager 会通过调度告诉客户端可以使用，然后客户端就可以把程序送到每个 Worker Node 上面去执行了。&lt;/p&gt;

&lt;h2 id=&quot;worker-node&quot;&gt;4. Worker Node&lt;/h2&gt;

&lt;p&gt;集群中任何一个可以运行spark应用代码的节点。Worker Node就是物理节点，可以在上面启动Executor进程。&lt;/p&gt;

&lt;h2 id=&quot;executor&quot;&gt;5. Executor&lt;/h2&gt;

&lt;p&gt;在每个 Worker Node 上为某应用启动的一个进程，该进程负责运行任务，并且负责将数据存在内存或者磁盘上，每个任务都有各自独立的 Executor。
Executor 是一个执行 Task 的容器。它的主要职责是：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;初始化程序要执行的上下文 SparkEnv，解决应用程序需要运行时的 jar 包的依赖，加载类。&lt;/li&gt;
  &lt;li&gt;同时还有一个 ExecutorBackend 向 cluster manager 汇报当前的任务状态，这一方面有点类似 hadoop的 tasktracker 和 task。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;总结：Executor 是一个应用程序运行的监控和执行容器。&lt;/p&gt;

&lt;h2 id=&quot;jobs&quot;&gt;6. Jobs&lt;/h2&gt;

&lt;p&gt;包含很多 task 的并行计算，可以认为是 Spark RDD 里面的 action，每个 action 的触发会生成一个job。
用户提交的 Job 会提交给 DAGScheduler，Job 会被分解成 Stage，Stage 会被细化成 Task，Task 简单的说就是在一个数据 partition 上的单个数据处理流程。关于 job，stage，task，详细可以参考这篇文章：&lt;a href=&quot;../deep-into-spark-exection-model&quot;&gt;『 Spark 』6. 深入研究 spark 运行原理之 job, stage, task&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/spark-web-ui-job.jpg&quot; alt=&quot;spark-web-ui-job.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A job is triggered by an &lt;code class=&quot;highlighter-rouge&quot;&gt;action&lt;/code&gt;, like &lt;code class=&quot;highlighter-rouge&quot;&gt;count()&lt;/code&gt; or &lt;code class=&quot;highlighter-rouge&quot;&gt;saveAsTextFile()&lt;/code&gt;, click on a job to see info about the &lt;code class=&quot;highlighter-rouge&quot;&gt;stages&lt;/code&gt; of &lt;code class=&quot;highlighter-rouge&quot;&gt;tasks&lt;/code&gt; inside it.&lt;/p&gt;

&lt;h2 id=&quot;stage&quot;&gt;7. Stage&lt;/h2&gt;

&lt;p&gt;一个 Job 会被拆分为多组 Task，每组任务被称为一个 Stage 就像 Map Stage， Reduce Stage。&lt;/p&gt;

&lt;p&gt;Stage 的划分在 RDD 的论文中有详细的介绍，简单的说是以 shuffle 和 result 这两种类型来划分。
在 Spark 中有两类 task:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;shuffleMapTask&lt;/p&gt;

    &lt;p&gt;输出是shuffle所需数据, stage的划分也以此为依据，shuffle之前的所有变换是一个stage，shuffle之后的操作是另一个stage。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;resultTask，&lt;/p&gt;

    &lt;p&gt;输出是result，比如 rdd.parallize(1 to 10).foreach(println) 这个操作没有shuffle，直接就输出了，那么只有它的task是resultTask，stage也只有一个；如果是rdd.map(x =&amp;gt; (x, 1)).reduceByKey(_ + _).foreach(println), 这个job因为有reduce，所以有一个shuffle过程，那么reduceByKey之前的是一个stage，执行shuffleMapTask，输出shuffle所需的数据，reduceByKey到最后是一个stage，直接就输出结果了。如果job中有多次shuffle，那么每个shuffle之前都是一个stage。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;task&quot;&gt;8. Task&lt;/h2&gt;

&lt;p&gt;被送到 executor 上的工作单元。&lt;/p&gt;

&lt;h2 id=&quot;partition&quot;&gt;9. Partition&lt;/h2&gt;

&lt;p&gt;Partition 类似 hadoop 的 Split，计算是以 partition 为单位进行的，当然 partition 的划分依据有很多，这是可以自己定义的，像 HDFS 文件，划分的方式就和 MapReduce 一样，以文件的 block 来划分不同的 partition。总而言之，Spark 的 partition 在概念上与 hadoop 中的 split 是相似的，提供了一种划分数据的方式。&lt;/p&gt;

&lt;h2 id=&quot;rdd&quot;&gt;10. RDD&lt;/h2&gt;

&lt;p&gt;先看看原文 &lt;a href=&quot;../files/spark-rdd-paper.pdf&quot;&gt;Resilient Distributed Datasets: A Fault-Tolerant Abstraction for
In-Memory Cluster Computing&lt;/a&gt; 是怎么介绍 RDD 的。&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;a &lt;code class=&quot;highlighter-rouge&quot;&gt;distributed memory abstraction&lt;/code&gt; that lets programmers perform in-memory computations on large clusters in a fault-tolerant manner.&lt;/p&gt;

&lt;p&gt;RDDs are motivated by two types of applications that current computing frameworks handle inefficiently:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;iterative algorithms;&lt;/li&gt;
  &lt;li&gt;interactive data mining tools;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In both cases, keeping data in memory can improve performance by an order of magnitude.&lt;/p&gt;

&lt;p&gt;To achieve fault tolerance efficiently, RDDs provide a restricted form of shared memory, based on coarsegrained transformations rather than fine-grained updates to shared state. However, we show that RDDs are expressive enough to capture a wide class of computations, including recent specialized programming models for iterative jobs, such as Pregel, and new applications that these models do not capture. We have implemented RDDs in a system called Spark, which we evaluate through a variety of user applications and benchmarks.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;每个RDD有5个主要的属性：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;一组分片（partition），即数据集的基本组成单位&lt;/li&gt;
  &lt;li&gt;一个计算每个分片的函数&lt;/li&gt;
  &lt;li&gt;对parent RDD的依赖，这个依赖描述了RDD之间的lineage&lt;/li&gt;
  &lt;li&gt;对于key-value的RDD，一个Partitioner，这是可选择的&lt;/li&gt;
  &lt;li&gt;一个列表，存储存取每个partition的preferred位置。对于一个HDFS文件来说，存储每个partition所在的块的位置。这也是可选择的&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;　　把上面这5个主要的属性总结一下，可以得出RDD的大致概念。首先要知道，RDD大概是这样一种表示数据集的东西，它具有以上列出的一些属性。是spark项目组设计用来表示数据集的一种数据结构。而spark项目组为了让RDD能handle更多的问题，又规定RDD应该是只读的，分区记录的一种数据集合中。可以通过两种方式来创建RDD：一种是基于物理存储中的数据，比如说磁盘上的文件；另一种，也是大多数创建RDD的方式，即通过其他RDD来创建【以后叫做转换】而成。而正因为RDD满足了这么多特性，所以spark把RDD叫做Resilient Distributed Datasets，中文叫做弹性分布式数据集。很多文章都是先讲RDD的定义，概念，再来说RDD的特性。我觉得其实也可以倒过来，通过RDD的特性反过来理解RDD的定义和概念，通过这种由果溯因的方式来理解RDD也未尝不可。反正对我个人而言这种方式是挺好的。&lt;/p&gt;

&lt;p&gt;RDD是Spark的核心，也是整个Spark的架构基础，可以总下出几个它的特性来：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;它是不变的数据结构存储&lt;/li&gt;
  &lt;li&gt;它是支持跨集群的分布式数据结构&lt;/li&gt;
  &lt;li&gt;可以根据数据记录的key对结构进行分区&lt;/li&gt;
  &lt;li&gt;提供了粗粒度的操作，且这些操作都支持分区&lt;/li&gt;
  &lt;li&gt;它将数据存储在内存中，从而提供了低延迟性&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;关于 rdd 的更多详情，可以参考这篇文章：&lt;a href=&quot;../spark-what-is-rdd&quot;&gt;『 Spark 』4. spark 之 RDD &lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;scparallelize&quot;&gt;11. sc.parallelize&lt;/h2&gt;

&lt;p&gt;先看看 api 文档里是怎么说的：&lt;a href=&quot;http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext.parallelize&quot;&gt;parallelize&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;parallelize(c, numSlices=None)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Distribute a local Python collection to form an RDD. Using xrange is recommended if the input represents a range for performance.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;简单的说，parallelize 就是把 driver 端定义的一个数据集，或者一个获取数据集的生成器，分发到 worker 上的 executor 中，以供后续分析。这种方式在测试代码逻辑时经常用到，但在构建真正的 spark 应用程序时很少会用到，一般都是从 hdfs 或者数据库去读取数据。&lt;/p&gt;

&lt;h2 id=&quot;code-distribute&quot;&gt;12. code distribute&lt;/h2&gt;

&lt;p&gt;提交 spark 应用时，spark 会把应用代码分发到所有的 worker 上面，应用依赖的包需要在所有的worker上都存在，有两种解决 worker 上相关包依赖的问题：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;选用一些工具统一部署 spark cluster；&lt;/li&gt;
  &lt;li&gt;在提交 spark 应用的时候，指定应用依赖的相关包，把 应用代码，应用依赖包 一起分发到 worker；&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;cache-priority&quot;&gt;13. cache priority&lt;/h2&gt;

&lt;p&gt;cache 是否支持 priority，目前不支持，而且 spark 里面对 rdd 的 cache 和我们常见的缓存系统是不一样的。细节可以找我讨论。&lt;/p&gt;

&lt;h2 id=&quot;cores&quot;&gt;14. cores&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;The number of cores to use on each executor. For YARN and standalone mode only. In standalone mode, setting this parameter allows an application to run multiple executors on the same worker, provided that there are enough cores on that worker. Otherwise, only one executor per application will run on each worker.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;每一个 core，相当于一个 worker 上的进程，这些进程会同时执行分配到这个 worker 上的任务。简单的说，就是 spark manager 把一个 job 切分几个 task 分发到 worker 上同步执行，而每个 worker 把分配给自己的 task 再切分成几个 subtask，分配给当前 worker 上的不同进程。&lt;/p&gt;

&lt;h2 id=&quot;memory&quot;&gt;15. Memory&lt;/h2&gt;

&lt;p&gt;分配给 spark 应用的内存是仅仅给 cache 数据用吗？&lt;/p&gt;

&lt;p&gt;不是，分配给 spark 应用的内存有三个方面的应用：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;spark 本身&lt;/li&gt;
  &lt;li&gt;spark 应用
    &lt;ul&gt;
      &lt;li&gt;spark 应用过程中 runtime 使用，比如 UDF 函数&lt;/li&gt;
      &lt;li&gt;spark 应用中的 cache&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;rdd-narrowwide-dependences&quot;&gt;16. RDD narrow/wide dependences&lt;/h2&gt;

&lt;p&gt;RDD 之间的依赖类别［ 或者，创建一个 RDD 的不同方法 ］&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/rdd-dependences.jpg&quot; alt=&quot;rdd-dependences.jpg&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;17. 本地内存与集群内存&lt;/h2&gt;

&lt;p&gt;所谓本地内存，是指在 driver 端的程序所需要的内存，由 driver 机器提供，一般用来生成测试数据，接受运算结果等；
所谓集群内存，是指提交到集群的作业能够向集群申请的最多内存使用量，一般用来存储关键数据；&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/spark-memory-cluster-and-driver.jpg&quot; alt=&quot;spark-memory-cluster-and-driver.jpg&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;18. 限制用户使用的内存&lt;/h2&gt;

&lt;p&gt;可以在启动 spark 应用的时候申请；完全可控。&lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;19. 当用户申请总资源超过当前集群总资源&lt;/h2&gt;

&lt;p&gt;FIFO 资源分配方式。&lt;/p&gt;

&lt;h2 id=&quot;sparkcontext--sc&quot;&gt;20. SparkContext [经常简称为 sc]&lt;/h2&gt;

&lt;p&gt;spark app 的起点和入口，一般用来加载数据集，生成第一个 rdd。&lt;/p&gt;

&lt;h2 id=&quot;rdd--cache-&quot;&gt;21. 对一个 rdd 多次 cache 会有什么影响吗？&lt;/h2&gt;

&lt;p&gt;不会，只会cache一次。&lt;a href=&quot;http://stackoverflow.com/questions/36195105/what-happens-if-i-cache-the-same-rdd-twice-in-spark/36195812#36195812&quot;&gt;stackoverflow&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;next&quot;&gt;4. Next&lt;/h2&gt;

&lt;p&gt;下一篇，通过几个简单的例子来介绍 spark 的基本编程模式。&lt;/p&gt;

&lt;h2 id=&quot;section-4&quot;&gt;5. 打开微信，扫一扫，点一点，棒棒的，^_^&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;../images/wechat_pay.png&quot; alt=&quot;wechat_pay.png&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-5&quot;&gt;参考文章&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;../files/spark-rdd-paper.pdf&quot;&gt;spark-rdd-paper : Resilient Distributed Datasets: A Fault-Tolerant Abstraction for
In-Memory Cluster Computing&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://spark.apache.org/docs/latest/api/python/pyspark.html&quot;&gt;spark python API&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-6&quot;&gt;本系列文章链接&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;../introduction-to-spark&quot;&gt;『 Spark 』1. spark 简介 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;../spark-questions-concepts&quot;&gt;『 Spark 』2. spark 基本概念解析 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;../spark-programming-model&quot;&gt;『 Spark 』3. spark 编程模式 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;../spark-what-is-rdd&quot;&gt;『 Spark 』4. spark 之 RDD &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;../spark-resouces-blogs-paper&quot;&gt;『 Spark 』5. 这些年，你不能错过的 spark 学习资源 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;../deep-into-spark-exection-model&quot;&gt;『 Spark 』6. 深入研究 spark 运行原理之 job, stage, task&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;../spark-dataframe-introduction&quot;&gt;『 Spark 』7. 使用 Spark DataFrame 进行大数据分析&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
   <entry>
     <title>『 Spark 』1. spark 简介</title>
     <link href="/introduction-to-spark"/>
     <updated>2016-02-01T00:00:00+08:00</updated>
     <id>/introduction-to-spark</id>
     <content type="html">&lt;h2 id=&quot;section&quot;&gt;写在前面&lt;/h2&gt;

&lt;p&gt;本系列是综合了自己在学习spark过程中的理解记录 ＋ 对参考文章中的一些理解 ＋ 个人实践spark过程中的一些心得而来。写这样一个系列仅仅是为了梳理个人学习spark的笔记记录，所以一切以能够理解为主，没有必要的细节就不会记录了，而且文中有时候会出现英文原版文档，只要不影响理解，都不翻译了。若想深入了解，最好阅读参考文章和官方文档。&lt;/p&gt;

&lt;p&gt;其次，本系列是基于目前最新的 spark 1.6.0 系列开始的，spark 目前的更新速度很快，记录一下版本好还是必要的。 &lt;br /&gt;
最后，如果各位觉得内容有误，欢迎留言备注，所有留言 24 小时内必定回复，非常感谢。   &lt;br /&gt;
Tips: 如果插图看起来不明显，可以：1. 放大网页；2. 新标签中打开图片，查看原图哦。&lt;/p&gt;

&lt;h2 id=&quot;spark&quot;&gt;1. 如何向别人介绍 spark&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Apache Spark™ is a fast and general engine for large-scale data processing.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Apache Spark is a fast and general-purpose cluster computing system.  &lt;br /&gt;
It provides high-level APIs in &lt;code class=&quot;highlighter-rouge&quot;&gt;Java, Scala, Python and R&lt;/code&gt;, and an optimized engine that supports general execution graphs.  &lt;br /&gt;
It also supports a rich set of higher-level tools including :&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Spark SQL for SQL and structured data processing, extends to DataFrames and DataSets&lt;/li&gt;
  &lt;li&gt;MLlib for machine learning&lt;/li&gt;
  &lt;li&gt;GraphX for graph processing&lt;/li&gt;
  &lt;li&gt;Spark Streaming for stream data processing&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;spark-&quot;&gt;2. spark 诞生的一些背景&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;../images/introduction-to-spark-1.jpg&quot; alt=&quot;introduction-to-spark-1.jpg&quot; /&gt;
&lt;img src=&quot;../images/introduction-to-spark-2.jpg&quot; alt=&quot;introduction-to-spark-2.jpg&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Spark started in 2009, open sourced 2010, unlike the various specialized systems[hadoop, storm], Spark’s goal was to :&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;generalize MapReduce to support new apps within same engine
    &lt;ul&gt;
      &lt;li&gt;it’s perfectly compatible with hadoop, can run on Hadoop, Mesos, standalone, or in the cloud. It can access diverse data sources including HDFS, Cassandra, HBase, and S3.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;speed up iteration computing over hadoop.
    &lt;ul&gt;
      &lt;li&gt;use memory + disk instead of disk as data storage medium&lt;/li&gt;
      &lt;li&gt;design a new programming modal, RDD, which make the data processing more graceful [RDD transformation, action, distributed jobs, stages and tasks]&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../images/introduction-to-spark-4.jpg&quot; alt=&quot;introduction-to-spark-4.jpg&quot; /&gt;
&lt;img src=&quot;../images/introduction-to-spark-5.jpg&quot; alt=&quot;introduction-to-spark-5.jpg&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;spark-1&quot;&gt;3. 为何选用 spark&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;designed, implemented and used as libs, instead of specialized systems;
    &lt;ul&gt;
      &lt;li&gt;much more useful and maintainable&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../images/introduction-to-spark-3.jpg&quot; alt=&quot;introduction-to-spark-3.jpg&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;from history, it is designed and improved upon hadoop and storm, it has perfect genes;&lt;/li&gt;
  &lt;li&gt;documents, community, products and trends;&lt;/li&gt;
  &lt;li&gt;it provides sql, dataframes, datasets, machine learning lib, graph computing lib and activitily growth 3-party lib, easy to use, cover lots of use cases in lots field;&lt;/li&gt;
  &lt;li&gt;it provides ad-hoc exploring, which boost your data exploring and pre-processing and help you build your data ETL, processing job;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;next&quot;&gt;4. Next&lt;/h2&gt;

&lt;p&gt;下一篇，简单介绍 spark 里必须深刻理解的基本概念。&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;参考文章&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://stanford.edu/~rezab/sparkclass/slides/itas_workshop.pdf&quot;&gt;Intro to Apache Spark&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://litaotao.github.io/files/introduing_spark.pdf&quot;&gt;introducing spark&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;section-2&quot;&gt;本系列文章链接&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;../introduction-to-spark&quot;&gt;『 Spark 』1. spark 简介 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;../spark-questions-concepts&quot;&gt;『 Spark 』2. spark 基本概念解析 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;../spark-programming-model&quot;&gt;『 Spark 』3. spark 编程模式 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;../spark-what-is-rdd&quot;&gt;『 Spark 』4. spark 之 RDD &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;../spark-resouces-blogs-paper&quot;&gt;『 Spark 』5. 这些年，你不能错过的 spark 学习资源 &lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;../deep-into-spark-exection-model&quot;&gt;『 Spark 』6. 深入研究 spark 运行原理之 job, stage, task&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;../spark-dataframe-introduction&quot;&gt;『 Spark 』7. 使用 Spark DataFrame 进行大数据分析&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
   <entry>
     <title>『 读书笔记 』effective Python</title>
     <link href="/effective-python"/>
     <updated>2015-12-30T00:00:00+08:00</updated>
     <id>/effective-python</id>
     <content type="html">&lt;h2 id=&quot;pythonic-thinking&quot;&gt;1. Pythonic Thinking&lt;/h2&gt;

&lt;h3 id=&quot;know-which-version-of-python-youre-using&quot;&gt;1.1 know which version of python you’re using&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;two major python version;&lt;/li&gt;
  &lt;li&gt;multiple popular &lt;code class=&quot;highlighter-rouge&quot;&gt;runtimes&lt;/code&gt; for python: cpython, jython, ironpython, pypy, etc;&lt;/li&gt;
  &lt;li&gt;be sure that the command line for running python on your system is the version you want;&lt;/li&gt;
  &lt;li&gt;prefer python 3 in your next project;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;follow-the-pep-8-style-guide&quot;&gt;1.2 follow the pep 8 style guide&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;always follow the pep 8 style guide;&lt;/li&gt;
  &lt;li&gt;sharing a common style with the larger community facilitates collaboration with others;&lt;/li&gt;
  &lt;li&gt;using a consistent style, making it easier to maintain your code later;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;know-the-differences-between-bytes-str-and-unicode&quot;&gt;1.3 know the differences between bytes, str and unicode&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://wklken.me/posts/2013/08/31/python-extra-coding-intro.html&quot;&gt;PYTHON-进阶-编码处理小结&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.cnblogs.com/evening/archive/2012/04/19/2457440.html&quot;&gt;python encode和decode函数说明&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;in python 3, bytes contains sequences of 8-bit values, str contains sequences of unicode characters. bytes and str instances can’t be used together with operators.&lt;/li&gt;
  &lt;li&gt;in python 2, str contains sequences of 8-bit, unicode contains sequences of unicode characters. str and unicode can be used together with operators if the str only contains 7-bit ascii characters;&lt;/li&gt;
  &lt;li&gt;if you want to read or write binary data to/from a file, always open the file using a binary mode (like ‘rb’, ‘wb’);&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;wirte-helper-functions-instead-of-complex-expressions&quot;&gt;1.4 wirte helper functions instead of complex expressions&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;python’s syntax make it easy to write a single line expression that are overly difficult to read;&lt;/li&gt;
  &lt;li&gt;move complex single line expressions to helper functions;&lt;/li&gt;
  &lt;li&gt;the &lt;code class=&quot;highlighter-rouge&quot;&gt;if/else&lt;/code&gt; expression make it more readable to alternative using bool operators like &lt;code class=&quot;highlighter-rouge&quot;&gt;and&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;or&lt;/code&gt;;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;know-how-to-slice-sequence&quot;&gt;1.5 know how to slice sequence&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;avoid being verbose, do not use [0] and [len(sequence)] to fetch the first or end element of a sequence;&lt;/li&gt;
  &lt;li&gt;assign to a list slice will replace the origin data in that slice part even if the new data’s length does not equal with the old one;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;avoid-using-start-end-and-stride-in-a-single-slice&quot;&gt;1.6 avoid using start, end and stride in a single slice&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;specifying start, end and stride in a slice can be extremely confusing;&lt;/li&gt;
  &lt;li&gt;avoid negative stride if possible;&lt;/li&gt;
  &lt;li&gt;avoid using start, end and stride in a single slice, consider doing two assignments(one to slice, the other to stride) or using islice from the itertools module;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;use-list-comprehension-instead-of-map-and-filter&quot;&gt;1.7 use list comprehension instead of map and filter&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;list comprehension is more clearer than map and filter because it does not need another &lt;code class=&quot;highlighter-rouge&quot;&gt;lambda&lt;/code&gt; expression;&lt;/li&gt;
  &lt;li&gt;list comprehension allows to skip items from the input list, but map can not make that without using &lt;code class=&quot;highlighter-rouge&quot;&gt;filter&lt;/code&gt;;&lt;/li&gt;
  &lt;li&gt;dict and set also support comprehension;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;avoid-more-than-two-expressions-in-list-comprehension&quot;&gt;1.8 avoid more than two expressions in list comprehension&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;list comprehension support multiple levels of loops and multiple conditions per loop;&lt;/li&gt;
  &lt;li&gt;list comprehension with two expressions are difficult to read in some degree;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;consider-generator-for-large-list-comprehension&quot;&gt;1.9 consider generator for large list comprehension&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;list comprehension can consume large memory for large input;&lt;/li&gt;
  &lt;li&gt;generator can avoid memory issues by produce one item each time;&lt;/li&gt;
  &lt;li&gt;generator can execute very quickly when chained together;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;prefer-enumerate-over-range&quot;&gt;1.10 prefer enumerate over range&lt;/h3&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;index : {}, item : {}&#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;enumerate provides a concise syntax for looping an iterator and getting the index for each item;&lt;/li&gt;
  &lt;li&gt;prefer enumerate instead looping over a range and indexing for each item;&lt;/li&gt;
  &lt;li&gt;you can specify a number to the enumerate function, specifying the index you want to iterate with;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;use-zip-to-process-iterators-in-parallel&quot;&gt;1.11 use zip to process iterators in parallel&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;the &lt;code class=&quot;highlighter-rouge&quot;&gt;zip&lt;/code&gt; can be used to iterate over multiple iterators;&lt;/li&gt;
  &lt;li&gt;in python 3, &lt;code class=&quot;highlighter-rouge&quot;&gt;zip&lt;/code&gt; is a lazy generator, in python 2, &lt;code class=&quot;highlighter-rouge&quot;&gt;zip&lt;/code&gt; return all the results as a list, but you can use &lt;code class=&quot;highlighter-rouge&quot;&gt;izip&lt;/code&gt; from &lt;code class=&quot;highlighter-rouge&quot;&gt;itertools&lt;/code&gt; to make it a generator;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;zip&lt;/code&gt; truncates it’s output silently if you supply it with different length, if you cannot lose any data, use the &lt;code class=&quot;highlighter-rouge&quot;&gt;zip_longest&lt;/code&gt; from &lt;code class=&quot;highlighter-rouge&quot;&gt;itertools&lt;/code&gt;;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;avoid-else-blocks-after-for-and-while-loops&quot;&gt;1.12 avoid else blocks after for and while loops&lt;/h3&gt;

&lt;h3 id=&quot;take-advantage-of-each-block-in-tryexceptelsefinally&quot;&gt;1.13 take advantage of each block in try/except/else/finally&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;the &lt;code class=&quot;highlighter-rouge&quot;&gt;try/finally&lt;/code&gt; statements let you run cleanup code regardless whether there is exceptions in your &lt;code class=&quot;highlighter-rouge&quot;&gt;try&lt;/code&gt; block or not;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;functions&quot;&gt;2. functions&lt;/h2&gt;

&lt;h3 id=&quot;prefering-exceptions-to-return-none&quot;&gt;2.1 prefering exceptions to return none&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;functions that return &lt;code class=&quot;highlighter-rouge&quot;&gt;None&lt;/code&gt; to indicate special meaning are error prone beacause &lt;code class=&quot;highlighter-rouge&quot;&gt;None&lt;/code&gt; and other values (0, empty string, etc) are evaluated to &lt;code class=&quot;highlighter-rouge&quot;&gt;False&lt;/code&gt; in the conditional expression;&lt;/li&gt;
  &lt;li&gt;Raise exceptions to indicate special situations instead of returning &lt;code class=&quot;highlighter-rouge&quot;&gt;None&lt;/code&gt;, expect the calling code to handle the exceptions properly;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;know-how-closures-interactive-with-variable-scope&quot;&gt;2.2 know how closures interactive with variable scope&lt;/h3&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;In&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;sort_priority&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;group&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
   &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;         &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;helper&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
   &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;                 &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;group&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
   &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;                         &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
   &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;                 &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
   &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;         &lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sort&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;helper&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
   &lt;span class=&quot;o&quot;&gt;...&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;In&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;In&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;group&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;In&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sort_priority&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;group&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;In&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;values&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;python supports closures: functions that refer to variables from the scope in which they were defined. this is why helper function is able to access the group argument to &lt;code class=&quot;highlighter-rouge&quot;&gt;sort_priority&lt;/code&gt;;&lt;/li&gt;
  &lt;li&gt;functions are first-class objects in python, meaning you can refer to them directly, assign them to variables, pass them as arguments to other functions, compare them in expressions and if statements, etc. this is how the sort method can accept a closure function as the key argument;&lt;/li&gt;
  &lt;li&gt;python has specific rules for comparing tuples. it first compares items in index zero, then index one and so on. this is why the return value from the helper closure causes the sort order to have two distinct groups.&lt;/li&gt;
  &lt;li&gt;when you refer a variable in an expression, the python interpreter will traverse the scope to resolve the reference in this order:
    &lt;ul&gt;
      &lt;li&gt;the current function’s scope;&lt;/li&gt;
      &lt;li&gt;any enclosing scope(like other containing functions)&lt;/li&gt;
      &lt;li&gt;the scope of the module that contains the code(also called the global scope)&lt;/li&gt;
      &lt;li&gt;the built-in scope&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;consider-generators-instead-of-returning-list&quot;&gt;2.3 consider generators instead of returning list&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;using generators can be clearer than the alternative of returning lists of accumulated results;&lt;/li&gt;
  &lt;li&gt;the iterator returned by a generator produces the set of values passed to yield expressions within the generator funtions’ body;&lt;/li&gt;
  &lt;li&gt;generators can produce a sequence of outputs for arbitrarily large inputs because their working memory doesn’t include all inputs and outputs;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;be-defensive-when-iterating-over-arguments&quot;&gt;2.4 be defensive when iterating over arguments&lt;/h3&gt;

&lt;h3 id=&quot;reduce-visual-noise-with-variable-positional-arguments&quot;&gt;2.5 reduce visual noise with variable positional arguments&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;functions can accept a variable number of positional arguments by using &lt;code class=&quot;highlighter-rouge&quot;&gt;*args&lt;/code&gt; in the def statement;&lt;/li&gt;
  &lt;li&gt;you can use the items from a sequence as the posistional arguments for a function with the &lt;code class=&quot;highlighter-rouge&quot;&gt;*&lt;/code&gt; operator;&lt;/li&gt;
  &lt;li&gt;using the &lt;code class=&quot;highlighter-rouge&quot;&gt;*&lt;/code&gt; operator with a generator may cause your program to ran out of memory and crash;&lt;/li&gt;
  &lt;li&gt;adding new positional parameters to functions that accept &lt;code class=&quot;highlighter-rouge&quot;&gt;*args&lt;/code&gt; can introduce hard-to-find bugs;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;provide-optional-behavior-with-keyword-argument&quot;&gt;2.6 provide optional behavior with keyword argument&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;function arguments can be positional or keyword arguments;&lt;/li&gt;
  &lt;li&gt;keyword arguments will make it clear when it will be confusing only using positional arguments;&lt;/li&gt;
  &lt;li&gt;keyword arguments with default values make it easy to add new behaviors, especially there exists some callers;&lt;/li&gt;
  &lt;li&gt;optional keyword should always be passed using keyword argument other than positional argument;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;use-none-and-docstrings-to-specify-dynamic-default-arguments&quot;&gt;2.7 use none and docstrings to specify dynamic default arguments&lt;/h3&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;# when the function is defined, default argument values are evaluated just once per module at the loading time.&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;msg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;now&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;msg: {}, time: {}&#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;msg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;log_version&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;msg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;msg_time&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datetime&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;now&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;msg: {}, time: {}&#39;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;msg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;msg_time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;use None for default value is especially important when the argument is mutable;&lt;/li&gt;
  &lt;li&gt;default argument values are only evaluated once;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;enforce-clarity-with-keyword-only-argument&quot;&gt;2.8 enforce clarity with keyword only argument&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;keyword argument make the intention of the function more clear;&lt;/li&gt;
  &lt;li&gt;prefer to use keyword arguments if possible, especially when there are many boolean flag arguments;&lt;/li&gt;
  &lt;li&gt;python 3 supports explicit syntax for keyword only arguments in functions;&lt;/li&gt;
  &lt;li&gt;python 2 can emulate keyword only argument by using &lt;code class=&quot;highlighter-rouge&quot;&gt;**kwargs&lt;/code&gt; and manually throw &lt;code class=&quot;highlighter-rouge&quot;&gt;TypeError&lt;/code&gt; exception;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;classes-and-inheritance&quot;&gt;3. Classes and Inheritance&lt;/h2&gt;

&lt;h3 id=&quot;prefer-helper-classes-over-bookkeeping-with-dictionaries-and-tuples&quot;&gt;3.1 prefer helper classes over bookkeeping with dictionaries and tuples&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;avoid making dicts with values that are other dicts or long tuple;&lt;/li&gt;
  &lt;li&gt;use namedtuple for lightweight, immutable data containers before you need the flexibility of full class;&lt;/li&gt;
  &lt;li&gt;move your bookkeeping code to use multiple helper classes when your internal state dicts get complicated;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;accept-functions-for-simple-interfaces-instead-of-classes&quot;&gt;3.2 accept functions for simple interfaces instead of classes&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;instead of defining classes, functions are often all you need for simple interfaces between components in python;&lt;/li&gt;
  &lt;li&gt;references to functions and methods in python are first class, meaning they can be used in expressions like any other type;&lt;/li&gt;
  &lt;li&gt;the &lt;code class=&quot;highlighter-rouge&quot;&gt;__call__&lt;/code&gt; special method enables instances of a class to be called like plain python functions;&lt;/li&gt;
  &lt;li&gt;when you need a function to maintain state, consider defining a class that provides the &lt;code class=&quot;highlighter-rouge&quot;&gt;__call__&lt;/code&gt; method instead of defining a stateful closure.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;use-classmethod-polymorphism-to-construct-objects-generically&quot;&gt;3.3 use @classmethod polymorphism to construct objects generically&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;python only supports a single constructor per class, the &lt;code class=&quot;highlighter-rouge&quot;&gt;__init__&lt;/code&gt; method;&lt;/li&gt;
  &lt;li&gt;use @classmethod to define alternative constructors for your classes;&lt;/li&gt;
  &lt;li&gt;use class method polymorphism to provide generic ways to build and connect concrete subclasses;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;initialized-parent-classes-with-super&quot;&gt;3.4 initialized parent classes with super&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;python’s standard method to resolution order (MRO) solves the problems of superclass init order and diamond inheritance;&lt;/li&gt;
  &lt;li&gt;always use the super built-in function to init parent classes;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;use-multiple-inheritance-only-for-mix-in-utility-classes&quot;&gt;3.5 use multiple inheritance only for mix-in utility classes&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;avoid using multiple inheritance if mix-in classes can achieve the same outcome;&lt;/li&gt;
  &lt;li&gt;use pluggable behaviors at the instance level to provide per-class customization when mix-in classes may require it;&lt;/li&gt;
  &lt;li&gt;compose mix-ins to create complex functionality from simple behaviors;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;prefer-public-attributes-over-private-ones&quot;&gt;3.6 prefer public attributes over private ones&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;private attributes aren’t rigorously enforced by the python compiler;&lt;/li&gt;
  &lt;li&gt;plan from the beginning to allow subclasses to do more with your internal APIs and attributes instead of locking them out by default;&lt;/li&gt;
  &lt;li&gt;use documentation of protected fields to guide subclasses instead of trying to force access control with private attributes;&lt;/li&gt;
  &lt;li&gt;only consider using private attributes to avoid naming conflicts with subclasses that are out of your control;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;inherit-from-collectionsabc-for-custom-container-types&quot;&gt;3.7 inherit from collections.abc for custom container types&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;inherit directly from python’s container types (list, dict) for simple use cases;&lt;/li&gt;
  &lt;li&gt;beaware of the large number of methods required to implement custom container types correctly;&lt;/li&gt;
  &lt;li&gt;have your custom container types inherit from the interfaces defined in collections.abc to ensure that your classes match required interfaces and behaviors;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;metaclasses-and-attributes&quot;&gt;4. metaclasses and attributes&lt;/h2&gt;

&lt;h3 id=&quot;use-plain-attributes-instead-of-get-and-set-methods&quot;&gt;4.1 use plain attributes instead of get and set methods&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;define new class interfaces using simple public attributes and avoid set and get methods;&lt;/li&gt;
  &lt;li&gt;use @property to define special behavior when attributes are accessed on your objects, if necessary;&lt;/li&gt;
  &lt;li&gt;follow the rule of least surprise and avoid weird side effects in your @property methods;&lt;/li&gt;
  &lt;li&gt;ensure that @property methods are fast, do slow or complex work using normal methods;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;consider-property-instead-of-refactoring-attributes&quot;&gt;4.2 consider @property instead of refactoring attributes&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;use @property to give existing instance attributes new functionality;&lt;/li&gt;
  &lt;li&gt;make incremental progress toward better data models by using @property;&lt;/li&gt;
  &lt;li&gt;consider refactoring a class and all call sites when you find yourself using @property too heavily;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;use-descriptors-for-reusable-property-methods&quot;&gt;4.3 use descriptors for reusable @property methods&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;reuse the behavior and validation of @property methods by defining your own descriptor classes;&lt;/li&gt;
  &lt;li&gt;use WeakKeyDict to ensure that your descriptor classes don’t cause memory leaks;&lt;/li&gt;
  &lt;li&gt;don’t get bogged down trying to understand exactly how &lt;code class=&quot;highlighter-rouge&quot;&gt;__getattribute__&lt;/code&gt; uses the descriptor protocol for getting and setting attrbutes;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;use-getattr-getattribute-and-setattr-for-lazy-attributes&quot;&gt;4.4 use &lt;code class=&quot;highlighter-rouge&quot;&gt;__getattr__&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;__getattribute__&lt;/code&gt;, and &lt;code class=&quot;highlighter-rouge&quot;&gt;__setattr__&lt;/code&gt; for lazy attributes&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;use &lt;code class=&quot;highlighter-rouge&quot;&gt;__getattr__&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;__setattr__&lt;/code&gt; to lazily load and save attributes for an object;&lt;/li&gt;
  &lt;li&gt;understand that &lt;code class=&quot;highlighter-rouge&quot;&gt;__getattr__&lt;/code&gt; only gets called once when accessing a missing attribute, whereas &lt;code class=&quot;highlighter-rouge&quot;&gt;__getattribute__&lt;/code&gt; gets called every time an attribute is accessed;&lt;/li&gt;
  &lt;li&gt;avoid infinite recursion in &lt;code class=&quot;highlighter-rouge&quot;&gt;__getattribute__&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;__setattr__&lt;/code&gt; by using methods from super() to access instance attributes directly;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;validate-subclass-with-metaclass&quot;&gt;4.5 validate subclass with metaclass&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;use metaclasses to ensure that subclasses are well formed at the time they are defined, before objects of their type are constructed;&lt;/li&gt;
  &lt;li&gt;metaclasses have slightly different syntax in python 2 and python 3;&lt;/li&gt;
  &lt;li&gt;the &lt;code class=&quot;highlighter-rouge&quot;&gt;__new__&lt;/code&gt; method of metaclasses is run after the class statement’s entire body has been processed;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;register-class-existence-with-metaclasses&quot;&gt;4.6 register class existence with metaclasses&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;class registeration is a helpful pattern for building modular python programs;&lt;/li&gt;
  &lt;li&gt;metaclasses let you run register code automatically each time your base class is subclassed in a program;&lt;/li&gt;
  &lt;li&gt;using metaclasses for class register avoids errors by ensuring that you never miss a register call;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;annotate-class-attributes-with-metaclasses&quot;&gt;4.7 annotate class attributes with metaclasses&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;metaclasses enable you to modify a class’s attributes before the class is fully defined;&lt;/li&gt;
  &lt;li&gt;descriptors and metaclasses make a powerful combination for declarative behavior and runtime introspection;&lt;/li&gt;
  &lt;li&gt;you can avoid both memory leaks and the weakref module by using metaclasses along with descriptors;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;concurrenc-and-parallelism&quot;&gt;5. concurrenc and parallelism&lt;/h2&gt;

&lt;h3 id=&quot;use-subprocess-to-manage-child-processes&quot;&gt;5.1 use subprocess to manage child processes&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;use the subprocess module to run child processes and manage their input and output manually;&lt;/li&gt;
  &lt;li&gt;child processes run in parallel with the python interpreter, enabling you to maximize your cpu usage;&lt;/li&gt;
  &lt;li&gt;use the timeout parameter with communicate to avoid deadlocks and hanging child proceses;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;use-threads-for-blocking-io-avoid-for-parallelism&quot;&gt;5.2 use threads for blocking i/o, avoid for parallelism&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;python threads can’t run bytecode in parallel on multiple cpu cores because of the glow interpreter lock;&lt;/li&gt;
  &lt;li&gt;python threads are still useful despite the gil because they provide an easy way to do multiple things at seemingly the same time;&lt;/li&gt;
  &lt;li&gt;use python threads to make multiple system calls in parallel, this allows you to do blocking io at the same time as computation;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;use-lock-to-prevent-data-races-in-threads&quot;&gt;5.3 use lock to prevent data races in threads&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;even though python has global interpreter lock, you’re still responsible for protecting against data races between the threads in your programs;&lt;/li&gt;
  &lt;li&gt;your programs will corrupt their data structures if you allow multiple threads to modify the same objects without locks;&lt;/li&gt;
  &lt;li&gt;the lock class in the threading built-in module is python’s standard mutual exclusion lock implementation;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;use-queue-to-coordinate-work-between-threads&quot;&gt;5.4 use queue to coordinate work between threads&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;pipelines are a great way to organize sequences of work that run concurrently using multiple python threads;&lt;/li&gt;
  &lt;li&gt;be aware of the many problems in building concurrent pipelines: busy waiting, stopping workers and memory explosion;&lt;/li&gt;
  &lt;li&gt;the queue class has all of the facilities you need to build robust pipelines: blocking operations, buffer sizes and joining;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;consider-coroutines-to-run-many-functions-concurrently&quot;&gt;5.5 consider coroutines to run many functions concurrently&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;coroutines provie an efficient way to run tens of thousands of functions seemingly at the same time;&lt;/li&gt;
  &lt;li&gt;within a generator, the value of the yield expression will be whatever value was passed to the generator’s send method from the exterior code;&lt;/li&gt;
  &lt;li&gt;coroutines give you a powerful tool for separating the core logic of your program from its interaction with the surrounding environment;&lt;/li&gt;
  &lt;li&gt;python 2 doesn’t support yield from or returning values from generators;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;consider-concurrentfutures-for-true-parallelism&quot;&gt;5.6 consider concurrent.futures for true parallelism&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;moving cpu bottlenecks to c-extension modules can be an effective way to improve performance while maximizing your investment in python code. however, the cost of doing so is high and may introduce bugs;&lt;/li&gt;
  &lt;li&gt;the multiprocessing module provides powerful tools that can parallelize certain types of python computation with minimal effort;&lt;/li&gt;
  &lt;li&gt;the power of multiprocessing is best accessed through the concurrent.futures built-in module and its simple processpollexecutor class;&lt;/li&gt;
  &lt;li&gt;the advanced parts of the multiprocessing module should be avoided because they are so complex;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;built-in-modules&quot;&gt;6. built-in modules&lt;/h2&gt;

&lt;h3 id=&quot;define-functions-decorators-with-functoolswraps&quot;&gt;6.1 define functions decorators with functools.wraps&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;decorators are python syntax for allowing one function to modify another function at runtime;&lt;/li&gt;
  &lt;li&gt;using decorators can cause strange behaviors in tools that do introspection, such as debuggers;&lt;/li&gt;
  &lt;li&gt;use the &lt;code class=&quot;highlighter-rouge&quot;&gt;wraps&lt;/code&gt; decorator from the &lt;code class=&quot;highlighter-rouge&quot;&gt;functools&lt;/code&gt; built-in modules when you define your own decorators to avoid any issues;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;consider-contextlib-and-with-statements-for-reusable-tryfinally-behavior&quot;&gt;6.2 consider contextlib and with statements for reusable try/finally behavior&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;the with statement allow you to reuse logic from try/finally blocks and reduce visual noise;&lt;/li&gt;
  &lt;li&gt;the contextlib built-in module provides a contextmanager decorator that makes it easy to use your own functions in with statements;&lt;/li&gt;
  &lt;li&gt;the value yielded by context managers is supplied to the as part of the with statement. it’s useful for letting your code directly access the cause of the special context;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;make-pickle-reliable-with-copyreg&quot;&gt;6.3 make pickle reliable with copyreg&lt;/h3&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pickle&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;module&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&#39;s serialization format is unsafe by design. the serialzed data contains what is essentially a program that describes how to reconstruct the original python object. this means a malicious pickle payload could be used to compromise any part of the python program that attempts to deserialize it.&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;the pickle module is only useful for serializing and deserializing objects between trusted programs;&lt;/li&gt;
  &lt;li&gt;the pickle module may break down when used for more than trivial use cases;&lt;/li&gt;
  &lt;li&gt;use the copyreg built-in module with pickle to add missing attribute values, allow versioning of classes, and provide stable import paths;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;use-datetime-instead-of-time-for-local-clocks&quot;&gt;6.4 use datetime instead of time for local clocks&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;avoid using the time module for translating between different time zones;&lt;/li&gt;
  &lt;li&gt;use the datetime built-in module along with the pytz module to reiliably convert between times in different time zones;&lt;/li&gt;
  &lt;li&gt;always represent time in utc and do conversions to local time as the final step before presentation;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;use-built-in-algorithms-and-data-structures&quot;&gt;6.5 use built-in algorithms and data structures&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;use python’s built-in modules for algorithms and data structures;&lt;/li&gt;
  &lt;li&gt;don’t reimplement this functionality yourself;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;use-decimal-when-precising-is-paramount&quot;&gt;6.6 use decimal when precising is paramount&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;python has built-in types and classes in modules that can represent practically every type of numerical value;&lt;/li&gt;
  &lt;li&gt;the decimal class is ideal for situations that require high precision and exact rounding behavior, such as computations of monetary values;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;know-where-to-find-community-built-modules&quot;&gt;6.7 know where to find community-built modules&lt;/h3&gt;

&lt;h2 id=&quot;collaboration&quot;&gt;7. collaboration&lt;/h2&gt;

&lt;h3 id=&quot;write-docstring-for-every-function-class-and-module&quot;&gt;7.1 write docstring for every function, class and module&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;write documentation for every module, class and function using docstrings. keep them up to date as your code changes;&lt;/li&gt;
  &lt;li&gt;for modules, introduce the contents of the module and any important classes or functions all users should know about;&lt;/li&gt;
  &lt;li&gt;for classes, document behavior, important attributes and subclass behavior in the docstring following the class statement;&lt;/li&gt;
  &lt;li&gt;for functions and methods, document every argument, returned value, raised exception and other behaviors in the docstring following the def statement;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;use-packages-to-organize-modules-and-provide-stable-apis&quot;&gt;7.2 use packages to organize modules and provide stable apis&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;packages in python are modules that contain other modules, packages allow you to organize your code into separate, non-conflicting namespaces with unique absolute module names;&lt;/li&gt;
  &lt;li&gt;simple packages are defined by adding an &lt;code class=&quot;highlighter-rouge&quot;&gt;__init__.py&lt;/code&gt; file to a directory that contains other source files. these files become the child modules of the directory’s package, package directories may also contain other packages;&lt;/li&gt;
  &lt;li&gt;you can provide an explicit api for a module by listing its publicly visible names in its &lt;code class=&quot;highlighter-rouge&quot;&gt;__all__&lt;/code&gt; special attribute;&lt;/li&gt;
  &lt;li&gt;you can hide a package’s internal implementation by only importing public names in the package’s &lt;code class=&quot;highlighter-rouge&quot;&gt;__init__.py&lt;/code&gt; file or by naming internal-only members with a leading underscore;&lt;/li&gt;
  &lt;li&gt;when collaborating within a single team or on a single codebase, using &lt;code class=&quot;highlighter-rouge&quot;&gt;__call__&lt;/code&gt; for explicit apis is probably unnecessary;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;define-a-root-exception-to-insulate-callers-from-apis&quot;&gt;7.3 define a root exception to insulate callers from apis&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;define root exceptions for your modules allows api consumers to insulate themselves from your api;&lt;/li&gt;
  &lt;li&gt;catching root exception can help your modules allows api consumers to insulate themselves from your api;&lt;/li&gt;
  &lt;li&gt;catching the python exception base class can help you find bugs in api implementation;&lt;/li&gt;
  &lt;li&gt;intermediate root exceptions let you add more specific types of exceptions in the future without breaking your api consumers;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;know-how-to-break-circular-dependecies&quot;&gt;7.4 know how to break circular dependecies&lt;/h3&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;# dialog.py&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;app&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Dialog&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;pass&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;save_dialog&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dialog&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prefs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;hello&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;pass&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# app.py&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;dialog&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Prefs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;pass&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;prefs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Prefs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dialog&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;then if you use app.py in your project, you’ll absolutely get an error like bellow:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;In&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;app&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;---------------------------------------------------------------------------&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;AttributeError&lt;/span&gt;                            &lt;span class=&quot;n&quot;&gt;Traceback&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;most&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;recent&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;call&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;last&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ipython&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f8537898a049&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;module&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;----&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;app&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Users&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;chenshan&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Desktop&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;py&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;module&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;----&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;dialog&lt;/span&gt;
      &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
      &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# import ipdb; ipdb.set_trace()&lt;/span&gt;
      &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;
      &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Prefs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Users&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;chenshan&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Desktop&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dialog&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;py&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;module&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
      &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;         &lt;span class=&quot;k&quot;&gt;pass&lt;/span&gt;
      &lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;---&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;save_dialog&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dialog&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prefs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;hello&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
     &lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;
     &lt;span class=&quot;mi&quot;&gt;12&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;AttributeError&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;module&#39;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;object&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;has&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;no&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;attribute&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&#39;prefs&#39;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;the problem with a circular dependency is that the attributes of a module aren’t defined until the code for those attributes has executed(after step 5), but the module can be loaded with the import statement immediately after it’s inserted into sys.modules(after step 4)&lt;/p&gt;

&lt;p&gt;in the example above,&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;the app.py import dialog before doing anything&lt;/li&gt;
  &lt;li&gt;then, dialog.py import app firstly, currently the &lt;code class=&quot;highlighter-rouge&quot;&gt;imported app is just an empty module&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;then, in dailog.py, the &lt;code class=&quot;highlighter-rouge&quot;&gt;save_dialog = Dialog(app.prefs.get(&quot;hello&quot;))&lt;/code&gt;, at that time, we’re sure that there are really a module named &lt;code class=&quot;highlighter-rouge&quot;&gt;app&lt;/code&gt; exists in current runtime, but it’s an empty module, so will raise an attribute error like this.&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;python import machinery:
    &lt;ul&gt;
      &lt;li&gt;searches for your module in locations from sys.path&lt;/li&gt;
      &lt;li&gt;loads the code from the module and ensures that it compiles&lt;/li&gt;
      &lt;li&gt;creates a corresponding empty module object&lt;/li&gt;
      &lt;li&gt;inserts the module into sys.modules&lt;/li&gt;
      &lt;li&gt;runs the code in the module object to define its contents&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://localhost:4000/python-cycle-import/&quot;&gt;sever ways to break circular imports&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;reorder imports&lt;/li&gt;
      &lt;li&gt;import first, config second, run last&lt;/li&gt;
      &lt;li&gt;dynamic import&lt;/li&gt;
      &lt;li&gt;re-strucuture or re-factory your code&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;use-virtual-env-for-isonated-and-reproducible-dependences&quot;&gt;7.5 use virtual env for isonated and reproducible dependences&lt;/h3&gt;

&lt;h2 id=&quot;production&quot;&gt;8. Production&lt;/h2&gt;

&lt;h3 id=&quot;consider-module-scoped-code-to-configure-deployment-env&quot;&gt;8.1 consider module-scoped code to configure deployment env&lt;/h3&gt;

&lt;h3 id=&quot;use-repr-string-for-debugging-output&quot;&gt;8.2 use repr string for debugging output&lt;/h3&gt;

&lt;h3 id=&quot;test-everything-with-unittest&quot;&gt;8.3 test everything with unittest&lt;/h3&gt;

&lt;h3 id=&quot;consider-interactive-debugging-with-pdb&quot;&gt;8.4 consider interactive debugging with pdb&lt;/h3&gt;

&lt;h3 id=&quot;profile-before-optimizing&quot;&gt;8.5 profile before optimizing&lt;/h3&gt;

&lt;h3 id=&quot;use-tracemalloc-to-understand-memory-usage-and-leaks&quot;&gt;8.6 use tracemalloc to understand memory usage and leaks&lt;/h3&gt;

</content>
   </entry>
   

</feed>


</body>
</html>
