<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8" />
  <title>Taotao's Zone</title>
  <meta name="renderer" content="webkit">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="mobile-web-app-capable" content="yes">
  <link rel="stylesheet" href="/css/font-awesome/css/font-awesome.min.css" type="text/css" />
  <link rel="stylesheet" href="/css/default.css" type="text/css" />
  <link rel="stylesheet" href="/css/desktop.css" type="text/css" />
  <link rel="stylesheet" href="/css/mobile.css" type="text/css" />
  <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
  <link rel="alternate" type="application/atom+xml" title="Recent Entries" href="/atom.xml" />
  <script src="/js/jquery-1.11.0.min.js" type="text/javascript"></script>
  <script src="/js/jquery-migrate-1.2.1.js" type="text/javascript"></script>
  <script src="/js/jquery.transit.min.js" type="text/javascript"></script>
  <script src="/js/common.js" type="text/javascript"></script>
</head>
<body>
  <?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

   <title>Taotao's Zone</title>
   <link href="http://litaotao.github.io/atom.xml" rel="self" type="application/atom+xml"/>
   <link href="http://litaotao.github.io" rel="alternate" type="text/html" />
   <updated>2015-03-04T16:49:52+08:00</updated>
   <id>http://litaotao.github.io</id>
   <author>
     <name></name>
     <email></email>
   </author>

   
   <entry>
     <title>［touch spark］10. Spark 学习资源集锦</title>
     <link href="/spark-resources"/>
     <updated>2015-03-01T00:00:00+08:00</updated>
     <id>/spark-resources</id>
     <content type="html">&lt;h2&gt;&lt;/h2&gt;

&lt;h2&gt;1. 写在前面&lt;/h2&gt;

&lt;p&gt;　　实际上，学习任何一门技术，最好的学习资料肯定是官网。但是，在我学习spark的过程中，我发现有两个理由告诉我为什么学习一门技术仍然需要一些官网之外的资料：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;广度：官网上的资料大都focus在技术本身的实现和用法，而有很多资料会提及这门技术的应用，以及实践、应用这门技术中遇到的坑，还有由这门技术衍生出来的其他辅助技术。&lt;/li&gt;
&lt;li&gt;深度：很多资料都会解析这门技术的实现以及原理，反应到官网上，大多都是API文档或技术架构；我发现，很多在官网上理解不了的细节，可能在一些资料上描述起来更明确。&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;　　基于上面这两个原因，我准备把自己在学习Spark过程中有幸遇到的好的资料搜集起来，并从广度和深度两个方面来整理这些资&lt;/p&gt;

&lt;p&gt;料，希望对大家有所帮助。还有，我会整理一些和spark相关的第三方开源工具，相信这些工具在帮助大家构建基于spark的应用时会助一臂之力。&lt;/p&gt;

&lt;p&gt;　　所有这些我都会同步到github上：&lt;a href=&quot;https://github.com/litaotao/spark-materials&quot;&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;2. 资源列表&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;广度&lt;/strong&gt;：&lt;a href=&quot;http://blog.cloudera.com/blog/2014/08/how-to-use-ipython-notebook-with-apache-spark/&quot;&gt;spark和ipython notebook结合&lt;/a&gt;&lt;br/&gt;
&lt;strong&gt;备注&lt;/strong&gt;：我参考这篇文章搭建了自己基于ipython notebook的分析平台，详细记录在这篇博客上了：&lt;a href=&quot;../ipython-notebook-server-spark&quot;&gt;当Ipython Notebook遇见Spark&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;广深&lt;/strong&gt;：&lt;a href=&quot;http://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf&quot;&gt;RDD论文英文版&lt;/a&gt;
&lt;strong&gt;备注&lt;/strong&gt;：这篇论文绝对值得多读和深读，中文版在&lt;a href=&quot;https://code.csdn.net/CODE_Translation/spark_matei_phd&quot;&gt;这里&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;一个对Spark 2分钟的介绍视频，如果有人要你介绍Spark，按照里面的说就perfect了。&lt;a href=&quot;https://www.youtube.com/watch?v=cs3_3LdCny8&quot;&gt;YouTube&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;blockquote&gt;&lt;blockquote&gt;&lt;p&gt;&lt;strong&gt;Apache Spark is：&lt;/strong&gt;  &lt;br/&gt;
    + A data analytics, cluster computing framework;   &lt;br/&gt;
    + Fits into Hadoop open-source community, and builds on top of Hadoop     Distributed File System(HDFS);  &lt;br/&gt;
    + Not tied to two-stage MapReduce paradigm, it&#39;s performance up to 100 times faster than Hadoop MapReduce for certain applications;         &lt;br/&gt;
    + Provides primitives for in-memeory cluster computing;         &lt;br/&gt;
    + In-memory cluster computing allows user load data to cluster&#39;s memory and queried repeatedly;        &lt;br/&gt;
    + Suited for machine learning algorithms;&lt;/p&gt;&lt;/blockquote&gt;&lt;/blockquote&gt;

&lt;h2&gt;3. Spark相关的第三方开源工具&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Zeppelin&lt;/strong&gt;：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;官网：&lt;a href=&quot;http://zeppelin-project.org/&quot;&gt;http://zeppelin-project.org/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Hue&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;官网：&lt;a href=&quot;http://gethue.com/&quot;&gt;http://gethue.com/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;IPyhon&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;官网：&lt;a href=&quot;http://ipython.org/&quot;&gt;http://ipython.org/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;ISpark&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Github：&lt;a href=&quot;https://github.com/tribbloid/ISpark&quot;&gt;https://github.com/tribbloid/ISpark&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;scala-notebook&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Github: &lt;a href=&quot;https://github.com/Bridgewater/scala-notebook&quot;&gt;https://github.com/Bridgewater/scala-notebook&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;spark-notebook&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Github: &lt;a href=&quot;https://github.com/andypetrella/spark-notebook&quot;&gt;https://github.com/andypetrella/spark-notebook&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Ipython-sql&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Github: &lt;a href=&quot;https://github.com/catherinedevlin/ipython-sql&quot;&gt;https://github.com/catherinedevlin/ipython-sql&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;sparknotebook&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Github: &lt;a href=&quot;https://github.com/hohonuuli/sparknotebook&quot;&gt;https://github.com/hohonuuli/sparknotebook&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;spark-kernel&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Github: &lt;a href=&quot;https://github.com/ibm-et/spark-kernel&quot;&gt;https://github.com/ibm-et/spark-kernel&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;spark-jobserver&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Github: &lt;a href=&quot;https://github.com/spark-jobserver/spark-jobserver&quot;&gt;https://github.com/spark-jobserver/spark-jobserver&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;spark-packages&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;官网：&lt;a href=&quot;http://spark-packages.org/&quot;&gt;http://spark-packages.org/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
   <entry>
     <title>今天发生了一件坏事，也发生了一件好事</title>
     <link href="/the-badest-thing-in-my-life"/>
     <updated>2015-02-06T00:00:00+08:00</updated>
     <id>/the-badest-thing-in-my-life</id>
     <content type="html">&lt;h2&gt;&lt;/h2&gt;

&lt;h2&gt;1. 写在前面&lt;/h2&gt;

&lt;p&gt;　　以前读过两篇文章，一片叫《为什么从现在起你应该写博客》，第二篇记不得了，但其中有一个中心思想，说的是你写的东西都是给别人看的，应该都是对别人有用的。看了第一篇文章，我从不久之前终于借助Github建立了自己的一个博客，记录自己的技术成长经历；看了第二篇文章，我几乎每周都会对自己有一个总结，而从不把这些总结发到博客上。因为我认为这些总结都是未来回忆自己年轻时的年少轻狂用的，不应该发到博客上了。&lt;br/&gt;
　　但是今天，发生了一件坏事，算是从小到大最丑的一件事了，但同时也是一件好事。至少以后别人再问我最丑的经历时，我有话可说了。&lt;/p&gt;

&lt;h2&gt;2. 人生最丑的事&lt;/h2&gt;

&lt;p&gt;　　今天在小组内做第二次关于spark的技术分享，讲得很烂，算是人生的一个污点了，具体经历就不讲了。但我保证这是历史和未来所有presentation中最烂的一次。我将从以下几个方面来记录这次事件，以后每次活动之前，我都要自己看看今天的这篇文章，告诫自己，不要在同一个地方跌倒两次。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;没有事先准备好&lt;br/&gt;
　　以后进行presentation，一定要提前半小时入场，把电脑环境配置好。今天失败的根本原因是一个自定义的配置文件在断网重连后没有进行source操作，导致IPython Notebook Server和Spark集群之间连接不上。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;紧急情况下不会冷静分析问题&lt;br/&gt;
　　在启动IPython Notebook后，发现 &lt;code&gt;from pyspark import SparkContext, SparkConf&lt;/code&gt; 不能成功，提示没有pyspark这个包。当时我就慌了，没有沿着本质去分析问题，然后在老板的催促和心慌当中就开始讲了，在大家的迷茫和我不知所以的回答中，我就这样浑浑噩噩的度过了几十分钟，中间还幸得一位同事解围。在最后几分钟的时候，我实在忍不住了，心想反正今天已经完蛋了，就不管别人在那里讨论什么了。我开始研究到底是什么原因导致Notebook Server连接不上spark集群。我首先查看了下IPython的配置，在 &lt;code&gt;~/.ipython/profile_pyspark/&lt;/code&gt; 目录下面，我先看ipython_notebook_config.py文件，发现没有什么异常的，心想也应该不会出现在这里吧，因为IPython notebook还是能用的，只是找不到pyspark这个包。我突然回忆起来，在当初配置spark+IPython的时候，需要设置一个启动脚本文件。接着，我查看了那个启动脚本，在 &lt;code&gt;/root/.ipython/profile_pyspark/startup/00-pyspark-setup.py&lt;/code&gt; 下，恍然大悟，我发现问题了，这个脚本是这样写的：&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;pre&gt;&lt;code&gt;import os
import sys

spark_home = os.environ.get(&#39;SPARK_HOME&#39;, None)

if not spark_home:
    raise ValueError(&#39;SPARK_HOME environment variable is not set&#39;)
sys.path.insert(0, os.path.join(spark_home, &#39;python&#39;))
sys.path.insert(0, os.path.join(spark_home, &#39;python/lib/py4j-0.8.2.1-src.zip&#39;))
#execfile(os.path.join(spark_home, &#39;python/pyspark/shell.py&#39;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　Say，关键就是这句 &lt;code&gt;spark_home = os.environ.get(&#39;SPARK_HOME&#39;, None)&lt;/code&gt; 因为没有定义一个环境变量，因此不能在IPython Notebook的环境变量sys.path里加入pyspark包所在的地址。再想想自己在哪里定义了SPARK_HOME这个变量，明明就在Desktop下的&lt;code&gt;ipython_notebook_spark.bashrc&lt;/code&gt; 里，看看里面是怎么写的：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;&quot;&gt;export SPARK_HOME=&quot;/usr/local/spark-1.2.0-bin-cdh4/&quot;
export PYSPARK_SUBMIT_ARGS=&quot;--master spark://10.21.208.21:7077 --deploy-mode client&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　这下知道了，原因是自己定义的.bashrc文件在登出linux后会失效，而刚才把电脑带到会议室后重新连接的网络，linux console重新登出了，所以必须重新source这个文件。 &lt;br/&gt;
　　现在回想，其实应该在看到错误提示没有pyspark这个包的时候就应该发现这个问题了，之所以一开始没有推测出这个问题，有几个原因：一是在这种情况下发现错误，头脑发热发慌，不能冷静处理；二是自己对这部分技术没有掌握彻底；3是在学习一门新技术时，没有详细地把学习过程和一些操作过程记录下来，导致出问题后不能及时参考。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;表达不清 &lt;br/&gt;
　　还有一条经验就是表达不清楚，思路不清楚，回答别人的问题不够明确。在别人提问没有表达清晰时，自己没有和别人进行沟通就贸然回答，导致交流效率极低。曾经看过很多文章，都讲做技术的一定要努力提高自己的表达能力，我满不在乎，因为平时和大家感觉交流起来还算流畅的，没想到这次碰了瓷。得到一条宝贵的经验，以后再做分享的时候，一定要准备好详细讲稿和提纲。别人在提问时，一定要先把别人提问的问题搞清楚，理解了再进行回答。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;一些细节问题 &lt;br/&gt;
　　这次分享出来本组人员，还有鹏哥和他们组的两个人员，这种情况下在开始前应该由主讲人[我]来互相介绍一下，介绍双方的人员和职责。 &lt;br/&gt;
　　在讲稿，PPT，程序准备的时候，要注意配色和字体，字体要大，颜色要和谐，让听众能清晰地看到投影仪的东西。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;最后&lt;br/&gt;
　　最后，我想说，这真的是人生最糟糕的一次经历，以后绝对不能出现同样的问题，在以后每次做分享的时候，我要多回头看看这篇文章，想想今天尴尬的我。今天的分享，远远没有达到预期效果，还耽误了大家不少时间，也影响了自己的形象。这种情况下不能逃避，我应该给参会的每个人员发封邮件表示歉意，这也是对别人的尊重。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;3. 感悟&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;机会，永远只给有所准备，并且已经准备好了的人；&lt;/li&gt;
&lt;li&gt;从每一件小事做起，把每一件小事做到极致；
&lt;img src=&quot;../images/make_little_thing_best.jpg&quot; alt=&quot;make_little_thing_best&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
   <entry>
     <title>［touch spark］9. 编译Zeppelin</title>
     <link href="/compile-zeppelin"/>
     <updated>2015-01-29T00:00:00+08:00</updated>
     <id>/compile-zeppelin</id>
     <content type="html">&lt;h2&gt;&lt;/h2&gt;

&lt;h2&gt;1. 写在前面&lt;/h2&gt;

&lt;p&gt;　　这篇记录是我按照&lt;a href=&quot;http://zeppelin-project.org/docs/install/install.html&quot;&gt;官网&lt;/a&gt;的步骤来写的，主要是记录在编译Zeppelin过程中的一些经验。&lt;br/&gt;
　　对于这类没有发布特别稳定版本的项目，我倾向于这样一种实践方法，在本地建两个文件夹：project, project-build，把project文件夹作为一个本地的repo，可以随时update到最新的源码，然后在project-build里构建，就算失败了也可以保证不会污染项目源文件。
　　再具体一点就是我在实践Zeppelin的时候使用的方法，如下所示，可以随时保证zeppelin文件夹里的源码最新，然后在zeppelin-build里用最新的源码构建。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@kali:~/Desktop# mkdir zeppelin zeppelin-build
root@kali:~/Desktop# cd zeppelin
root@kali:~/Desktop/zeppelin# git init
Initialized empty Git repository in /root/Desktop/zeppelin/.git/
root@kali:~/Desktop/zeppelin# git remote add origin git@github.com:NFLabs/zeppelin.git
root@kali:~/Desktop/zeppelin# git pull origin master
root@kali:~/Desktop/zeppelin# cd ../zeppelin-build/
root@kali:~/Desktop/zeppelin-build# cp -R ../zeppelin/* .
root@kali:~/Desktop/zeppelin-build# mvn clean package -DskipTests
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;2. 详细步骤及错误解决&lt;/h2&gt;

&lt;h3&gt;2.1 按照默认配置编译 : mvn clean package -DskipTests&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;root@ubuntu2[15:30:17]:~/Desktop/zeppelin-build#mvn clean package -DskipTests
.
.
.
[INFO] npm WARN deprecated grunt-ngmin@0.0.3: use grunt-ng-annotate instead
[INFO] npm ERR! 
[INFO] npm ERR! Additional logging details can be found in:
[INFO] npm ERR!     /root/Desktop/zeppelin-build/zeppelin-web/npm-debug.log
[INFO] npm ERR! not ok code 0
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Zeppelin ........................................... SUCCESS [ 37.320 s]
[INFO] Zeppelin: Zengine .................................. SUCCESS [  9.899 s]
[INFO] Zeppelin: Spark .................................... SUCCESS [ 12.115 s]
[INFO] Zeppelin: Markdown interpreter ..................... SUCCESS [  2.257 s]
[INFO] Zeppelin: Shell interpreter ........................ SUCCESS [  2.239 s]
[INFO] Zeppelin: web Application .......................... FAILURE [04:10 min]
[INFO] Zeppelin: Server ................................... SKIPPED
[INFO] Zeppelin: Packaging distribution ................... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 05:15 min
[INFO] Finished at: 2015-02-11T15:30:16+08:00
[INFO] Final Memory: 61M/409M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal com.github.eirslett:frontend-maven-plugin:0.0.20:npm (npm install) on project zeppelin-web: Failed to run task: &#39;npm install --color=false&#39; failed. (error code 1) -&amp;gt; [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn &amp;lt;goals&amp;gt; -rf :zeppelin-web
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;2.1 出错: npm install --color=false&lt;/h3&gt;

&lt;p&gt;　　这个错误在&lt;a href=&quot;https://groups.google.com/forum/#!searchin/zeppelin-developers/npm$20install&quot;&gt;mailing list&lt;/a&gt;里提到了，我们按照里面的解决方案来尝试一下。注意，以前的mailing list的维护在google groups里面的，但今年2月份之后groups就只读，不可以发帖了，新的mailing list移植到apache旗下，地址在&lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/incubator-zeppelin-users/&quot;&gt;这里&lt;/a&gt;。
　　解决方案：先在zeppelin-web文件夹里运行 &lt;code&gt;npm install&lt;/code&gt;，然后再回到zeppelin-build目录构建。&lt;/p&gt;

&lt;h3&gt;2.2 启动zeppelin服务器错误，提示Unsupported major.minor&lt;/h3&gt;

&lt;p&gt;　　在运行 bin/zeppelin-daemon.sh start 的时候提示错误如下。一开始没有发现什么原因，后来google 关键字 &lt;code&gt;nsupported major.minor&lt;/code&gt;后在&lt;a href=&quot;http://www.oecp.cn/hi/yangtaoorange/blog/1168263&quot;&gt;这里&lt;/a&gt;知道问题应该和JAVA编译器的版本有问题，因为zeppelin需要java 1.7以上，我本机是java 1.6的，当时为了编译zeppelin我自己下了java 1.7和设置了一个java 1.7的环境变量文件，应该是没有source这个环境变量吧，测试一下发现$JAVA_HOME这个环境变量为空。这里再次source一下java_1.7_path.bashrc这个文件就可以了，这个文件是这样写的：
&lt;code&gt;
root@ubuntu2[10:59:24]:~/Desktop#cat java_1.7_path.bashrc
export PATH=/usr/local/jdk1.7.0_71/bin:$PATH
export CLASSPATH=&quot;/usr/local/jdk1.7.0_71/lib:.&quot;
export JAVA_HOME=&quot;/usr/local/jdk1.7.0_71/&quot;
&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@ubuntu2[10:51:15]:~/Desktop/zeppelin-build#vi logs/zeppelin-root-ubuntu2.out 
Exception in thread &quot;main&quot; java.lang.UnsupportedClassVersionError: com/nflabs/zeppelin/
server/ZeppelinServer : Unsupported major.minor version 51.0
        at java.lang.ClassLoader.defineClass1(Native Method)
        at java.lang.ClassLoader.defineClass(ClassLoader.java:643)
        at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
        at java.net.URLClassLoader.defineClass(URLClassLoader.java:277)
        at java.net.URLClassLoader.access$000(URLClassLoader.java:73)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:212)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:205)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:323)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:294)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:268)
Could not find the main class: com.nflabs.zeppelin.server.ZeppelinServer. Program will
exit.
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;2.3 运行出错，日志提示：java.net.UnknownHostException: &lt;em&gt;&lt;your hostname&gt;&lt;/em&gt;: nodename nor servname provided, or not known&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;../images/zeppelin-run-error.jpg&quot; alt=&quot;run-error&quot; /&gt;&lt;/p&gt;

&lt;p&gt;解决办法[我的机器host名叫mac007]，修改/etc/hosts，新增一项 127.0.0.1   mac007&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/zeppelin-hosts.jpg&quot; alt=&quot;revise-hosts&quot; /&gt;
　　&lt;/p&gt;
</content>
   </entry>
   
   <entry>
     <title>［touch spark］8. 当Ipython Notebook遇见Spark</title>
     <link href="/ipython-notebook-server-spark"/>
     <updated>2015-01-27T00:00:00+08:00</updated>
     <id>/ipython-notebook-server-spark</id>
     <content type="html">&lt;h2&gt;&lt;/h2&gt;

&lt;h2&gt;1. 致谢&lt;/h2&gt;

&lt;p&gt;　　首先我忠心地感谢Ipython，Spark的开源作者，真心谢谢你们开发这么方便，好用，功能强大的项目，而且还无私地奉献给大众使用。刚刚很轻松地搭建了一个机遇Ipython Notebook的Spark客户端，真的感受到 The power of technology, the power of open source.&lt;br/&gt;
　　下面是这两个项目的github地址：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/ipython/ipython&quot;&gt;Ipython&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/apache/spark&quot;&gt;Spark&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;　　同时，这篇文章在刚开始的部分，参考了很多 &lt;a href=&quot;http://blog.cloudera.com/blog/2014/08/how-to-use-ipython-notebook-with-apache-spark/&quot;&gt;这篇博客&lt;/a&gt;的内容，感谢这么多人能无私分享如此高质量的内容。 &lt;br/&gt;
　　但是，这篇文章不是简单记录怎么做，我尽量做到量少质高，所以有些地方会说得比较详细，其中也会提到在解决遇到的问题上的一些方法和思路。&lt;/p&gt;

&lt;h2&gt;2. 路线规划&lt;/h2&gt;

&lt;p&gt;　　基于 &lt;a href=&quot;http://www.databricks.com/&quot;&gt;Databricks&lt;/a&gt;，&lt;a href=&quot;zeppelin-project.org&quot;&gt;Zeppelin&lt;/a&gt; 和 &lt;a href=&quot;www.gethue.com&quot;&gt;Hue&lt;/a&gt; 的启发，我也想尝试搭建一个丰富可用的在线大数据REPL分析平台，正好用此机会好好实践一下spark，毕竟都学习spark几个月了呢。 &lt;br/&gt;
　　不说废话，同&lt;a href=&quot;../weibo-api-in-action&quot;&gt;使用spark分析微博数据那篇博文一样&lt;/a&gt;，我们也要有一个路线规划：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;搭建一个可多用户使用的，底层接入了spark集群的Ipython Notebook Server；&lt;/li&gt;
&lt;li&gt;完善 Weibo Message Driver，使用户可在Notebook里获取、分析微博数据，as simple as possible；&lt;/li&gt;
&lt;li&gt;研究Zeppelin和Hue项目，把其中一个嫁接在Notebook的上层，实现准产品级的大数据实时ETL，Analytic，Sharing平台；这一步可能需要较长时间，可根据自己的时间安排灵活调整；&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;　　Dream：在年前完成上面三步，that&#39;s really full or chanllenge, but more funny. &lt;strong&gt;Anyway, we need dreams, and I can&#39;t wait to make this dream into reality.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/dreams.jpg&quot; alt=&quot;dreams&quot; /&gt;&lt;/p&gt;

&lt;p&gt;　　这篇主要记录我在实现第一步的过程中遇到的主要步骤，遇到的问题和解决方法：搭建一个可多用户使用的，底层接入了spark集群的Ipython Notebook Server。&lt;/p&gt;

&lt;h2&gt;3. 配置Ipython&lt;/h2&gt;

&lt;h3&gt;3.1: ipython 配置名profile介绍&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;profile 命令说明&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;　　profile是ipython的一个子命令，其中profile又有两个子命令，分别是create和list，顾名思义，create就是创建一个配置文件，list就是列出当前配置文件。如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@ubuntu2[13:54:01]:~/Desktop#ipython profile 
No subcommand specified. Must specify one of: [&#39;create&#39;, &#39;list&#39;]

Manage IPython profiles

Profile directories contain configuration, log and security related files and
are named using the convention &#39;profile_&amp;lt;name&amp;gt;&#39;. By default they are located in
your ipython directory.  You can create profiles with `ipython profile create
&amp;lt;name&amp;gt;`, or see the profiles you already have with `ipython profile list`

To get started configuring IPython, simply do:

$&amp;gt; ipython profile create

and IPython will create the default profile in &amp;lt;ipython_dir&amp;gt;/profile_default,
where you can edit ipython_config.py to start configuring IPython.

Subcommands
-----------

Subcommands are launched as `ipython cmd [args]`. For information on using
subcommand &#39;cmd&#39;, do: `ipython cmd -h`.

create
    Create an IPython profile by name
list
    List available IPython profiles
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;profile子命令list说明&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;　　本想list命令应该很简单的，和linux下的ls差不多嘛，但我自己看了下，其中还是有些细节值得推敲的。其中这项 &lt;code&gt;Available profiles in /root/.config/ipython:&lt;/code&gt; 是说目前有两个配置文件在那个目录下面，pyspark是我自己创建的了。在参考的&lt;a href=&quot;http://blog.cloudera.com/blog/2014/08/how-to-use-ipython-notebook-with-apache-spark/&quot;&gt;这篇文章&lt;/a&gt;中，作者说创建的配置文件会放到 &lt;code&gt;~/.ipython/profile_pyspark/&lt;/code&gt; 下，其实这并不是一定的，具体放在哪个目录下面，可以根据profile list的命令来查看。如此看来，我们在这台机器上创建的配置文件应该是放在目录 &lt;code&gt;/root/.config/ipython&lt;/code&gt; 下面的。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@ubuntu2[14:09:12]:~/Desktop#ipython profile list

Available profiles in IPython:
    pysh
    math
    sympy
    cluster

    The first request for a bundled profile will copy it
    into your IPython directory (/root/.config/ipython),
    where you can customize it.

Available profiles in /root/.config/ipython:
    default
    pyspark

To use any of the above profiles, start IPython with:
    ipython --profile=&amp;lt;name&amp;gt;  
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;profile子命令create说明&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;　　简单介绍下create子命令的用法。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@ubuntu2[09:25:57]:~/Desktop#ipython profile help create
Create an IPython profile by name

Create an ipython profile directory by its name or profile directory path.
Profile directories contain configuration, log and security related files and
are named using the convention &#39;profile_&amp;lt;name&amp;gt;&#39;. By default they are located in
your ipython directory. Once created, you will can edit the configuration files
in the profile directory to configure IPython. Most users will create a profile
directory by name, `ipython profile create myprofile`, which will put the
directory in `&amp;lt;ipython_dir&amp;gt;/profile_myprofile`.
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;3.2 创建新的Ipython配置文件&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;创建配置文件&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;　　因为我之前已经配置过一个pyspark的配置文件了，这里我们创建一个测试用的配置文件，pytest。运行一下命令后，会在 &lt;code&gt;/root/.config/ipython&lt;/code&gt; 下生成一个 pytest的目录。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@ubuntu2[14:54:14]:~/Desktop#ipython profile create pytest
[ProfileCreate] Generating default config file: u&#39;/root/.config/ipython/profile_pytest/ipython_config.py&#39;
[ProfileCreate] Generating default config file: u&#39;/root/.config/ipython/profile_pytest/ipython_notebook_config.py&#39;

root@ubuntu2[15:00:57]:~/Desktop#ls ~/.config/ipython/profile_pytest/
ipython_config.py  ipython_notebook_config.py  log  pid  security  startup 
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;3.3 编辑配置文件&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;编辑ipython_notebook_config.py&lt;/li&gt;
&lt;/ul&gt;


&lt;pre&gt;&lt;code&gt;    c = get_config()

    # about line 15, the ip address the notebook server will listen on. Set it to * means that any IP/Machine which can connect to the server can connect to the notebook server.
    c.NotebookApp.ip = &#39;*&#39;
    # about line 37, whether to open a browser or not. cause what we want to build is a backend server, so we set it False, no need to open a browser.
    c.NotebookApp.open_browser = False
    # about line 54, the port which the notebook server will listen on
    c.NotebookApp.port = 8880 # or whatever you want, make sure the port is available  
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;设置访问密码 &lt;br/&gt;
　　如果你的notebook server是需要访问控制的，简单的话可以设置一个访问密码。听说Ipython 2.x 版本有用户访问控制，这里我还没有接触过，晚点会看看是否有成熟的可用的用户控制方案。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;生成密码文件&lt;br/&gt;
这里我们用python自带的密码包生成一个密码，然后再把这个密码重定向到nvpasswd.txt文件里。注意这里重定向的路径哦。&lt;/li&gt;
&lt;li&gt;编辑配置文件，设置读取密码文件配置项
这里有一个需要注意的，就是PWDFILE的设置，一开始我设置为 &lt;code&gt;~/.config/ipython/profile_pytest/nbpasswd.txt&lt;/code&gt;，但是启动ipython notebook server的时候老师报错，说找不到密码文件nbpasswd.txt，很奇怪，明明文件就是在的，可就是提示找不到。无奈我到nbpasswd.txt路径下用 pwd 打印当前路径，显示为 &lt;code&gt;root/.config/ipython/profile_pytest/nbpasswd.txt&lt;/code&gt;，可是这两个路径应该是一样的啊。无奈之下，死马当作活马医，我就把PWDFILE设置成为 &lt;code&gt;root/.config/ipython/profile_pytest/nbpasswd.txt&lt;/code&gt;，没想到这样还成功了。关于这点为什么会有效，目前我还不是很清楚，等我请教了公司大神后再补上这一个tip吧。&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;  示例如下：&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;pre&gt;&lt;code&gt;root@ubuntu2[09:40:29]:~/Desktop#python -c &#39;from IPython.lib import passwd; print passwd()&#39; &amp;gt; ~/.config/ipython/profile_pytest/nbpasswd.txt
Enter password: 
Verify password: 
root@ubuntu2[09:43:35]:~/Desktop#vi /root/.config/ipython/profile_pytest/nbpasswd.txt 
sha1:c6b748a8e1e2:4688f91ccfb9a8e0afd041ec77cdda99d0e1fb8f  

root@ubuntu2[09:49:09]:~/Desktop#vi /root/.config/ipython/profile_pytest/ipython_notebook_config.py 
# about line 95
PWDFILE=&#39;root/.config/ipython/profile_pytest/nbpasswd.txt&#39;
c.NotebookApp.password = open(PWDFILE).read().strip()
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;设置启动文件&lt;br/&gt;
　　这一步算是比较重要的了，也是我在配置这个notebook server中遇到的比较难解的问题。这里我们首先需要创建一个启动文件，并在启动文件里设置一些spark的启动参数。如下：&lt;/li&gt;
&lt;/ul&gt;


&lt;pre&gt;&lt;code&gt;root@ubuntu2[09:52:14]:~/Desktop#touch ~/.config/ipython/profile_pytest/startup/00-pytest-setup.py 
root@ubuntu2[10:08:44]:~/Desktop#vi ~/.config/ipython/profile_pytest/startup/00-pytest-setup.py   

import os
import sys

spark_home = os.environ.get(&#39;SPARK_HOME&#39;, None)
if not spark_home:
    raise ValueError(&#39;SPARK_HOME environment variable is not set&#39;)
sys.path.insert(0, os.path.join(spark_home, &#39;python&#39;))
sys.path.insert(0, os.path.join(spark_home, &#39;python/lib/py4j-0.8.1-src.zip&#39;))
execfile(os.path.join(spark_home, &#39;python/pyspark/shell.py&#39;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　上面的启动配置文件也还简单，即拿到spark_home路径，并在系统环境变量path里加上两个路径，然后再执行一个shell.py文件。不过，在保存之前还是先确认下配置文件写对了，比如说你的SPARK_HOME配置对了，并且下面有python这个文件夹，并且python/lib下有py4j-0.8.1这个文件。我在检查的时候就发现我的包版本是py4j-0.8.2.1的，所以还是要改得和自己的包一致才行。 &lt;br/&gt;
　　这里得到一个经验，在这种手把手，step by step的教程中，一定要注意版本控制，比较各人的机器，操作系统，软件版本等都不可能完全一致，也许在别人机器上能成功，在自己的机器上不成功也是很正常的事情，毕竟细节决定成败啊！所以在我这里，这句我是这样写的： &lt;code&gt;sys.path.insert(0, os.path.join(spark_home, &#39;python/lib/py4j-0.8.2.1-src.zip&#39;))&lt;/code&gt;&lt;/p&gt;

&lt;h2&gt;4. Ok，here we go　　&lt;/h2&gt;

&lt;p&gt;　　到这里差不多大功告成了，可以启动notebook server了。不过在启动之前，需要配置两个环境变量参数，同样，这两个环境变量参数在也是根据个人配置而定的。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# for the CDH-installed Spark
export SPARK_HOME=&#39;/usr/local/spark-1.2.0-bin-cdh4/&#39;

# this is where you specify all the options you wou
ld normally add after bin/pyspark
  export PYSPARK_SUBMIT_ARGS=&#39;--master spark://10.21.208.21:7077 --deploy-mode client&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　ok，万事具备，只欠东风了。让我们来尝尝鲜吧：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@ubuntu2[10:40:50]:~/Desktop#ipython notebook --profile=pyspark
2015-02-01 10:40:54.850 [NotebookApp] Using existing profile dir: u&#39;/root/.config/ipython/profile_pyspark&#39;
2015-02-01 10:40:54.858 [NotebookApp] Using MathJax from CDN: http://cdn.mathjax.org/mathjax/latest/MathJax.js
2015-02-01 10:40:54.868 [NotebookApp] CRITICAL | WARNING: The notebook server is listening on all IP addresses and not using encryption. This is not recommended.
2015-02-01 10:40:54.869 [NotebookApp] Serving notebooks from local directory: /root/Desktop
2015-02-01 10:40:54.869 [NotebookApp] The IPython Notebook is running at: http://[all ip addresses on your system]:8880/
2015-02-01 10:40:54.869 [NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　在浏览器输入driver:8880即可访问notebook server了，首先会提示输入密码，密码正确后就可以使用了。
&lt;img src=&quot;../images/notebook-spark-1.jpg&quot; alt=&quot;notebook-spark-1&quot; /&gt;
&lt;img src=&quot;../images/notebook-spark-2.jpg&quot; alt=&quot;notebook-spark-2&quot; /&gt;&lt;/p&gt;
</content>
   </entry>
   
   <entry>
     <title>［touch spark］6. 终于等到你，spark streaming + 新浪微博数据</title>
     <link href="/weibo-api-in-action"/>
     <updated>2015-01-05T00:00:00+08:00</updated>
     <id>/weibo-api-in-action</id>
     <content type="html">&lt;p&gt;注：与本文相关的所有源代码已放在最最喜爱的 &lt;a href=&quot;https://github.com/litaotao/weibostreaming&quot;&gt;Github&lt;/a&gt; 上。&lt;/p&gt;

&lt;h2&gt;&lt;/h2&gt;

&lt;h2&gt;1. 写在前面&lt;/h2&gt;

&lt;p&gt;　　从最开始想尝试streaming的时候，我一心想实现databricks在spark summit 2014上的那个演示---实时获取twiiter数据，并做分析展示。无奈，在折腾twitter几天后只得放弃。刚开始注册twitter得需要手机号，可是twitter居然不支持中国内地的号码，这里不知道是twitter不支持，还是中国内地的运营商不支持twitter，我想应该是后者吧。当时纠结了好久，一种心碎的感觉。后来终于突然有一天无需手机号也能注册了，我欣喜若狂地注册了一个twitter号，后来发现尼玛新建一个twitter app需要手机验证，这下我就完全down机了，无奈，twitter这条路是完全走不通了。这段时间的心情，就像过山车一样，从心碎到兴奋再到最后的心死。&lt;br/&gt;
&lt;img src=&quot;../../images/guoshanche.gif&quot; alt=&quot;过山车&quot; /&gt;
　　可是我不死心啊，还行想玩玩streaming，想体验体验streaming的power，怎么办怎么办？我左思右想，想要做到databricks的效果，唯一的办法就是利用咱们天朝的新浪微博了。在调研了一下新浪微博的API后，其原理和twitter API的原理也是一样的，但是新浪微博的streaming API被取消了。好吧，硬骨头挺多，还得自己啃了。没办法，我想了想，就2个选择，要么不做，然后天天后悔；要么做，然后天天折腾。好吧，我承认我还是义无返顾地选择了第二条路，sigh。&lt;br/&gt;
　　在写这篇博客的时候看到徐静蕾的新片《有一个地方只有我们知道》，里面一句话让我深有感触---&lt;strong&gt;没有在一起的，就是不对的人，对的人，你是不会失去他的&lt;/strong&gt;，我也想说，没有学到的技术，就是你不喜爱的技术，喜欢的技术，你是不可能学不会的。&lt;/p&gt;

&lt;h2&gt;2. 实验目的&lt;/h2&gt;

&lt;p&gt;　　这次实验是想尝试一下spark streaming的效果，预期是这样的：通过每隔 &lt;strong&gt;几秒&lt;/strong&gt; 从新浪微博拿到 &lt;strong&gt;一些&lt;/strong&gt; 公开的微博数据，然后实时 &lt;strong&gt;处理&lt;/strong&gt; 一下这些数据并 &lt;strong&gt;展现&lt;/strong&gt; 出来。&lt;br/&gt;
　　ok，这里的关键上面已经用粗体标记出来了。有两个方面，一是时间间隔的设置，数据流量的设置，这关系到streaming的稳定性，比如说若处理速度小于数据流入的速度的话，那数据会慢慢堆积起来；若数据流入速度小于处理速度的话，展现处理结果肯定也不好看。这里是属于tuning的环境，可以在spark官网上仔细瞧瞧，不过具体还是要根据应用需求来定。第二个方面是数据处理和展示，这应该是应用的核心了。这里我们做得很初级，简单做一些TF-IDF的测试或者是更简单的包含性测试。 &lt;br/&gt;
　　既然是第一次，那就不要太那啥，还是温柔一点比较好。暂时定一个目标，我们想看看实时微博中哪些是包含某某字段的，然后输出这些微博的信息。比如说，我想知道实时微博中，哪些是包含 &lt;strong&gt;建设银行&lt;/strong&gt;，&lt;strong&gt;涨&lt;/strong&gt; 这样两个关键字的微博，并实时打印出来。&lt;/p&gt;

&lt;h2&gt;3. streaming，我的目标&lt;/h2&gt;

&lt;p&gt;　　既然要玩，那就玩得痛快点，下面是我准备在微博streaming这块做的一些各个版本的安排：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;数据方面：

&lt;ul&gt;
&lt;li&gt;第一步，能够获取微博伪实时数据即可【微博API请求有限制】；&lt;/li&gt;
&lt;li&gt;第二步，自己设计一个搜集系统，能获取近实时的微博数据，希望能媲美原来微博的streaming API；&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;应用方面：

&lt;ul&gt;
&lt;li&gt;第一步，啥都不干，确认spark   cluster能连到我的数据源，把所有接收到的数据简单打印出来；&lt;/li&gt;
&lt;li&gt;第二步，简单处理，对实时数据流进行一个简单的filter操作，比如说，看看哪些消息是提到了某人，或某支股票；&lt;/li&gt;
&lt;li&gt;第三步，复杂一点，用用MLib来对每条消息学习一下，其实就是高级的filter；&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;　　目前我打算从上面几个小目标一点一点上，最近发现一个NB项目，Zeppelin，底层可以集成spark，届时看看是否有需要，可以尝试下
eppelin+spark。【最近我尝试编译过Zeppelin，遇到很多问题，目前这个项目还不是很成熟，不过项目组说了，他们正在迁移到Apache的孵化器中，完成迁移后会专心发布新版本。Good，看来的确是一个NB项目】 &lt;br/&gt;
　　冲啊，每天进步一点点。
&lt;img src=&quot;../../images/stepbystep.jpg&quot; alt=&quot;每天进步一点点咯&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;4. 步骤规划&lt;/h2&gt;

&lt;p&gt;　　我们第一次的目标很简单，我准备在数据方面，简单地完成第一步；在应用方面，也是简单地完成第一步，算是一个最小的MVP了，暂定为MVP 0.1.0
吧，哈哈。好，现在简单地分析下，大概有下面几个步骤：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;获取数据，通过新浪微博API，这里需要我们设计一个数据收集器&lt;/li&gt;
&lt;li&gt;发送/接收数据，因为我改用Python来玩spark了，目前spark 1.2版的python streaming只支持socket包，当然socket包也是最简单的了，所以我准备用socket方式进行数据的收发。So，这里我们需要写一个简单的Socket Server&lt;/li&gt;
&lt;li&gt;展示数据，简单的打印下来即可&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;　　下面，我们就按照MVP 0.1.0 的步骤规划，一步一步来搞定咱们这个小系统。&lt;/p&gt;

&lt;h2&gt;5. 获取数据：新浪微博API使用&lt;/h2&gt;

&lt;p&gt;　　微博官方已经有详细的新手引导了，这里就不重复造轮子了，大家可以直接参考 &lt;a href=&quot;http://open.weibo.com/wiki/%E6%96%B0%E6%89%8B%E6%8C%87%E5%8D%97&quot;&gt;这里&lt;/a&gt;。我用的是&lt;a href=&quot;https://github.com/michaelliao/sinaweibopy&quot;&gt;Python SDK&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;6. 收发数据：Socket Server&lt;/h2&gt;

&lt;p&gt;　　好久没有接触网络编程这块了，这里为了快速完成MVP的效果，我用了最简单的多线程socket server模型，即新建一个线程用于处理一个新的连接。整个socket server的模型如下：&lt;br/&gt;
&lt;img src=&quot;../../images/socket-server.jpg&quot; alt=&quot;simple-socket-server&quot; /&gt;&lt;/p&gt;

&lt;p&gt;　　socket server的核心代码如下，完整代码请查看github。这里send_data应该是到去拿新浪微博的数据的，但是我在测试的时候为了方便起见，先简单地用了一条测试数据： data = &#39;hello, I am litaotao&#39;。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def send_data(conn, client):
    # data = get_data(client)
    data = &#39;hello, I am litaotao&#39;
    conn.sendall(data.encode(&#39;utf-8&#39;))
    print &#39;\nIN THREAD: send to {}, data length: {}&#39;.format(str(conn), str(len(data)))
    conn.close()

def socket_server(HOST, PORT):
    client = get_local_weibo_client() or get_weibo_client()

    # s = socket.socket(socket.AF_INET,socket.SOCK_STREAM)  
    s = socket.socket()    
    s.bind((HOST, PORT))
    s.listen(10)

    while  True:
        print &#39;wait for connection ...&#39;
        conn, addr = s.accept()
        print &#39;connect with {} : {}&#39;.format(addr[0], str(addr[1]))
        thread.start_new_thread(send_data, (conn, client))      
    s.shutdown()

if __name__ == &#39;__main__&#39;:
    HOST, PORT = &#39;&#39;, 9999
    socket_server(HOST, PORT)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;7. 展示数据: Just print&lt;/h2&gt;

&lt;p&gt;　　现在socket server已经准备就绪了，接下来准备一下spark端的任务逻辑。在这之前，我先简单介绍一下目前我的spark集群环境，为了方便理解，我也把环境的IP地址列出来了，这样在以后启动命令的时候也比较清楚。&lt;br/&gt;
　　可以清楚的看到，目前spark cluster测试环境里一共有10台机器，其中一台&lt;code&gt;10.21.208.21&lt;/code&gt;作为cluster manager，即master使用，其他9台作为worker使用。而我们写spark任务程序以及提交任务，拿到任务运行结果，都在一台driver机器上，driver机器ip是&lt;code&gt;10.20.70.80&lt;/code&gt;。
&lt;img src=&quot;../../images/spark-cluster.jpg&quot; alt=&quot;spark-cluster&quot; /&gt;
　　ok，现在可以来写spark的任务程序了，很简单，就是一个print语句，我是用python写的，代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# -*- coding: utf-8 -*-
import sys

from pyspark import SparkContext
from pyspark.streaming import StreamingContext


def change_nothing(lines):
    return lines

if __name__ == &quot;__main__&quot;:
    if len(sys.argv) != 3:
        print &amp;gt;&amp;gt; sys.stderr, &quot;Usage: weibo_message.py &amp;lt;hostname&amp;gt; &amp;lt;port&amp;gt;&quot;
        exit(-1)
    sc = SparkContext(appName=&quot;PythonStreamingWeiboMessage&quot;)
    ssc = StreamingContext(sc, 5)

    lines = ssc.socketTextStream(sys.argv[1], int(sys.argv[2]))
    lines = change_nothing(lines)
    lines.pprint()
    ssc.start()
    ssc.awaitTermination()
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;8. Opps, 为神马只有一个worker接收到数据了&lt;/h2&gt;

&lt;p&gt;　　原本以为这样就大功告成，可是当我兴奋地运行程序的时候，突然发现一个极为严重的问题---只有一个worker会到socket server这里来获取数据，并&lt;strong&gt;处理&lt;/strong&gt;后返回给driver，而且有时候是第一台worker来拿数据，有时候却又是另外一台worker来拿数据，anyway，问题就是：9太worker中，一直只有1台worker来拿数据，处理并返回，且这台worker并不是固定的。 &lt;br/&gt;
　　下面是日志：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;socket server的运行日志&lt;/li&gt;
&lt;/ul&gt;


&lt;pre&gt;&lt;code&gt;C:\Users\taotao.li\Desktop\weibostreaming (master)                                        
λ python socket_server_1.py                                                               
wait for connection ...                                                                   
-----------------------                                                                   
connect with 10.21.208.30 : 48927                                                         
wait for connection ...                                                                   
-----------------------                                                                   

IN THREAD: send to &amp;lt;socket._socketobject object at 0x0000000002C45E18&amp;gt;, data length: 20   
connect with 10.21.208.30 : 48929                                                         
wait for connection ...                                                                   
-----------------------                                                                   
IN THREAD: send to &amp;lt;socket._socketobject object at 0x0000000002C45E80&amp;gt;, data length: 20   

connect with 10.21.208.30 : 48931                                                         
wait for connection ...                                                                   
 -----------------------                                                                  
IN THREAD: send to &amp;lt;socket._socketobject object at 0x0000000002C45E18&amp;gt;, data length: 20   

connect with 10.21.208.30 : 48933                                                         
wait for connection ...                                                                   
-----------------------                                                                   

IN THREAD: send to &amp;lt;socket._socketobject object at 0x0000000002C45E80&amp;gt;, data length: 20   
connect with 10.21.208.30 : 48935                                                         
wait for connection ...                                                                   
IN THREAD: send to &amp;lt;socket._socketobject object at 0x0000000002C45E18&amp;gt;, data length: 20   

-----------------------                                                                   
connect with 10.21.208.30 : 48937                                                         
wait for connection ...                                                                   
IN THREAD: send to &amp;lt;socket._socketobject object at 0x0000000002C45E80&amp;gt;, data length: 20   

-----------------------                                                                   
connect with 10.21.208.30 : 48938                                                         
wait for connection ...                                                                   
 -----------------------                                                                  
IN THREAD: send to &amp;lt;socket._socketobject object at 0x0000000002C45E18&amp;gt;, data length: 20   

connect with 10.21.208.30 : 48940                                                         
wait for connection ...                                                                   
IN THREAD: send to &amp;lt;socket._socketobject object at 0x0000000002C45E80&amp;gt;, data length: 20   

-----------------------                                                                   
connect with 10.21.208.30 : 48942                                                         
wait for connection ...                                                                   
IN THREAD: send to &amp;lt;socket._socketobject object at 0x0000000002C45E18&amp;gt;, data length: 20   

-----------------------                                                                   
connect with 10.21.208.30 : 48944                                                         
wait for connection ...                                                                   
-----------------------                                                                   
IN THREAD: send to &amp;lt;socket._socketobject object at 0x0000000002C45E80&amp;gt;, data length: 20   

connect with 10.21.208.30 : 48946                                                         
wait for connection ...                                                                   
 -----------------------                                                                  
IN THREAD: send to &amp;lt;socket._socketobject object at 0x0000000002C45E18&amp;gt;, data length: 20   

connect with 10.21.208.30 : 48948                                                         
wait for connection ...                                                                   
 -----------------------                                                                  
IN THREAD: send to &amp;lt;socket._socketobject object at 0x0000000002C45E80&amp;gt;, data length: 20   

connect with 10.21.208.30 : 48952                                                         
wait for connection ...                                                                   
IN THREAD: send to &amp;lt;socket._socketobject object at 0x0000000002C45E18&amp;gt;, data length: 20   
-----------------------                                                                   

connect with 10.21.208.30 : 48953                                                         
wait for connection ...                                                                   
IN THREAD: send to &amp;lt;socket._socketobject object at 0x0000000002C45E80&amp;gt;, data length: 20   

-----------------------                                                                   
connect with 10.21.208.30 : 48955                                                         
wait for connection ...                                                                   
IN THREAD: send to &amp;lt;socket._socketobject object at 0x0000000002C45E18&amp;gt;, data length: 20   

-----------------------                                                                   
connect with 10.21.208.30 : 48956                                                         
wait for connection ...                                                                   
IN THREAD: send to &amp;lt;socket._socketobject object at 0x0000000002C45E80&amp;gt;, data length: 20   

-----------------------                                                                   
connect with 10.21.208.30 : 48957                                                         
wait for connection ...                                                                   
 -----------------------                                                                  
IN THREAD: send to &amp;lt;socket._socketobject object at 0x0000000002C45E18&amp;gt;, data length: 20   

-----------------------
Traceback (most recent call last):
  File &quot;socket_server_1.py&quot;, line 24, in &amp;lt;module&amp;gt;
    socket_server(HOST, PORT)
  File &quot;socket_server_1.py&quot;, line 16, in socket_server
    conn, addr = s.accept()
  File &quot;C:\Anaconda\lib\socket.py&quot;, line 202, in accept
    sock, addr = self._sock.accept()
KeyboardInterrupt
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;spark 任务的启动命令，我把处理返回的结果重定向到log.txt里，方便查看&lt;/li&gt;
&lt;/ul&gt;


&lt;pre&gt;&lt;code&gt;root@ubuntu2[17:41:01]:~/Desktop/streaming#spark-submit --master spark://10.21.208.21:7077 weibo_message.py 10.20.102.52 9999 &amp;gt; log.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;spark 任务运行日志，太多了，完整的日志可以到 &lt;a href=&quot;../../files/spark-console.log&quot;&gt;这里下载 spark-console.log&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;spark 任务运行结果日志 log.txt，完整的日志可以到 &lt;a href=&quot;../../files/log.txt&quot;&gt;这里下载 log.txt&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;pre&gt;&lt;code&gt;-------------------------------------------
Time: 2015-01-21 18:50:05
-------------------------------------------

-------------------------------------------
Time: 2015-01-21 18:50:10
-------------------------------------------
hello, I am litaotao

-------------------------------------------
Time: 2015-01-21 18:50:15
-------------------------------------------
hello, I am litaotao

-------------------------------------------
Time: 2015-01-21 18:50:20
-------------------------------------------
hello, I am litaotao
hello, I am litaotao

-------------------------------------------
Time: 2015-01-21 18:50:25
-------------------------------------------

-------------------------------------------
Time: 2015-01-21 18:50:30
-------------------------------------------

-------------------------------------------
Time: 2015-01-21 18:50:35
-------------------------------------------

-------------------------------------------
Time: 2015-01-21 18:50:40
-------------------------------------------

-------------------------------------------
Time: 2015-01-21 18:50:45
-------------------------------------------

-------------------------------------------
Time: 2015-01-21 18:50:50
-------------------------------------------

-------------------------------------------
Time: 2015-01-21 18:50:55
-------------------------------------------

-------------------------------------------
Time: 2015-01-21 18:51:00
-------------------------------------------
hello, I am litaotao

-------------------------------------------
Time: 2015-01-21 18:51:05
-------------------------------------------
hello, I am litaotao

-------------------------------------------
Time: 2015-01-21 18:51:10
-------------------------------------------

-------------------------------------------
Time: 2015-01-21 18:51:15
-------------------------------------------

-------------------------------------------
Time: 2015-01-21 18:51:20
-------------------------------------------

-------------------------------------------
Time: 2015-01-21 18:51:25
-------------------------------------------

-------------------------------------------
Time: 2015-01-21 18:51:30
-------------------------------------------
hello, I am litaotao
hello, I am litaotao

-------------------------------------------
Time: 2015-01-21 18:51:35
-------------------------------------------
hello, I am litaotao
hello, I am litaotao
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;Web UI 监控截图，显示只有一个receiver
&lt;img src=&quot;../../images/only-one-receiver.jpg&quot; alt=&quot;only-one-receiver&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;9. Why! What happened?&lt;/h2&gt;

&lt;p&gt;　　百思不得其解，这是为什么呢，这里我有两个疑点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;难道我对这个任务的理解有误吗。我的理解是，当运行spark-submit提交任务后，master应该会把这个weibo_message代码分发到9个worker上，然后9个worker分别在自己的机器上新建TCP连接到socket server，并从这个socket server上获取数据，然后处理后各自独立返回给driver。难道是我理解错误了吗。&lt;/li&gt;
&lt;li&gt;我仔细研读了官方&lt;a href=&quot;http://spark.apache.org/docs/latest/streaming-programming-guide.html#reducing-the-processing-time-of-each-batch&quot;&gt;spark streaming的教程&lt;/a&gt;，在里面发现这样一个主题 &lt;a href=&quot;http://spark.apache.org/docs/latest/streaming-programming-guide.html#reducing-the-processing-time-of-each-batch&quot;&gt;Level of Parallelism in Data Receiving&lt;/a&gt;，似乎对比起上图的Web UI监控图来看，难道是要自己根据worker的数据自定义receiver的数量。即目前我有9太worker，那我必须手动定义9个receiver？难道真的应该是这样的吗，我怎么觉得这样设计会很不灵活呢？why？&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
   <entry>
     <title>记录MongoDB一些优化方法</title>
     <link href="/mongodb-optimizing"/>
     <updated>2014-12-24T00:00:00+08:00</updated>
     <id>/mongodb-optimizing</id>
     <content type="html">&lt;h2&gt;&lt;/h2&gt;

&lt;h2&gt;1. 决定是否优化&lt;/h2&gt;

&lt;h3&gt;1.1 使用MongoDB自带的explain命令查看查询性能&lt;/h3&gt;

&lt;p&gt;　　下面是我在本地测试的一个例子。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; db.user.find({&#39;user&#39;:&#39;taotao.li@datayes.com&#39;}).explain()         
{             
        // 返回游标类型(BasicCursor | BtreeCursor | GeoSearchCursor | Complex Plan | multi)                                                         
        &quot;cursor&quot; : &quot;BasicCursor&quot;,
        // 是否使用多重索引(true | false)                                     
        &quot;isMultiKey&quot; : false,    
        // 返回的文档数量   (number)                                      
        &quot;n&quot; : 1,    
        // 被扫描的文档数量，这个值小于或等于nscanned   (number)                                                   
        &quot;nscannedObjects&quot; : 9,
        // 被扫描的文档数量，我们应该尽量使这个值和上面提到的n的值相近    (number)                                          
        &quot;nscanned&quot; : 9,    
        // 这个值表明在进行一次查询时，数据库计划扫描的文档数量 (number)                                            
        &quot;nscannedObjectsAllPlans&quot; : 9,                                
        &quot;nscannedAllPlans&quot; : 9,    
        // 若为true，表示查询不能利用文档在索引里的排序来返回结果，用户需要手动对返回进行排序操作，反之(true | false)                                     
        &quot;scanAndOrder&quot; : false,    
        // 若为true，表示查询是充分利用了现有的索引的，在设计索引的时候，应该尽量确保热点查询都利用到了已有的索引(true | false)                                     
        &quot;indexOnly&quot; : false,   
        // 表示查询语句执行时写锁的等待时间  (ms)                                       
        &quot;nYields&quot; : 0,          
        // 在分片的时候可以通过这个字段看分片的效果 (number)                                       
        &quot;nChunkSkips&quot; : 0, 
        // 耗时 (ms)                                               
        &quot;millis&quot; : 65,    
        // 所使用的索引 (dict of dict)                                             
        &quot;indexBounds&quot; : {                                             

        },        
        // mongo所在的服务器地址                                            
        &quot;server&quot; : &quot;SHN1408GPVG612:27017&quot;                             
}                                                                
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;1.2 使用MongoDB自带的profile优化器查看查询性能&lt;/h3&gt;

&lt;p&gt;　　MongoDB Database Profiler是一种慢查询日志功能,可以作为我们优化数据库的依据.
开启Profiling功能,有2种方式可以控制Profiling的开关盒级别。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;启动MonggoDB时加上 &lt;code&gt;-profile=级别&lt;/code&gt; 即可&lt;/li&gt;
&lt;li&gt;在客户端调用db.setProfilingLevel(级别)命令来实时配置&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;　　Profiler信息保存在system.profile中.我们可以通过db.getProfilingLevel()命令来获取当前的Profile级别。profile的级别有4个，分别是-1、0、1、2，默认没有开启。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;-1: 返回当前设置的级别&lt;/li&gt;
&lt;li&gt;0： 表示不开启&lt;/li&gt;
&lt;li&gt;1： 表示记录慢命令(默认为&gt;100ms)&lt;/li&gt;
&lt;li&gt;2： 表示记录所有命令&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;　　下面是我运行profile的一个示例及各个字段的解释：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; db.system.profile.findOne()
{
    //操作类型：insert | query | update | remove | getmore | command
    &quot;op&quot; : &quot;query&quot;,
    //进行op操作的地方，比如现在是说在community数据库的system集合的indexes中进行了一次查询操作
    &quot;ns&quot; : &quot;community.system.indexes&quot;,
    &quot;query&quot; : {
            &quot;expireAfterSeconds&quot; : {
                    &quot;$exists&quot; : true
            }
    },
    &quot;ntoreturn&quot; : 0,
    &quot;ntoskip&quot; : 0,
    &quot;nscanned&quot; : 2,
    &quot;keyUpdates&quot; : 0,
    &quot;numYield&quot; : 0,
    // 此次查询花在处理锁上的时间；其中R/W代表全局读/写锁，r/w代表数据库层面的读/写锁；
    &quot;lockStats&quot; : {
            &quot;timeLockedMicros&quot; : {
                    &quot;r&quot; : NumberLong(79),
                    &quot;w&quot; : NumberLong(0)
            },
            &quot;timeAcquiringMicros&quot; : {
                    &quot;r&quot; : NumberLong(1),
                    &quot;w&quot; : NumberLong(2)
            }
    },
    // 返回的文档数量
    &quot;nreturned&quot; : 0,
    // 返回字节长度，如果这个数字很大，考虑值返回所需字段
    &quot;responseLength&quot; : 20,
    // 查询所耗时间，这个时间是在mongo服务器端，从这个查询开始到查询结束；类似于一般程序执行的CPU时间；
    &quot;millis&quot; : 0,
    &quot;ts&quot; : ISODate(&quot;2014-11-19T09:33:58.965Z&quot;),
    // 发起此次查询的远程地址
    &quot;client&quot; : &quot;0.0.0.0&quot;,
    &quot;allUsers&quot; : [ ],
    // 执行此查询语句的用户，不知道这里为什么是空的
    &quot;user&quot; : &quot;&quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;1.3 使用MongoDB自带的mongostat优化器查看查询性能&lt;/h3&gt;

&lt;p&gt;　　下面是我本地测试的一个mongostat示例即相关字段含义：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;D:\mongodb-2.4.10\bin
λ mongostat.exe
connected to: 127.0.0.1
insert  query update delete getmore command flushes mapped  vsize    res faults  locked db idx miss %     qr|qw   ar|aw  netIn netOut  conn       time
    *0     *0     *0     *0       0     1|0       0   608m   1.4g    14m      1  test:3.3%          0       0|0     0|0    62b     3k    60   10:42:44
    *0     *0     *0     *0       0     1|0       0   608m   1.4g    14m      0  test:0.0%          0       0|0     0|0    62b     3k    60   10:42:45
    *0     *0     *0     *0       0     1|0       0   608m   1.4g    14m      0  test:0.0%          0       0|0     0|0    62b     3k    60   10:42:46
    *0     *0     *0     *0       0     1|0       0   608m   1.4g    14m      0  test:0.0%          0       0|0     0|0    62b     3k    60   10:42:47
    *0     *0     *0     *0       0     3|0       0   608m   1.4g    14m      0  test:0.0%          0       0|0     0|0   174b     3k    60   10:42:48
    *0     *0     *0     *0       0     1|0       0   608m   1.4g    14m      0  test:0.0%          0       0|0     0|0    62b     3k    60   10:42:49
    *0     *0     *0     *0       0     3|0       0   608m   1.4g    14m      0 local:0.0%          0       0|0     0|0   178b     3k    60   10:42:50
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　mongostat各个字段解释：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;insert/query/update/delete/getmore：每秒执行这5个操作的次数；&lt;/li&gt;
&lt;li&gt;command：每秒执行指令的次数，在从数据库中，这个字段的值是一个以“|”分开的两个值，表示 local|replicated 数量；&lt;/li&gt;
&lt;li&gt;flushes：每秒fsync操作的次数；&lt;/li&gt;
&lt;li&gt;mapped：按照官方解释，这个字段表示上一次执行mongostat指令是所有数据的大小，应该是指所有数据占磁盘的大小，但似乎不是很对；&lt;/li&gt;
&lt;li&gt;vsize：mongod服务占用的虚拟内存大小；&lt;/li&gt;
&lt;li&gt;res：mongod所占用的物理内存；&lt;/li&gt;
&lt;li&gt;faluts：page faults次数；&lt;/li&gt;
&lt;li&gt;index miss：索引缺失的数量；&lt;/li&gt;
&lt;li&gt;qr/qw：表示在队列中等待的客户端，rw表示读写；&lt;/li&gt;
&lt;li&gt;ar/aw：表示正在进行请求的客户端；&lt;/li&gt;
&lt;li&gt;netIn/netOut表示网络流量，单位是字节；&lt;/li&gt;
&lt;li&gt;conn：表示连接数；&lt;/li&gt;
&lt;li&gt;repl：表示同步状态；&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;1.4 使用MongoDB自带的db.serverStatus查看服务器状态&lt;/h3&gt;

&lt;p&gt;　　这里输出的信息太多了，看一个Robomongo的截图吧，里面有几个字段也是很重要的。 &lt;img src=&quot;../../images/mongo-db-serverStats.jpg&quot; alt=&quot;mongo-db-serverStatus&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;1.5 使用MongoDB自带的db.stats查看服务器状态&lt;/h3&gt;

&lt;p&gt;　　说到这里，我突然发现监控MongoDB performance的工具真的挺多的，完全可以自己给予这些命令和工具来开发后台管理的工具啊。比如说stats这个命令，也提供了挺多的信息：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; db.stats()                                        
{                                                   
        &quot;db&quot; : &quot;community&quot;,                         
        &quot;collections&quot; : 7,   
        // 记录在数据库中的所有文档总数                       
        &quot;objects&quot; : 73,                
        // 数据库中所有文档的平均大小，等于 dataSize/objects             
        &quot;avgObjSize&quot; : 24965.424657534248,          
        // 数据库所有文档的总大小，以字节为单位
        &quot;dataSize&quot; : 1822476,                  
        // 分配给每一个文档的磁盘空间，奇怪这里为什么不是16MB     
        &quot;storageSize&quot; : 11943936,                   
        &quot;numExtents&quot; : 12,                          
        &quot;indexes&quot; : 4,                              
        &quot;indexSize&quot; : 32704,                        
        &quot;fileSize&quot; : 201326592,                     
        &quot;nsSizeMB&quot; : 16,                            
        &quot;dataFileVersion&quot; : {                       
                &quot;major&quot; : 4,                        
                &quot;minor&quot; : 5                         
        },                                          
        &quot;ok&quot; : 1                                    
}                                                   
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;2. 优化Schema&lt;/h2&gt;

&lt;p&gt;　　解决一个问题永远都有多种方法，且在产品的不同时期也会有不同的解决办法。但核心观点都不变：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;理解产品的核心应用；&lt;/li&gt;
&lt;li&gt;合理平衡数据库的读写，读写比例很大程度上决定你的Schema设计；&lt;/li&gt;
&lt;li&gt;避免随机性IO操作；&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;　　即使是NoSQL，也无法避免一些数据库字段在关系上的建立。所以在设计NoSQL Schema的时候不可避免地要进行一些关系的处理。在MongoDB方面，处理关系有以下三种方法：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;数据库引用&lt;/li&gt;
&lt;li&gt;集合间的应用&lt;/li&gt;
&lt;li&gt;文档嵌套&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;　　其中数据库引用很少用到，并且官方也不推荐这种用法。相对于集合间引用和文档嵌套，这个需要看具体设计。特别是在应用文档嵌套方式的时候，需要注意一个文档的最大容量是&lt;a href=&quot;http://docs.mongodb.org/manual/core/document/&quot;&gt;16MB&lt;/a&gt;。文章末尾的&lt;strong&gt;6 Rules of Thumb for MongoDB Schema Design&lt;/strong&gt;详细介绍、对比了一些集合引用和嵌套文档方面的案例，&lt;strong&gt;MongoDB Schema Design: Four Real-World Examples&lt;/strong&gt;介绍了MongoDB 4个真实的应用设计案例，有比较大的参考价值。&lt;/p&gt;

&lt;h2&gt;2.1 MongoDB 和 RDBMS的一些概念联系&lt;/h2&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt; RDBMS &lt;/th&gt;
&lt;th&gt; MongoDB &lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt; Database &lt;/td&gt;
&lt;td&gt; Database &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; Table &lt;/td&gt;
&lt;td&gt; Collection &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; Row &lt;/td&gt;
&lt;td&gt; Document &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; Index &lt;/td&gt;
&lt;td&gt; Index &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; Join &lt;/td&gt;
&lt;td&gt; Embedded Document &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; Foreign Key &lt;/td&gt;
&lt;td&gt; Reference &lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;


&lt;h2&gt;3. 查询优化&lt;/h2&gt;

&lt;h3&gt;3.1 建立索引&lt;/h3&gt;

&lt;p&gt;　　首先要明确的是，mongo里会自动根据&lt;em&gt;id来创建一个唯一性索引，所以如果你是以&lt;/em&gt;id为key来进行查询的话都会很快的。比如下面这个截图，nscanned为1。
&lt;img src=&quot;../../images/mongo-index-id.jpg&quot; alt=&quot;mongo-explain&quot; /&gt;&lt;br/&gt;
　　database.collection.ensureIndex( { key : 1 } , { background : true } ); &lt;br/&gt;
　　说明：在数据库database里，对collection中的字段key建立索引，按照升序方式建立索引background参数设置为true时表示后台创建索引【建立索引略耗时】。  &lt;br/&gt;
　　索引是一把双刃剑啊，用得好不好，全看也许需求和数据库设计了，在设计索引前最好参考参考&lt;a href=&quot;http://docs.mongodb.org/manual/core/indexes-introduction/&quot;&gt;官方文档&lt;/a&gt;，而且最好要有一个建索引前后的performance的一些对比，talk is cheap, show me the data.  &lt;br/&gt;
　　总而言之，当需要建立索引的时候，一定要仔细思考下面几个方面：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;热点查询的数据是什么，可否用缓存替代；&lt;/li&gt;
&lt;li&gt;建立索引的顺序也会影响查询速度，参考: 10gen工程师谈MongoDB组合索引的优化；&lt;/li&gt;
&lt;li&gt;索引的更新周期，更新索引是一件很tricky的事情；&lt;/li&gt;
&lt;li&gt;索引建立后，是否能跟上后期系统扩展的脚步；&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;3.2 限制返回数据&lt;/h3&gt;

&lt;p&gt;　　使用limit，skip方式返回查询数据，只返回需要返回的数据。&lt;/p&gt;

&lt;h2&gt;4. 总结&lt;/h2&gt;

&lt;p&gt;　　在记录这篇文章的过程中，我发现监控mongo性能的工具还真的挺多的。但是现在我参与的产品中数据量还很少，还没有涉及到数据库这方面的优化，所以上面提到的这些都是自己在官网和一些优秀博客收集的资料。在后续涉及到自己优化这些查询的时候，我会再把实际经验和优化对比记录下来。&lt;/p&gt;

&lt;h2&gt;5. 一些资料&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://caizi.blog.51cto.com/5234706/1542480&quot;&gt;mongodb性能优化&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://blog.mongodb.org/post/87200945828/6-rules-of-thumb-for-mongodb-schema-design-part-1&quot;&gt;6 Rules of Thumb for MongoDB Schema Design&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.slideshare.net/friedo/data-modeling-examples&quot;&gt;MongoDB Schema Design: Four Real-World Examples&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://docs.mongodb.org/v2.4/administration/optimization/&quot;&gt;Optimization Strategies for MongoDB&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://docs.mongodb.org/v2.4/reference/database-profiler/&quot;&gt;Database Profiler Output&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.csdn.net/article/2012-11-09/2811690-optimizing-mongodb-compound&quot;&gt;10gen工程师谈MongoDB组合索引的优化&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://docs.mongodb.org/v2.4/reference/command/dbStats/&quot;&gt;dbStats&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
   <entry>
     <title>［touch spark］5. spark RDD 之：RDD Transformation</title>
     <link href="/rdd-transformations"/>
     <updated>2014-12-20T00:00:00+08:00</updated>
     <id>/rdd-transformations</id>
     <content type="html">&lt;p&gt;&lt;img src=&quot;../../images/transfomers.jpg&quot; alt=&quot;transfomers&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;&lt;/h2&gt;

&lt;h2&gt;1. 什么是RDD&lt;/h2&gt;

&lt;p&gt;　　关于什么是RDD，可以参考上一篇 &lt;a href=&quot;../spark-what-is-rdd&quot;&gt;4. spark RDD 之：什么是RDD&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;2. RDD transfomation 一览&lt;/h2&gt;

&lt;p&gt;　　ok，了解了RDD的含义，我们可以来看看神马叫transformation了，中文叫转换。上一篇提到RDD可以由两种方式创建，而在实际应用中，一般第一种方式都是用于新建一个RDD的时候，大多数时候都是通过第二种方式来生成一个新的RDD。so，想想这里我们应该怎么来根据一个已存在的RDD来transform出另一个新的RDD呢？当然就是根据一些规则，比如说筛选，映射，分组等等，而用于支撑这些规则的函数，就叫做RDD的transformation。&lt;br/&gt;
　　下面我们通过下面这张表来看看RDD都支持哪些规则的transformation吧。 这些是&lt;a href=&quot;http://spark.apache.org/docs/latest/programming-guide.html#transformations&quot;&gt;官方&lt;/a&gt;列出的一些常用的transformation，我原本想把所有transformation列出来的，可考虑到2/8原则，想想下面这些在实际应用中应该足够了。如果真的需要其他transformation的时候，相比彼时你的功力应该已到阅读、贡献源码的级别了。也就不需要再参考我下面将要写的东西了。 &lt;br/&gt;
　　这些transformation中有一些是我不太熟悉的，所以这里记录一下我不太熟悉的那些转换函数的用法，仅供个人参考哦。&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt; No &lt;/th&gt;
&lt;th&gt; Transformation  &lt;/th&gt;
&lt;th&gt;  Meaning &lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt; 1 &lt;/td&gt;
&lt;td&gt; map(func) &lt;/td&gt;
&lt;td&gt; Return a new distributed dataset formed by passing each element of the source through a function func.  &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; 2 &lt;/td&gt;
&lt;td&gt; filter(func) &lt;/td&gt;
&lt;td&gt; Return a new dataset formed by selecting those elements of the source on which func returns true. &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; 3 &lt;/td&gt;
&lt;td&gt; flatMap(func) &lt;/td&gt;
&lt;td&gt; Similar to map, but each input item can be mapped to 0 or more output items (so func should return a Seq rather than a single item). &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; 4 &lt;/td&gt;
&lt;td&gt; mapPartitions(func) &lt;/td&gt;
&lt;td&gt; Similar to map, but runs separately on each partition (block) of the RDD, so func must be of type Iterator&lt;T&gt; =&gt; Iterator&lt;U&gt; when running on an RDD of type T. &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; 5 &lt;/td&gt;
&lt;td&gt; mapPartitionsWithIndex(func) &lt;/td&gt;
&lt;td&gt; Similar to mapPartitions, but also provides func with an integer value representing the index of the partition, so func must be of type (Int, Iterator&lt;T&gt;) =&gt; Iterator&lt;U&gt; when running on an RDD of type T. &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; 6 &lt;/td&gt;
&lt;td&gt; sample(withReplacement, fraction, seed) &lt;/td&gt;
&lt;td&gt; Sample a fraction fraction of the data, with or without replacement, using a given random number generator seed. &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; 7 &lt;/td&gt;
&lt;td&gt; union(otherDataset) &lt;/td&gt;
&lt;td&gt; Return a new dataset that contains the union of the elements in the source dataset and the argument. &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; 8 &lt;/td&gt;
&lt;td&gt; intersection(otherDataset) &lt;/td&gt;
&lt;td&gt; Return a new RDD that contains the intersection of elements in the source dataset and the argument. &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; 9 &lt;/td&gt;
&lt;td&gt; distinct([numTasks])) &lt;/td&gt;
&lt;td&gt; Return a new dataset that contains the distinct elements of the source dataset. &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; 10 &lt;/td&gt;
&lt;td&gt; groupByKey([numTasks]) &lt;/td&gt;
&lt;td&gt; When called on a dataset of (K, V) pairs, returns a dataset of (K, Iterable&lt;V&gt;) pairs.   &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; 11 &lt;/td&gt;
&lt;td&gt; reduceByKey(func, [numTasks]) &lt;/td&gt;
&lt;td&gt; When called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function func, which must be of type (V,V) =&gt; V. Like in groupByKey, the number of reduce tasks is configurable through an optional second argument. &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; 12 &lt;/td&gt;
&lt;td&gt; aggregateByKey(zeroValue)(seqOp, combOp, [numTasks]) &lt;/td&gt;
&lt;td&gt; When called on a dataset of (K, V) pairs, returns a dataset of (K, U) pairs where the values for each key are aggregated using the given combine functions and a neutral &quot;zero&quot; value. Allows an aggregated value type that is different than the input value type, while avoiding unnecessary allocations. Like in groupByKey, the number of reduce tasks is configurable through an optional second argument. &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; 13 &lt;/td&gt;
&lt;td&gt; sortByKey([ascending], [numTasks]) &lt;/td&gt;
&lt;td&gt; When called on a dataset of (K, V) pairs where K implements Ordered, returns a dataset of (K, V) pairs sorted by keys in ascending or descending order, as specified in the boolean ascending argument. &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; 14 &lt;/td&gt;
&lt;td&gt; join(otherDataset, [numTasks]) &lt;/td&gt;
&lt;td&gt; When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs with all pairs of elements for each key. Outer joins are also supported through leftOuterJoin and rightOuterJoin. &lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;


&lt;h2&gt;3. RDD transfomation&lt;/h2&gt;

&lt;p&gt;　　RDD 的transformation有几个和scala里的函数组合子一样，其他的我估计也是基于scala的组合子来写的。所以，为了方便起见，能用scala来展现的，我就用scala来写；不可以的，我再用spark来写示例。下面提到的transformation更新到&lt;a href=&quot;http://spark.apache.org/docs/latest/programming-guide.html#transformations&quot;&gt;spark 1.1.1版本&lt;/a&gt;。要想查看最新版本的，可以上&lt;a href=&quot;http://spark.apache.org/&quot;&gt;官网&lt;/a&gt;和&lt;a href=&quot;http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD&quot;&gt;官方API文档&lt;/a&gt;。这里顺带提一下，说到transformation，网上几乎每篇说spark的文章都会用到下面这张图。可是我真的想说，这张图可是Matei这哥们2012年发的&lt;a href=&quot;https://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf&quot;&gt;RDD论文&lt;/a&gt;里的啊，spark更新的速度也许比大多数人换对象的速度还快啊，好多东西已经变了。大家以后要是发这张图，就该说明出处和版本吧，免得大家误解transformation和actions就那几种类型啊。&lt;br/&gt;
　　以下都是基于官方&lt;a href=&quot;http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD&quot;&gt;API DOC&lt;/a&gt;，在我本机上的测试以及网络资料整理而来。所有参考过的资料我都会在最后列出来，供参考。
&lt;img src=&quot;../../images/trans-action-2012.png&quot; alt=&quot;trans-action-2012&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;3.1 RDD transfomation － flatMap&lt;/h2&gt;

&lt;p&gt;　　说明：&lt;br/&gt;
　　flatMap可以理解成map和flat的组合。他处理一个嵌套列表，对其中每个列表中的元素执行map，然后对每个列表执行flat，最后返回一个列表。 &lt;br/&gt;
　　所以，我们也可以先flatten一个列表，再对列表里的每个元素做mapping；当然也可以对嵌套列表里的每个元素做mapping，再对列表做flatten。看下面的例子就明白了：&lt;/p&gt;

&lt;p&gt;　　定义：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def
flatMap[U](f: (T) ⇒ TraversableOnce[U])(implicit arg0: ClassTag[U]): RDD[U]
Return a new RDD by first applying a function to all elements of this RDD, and then flattening the results.   
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　示例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// flatMap operation
scala&amp;gt; val test = List(List(1,2,3,4,5), List(10,20,30,40,50))
test: List[List[Int]] = List(List(1, 2, 3, 4, 5), List(10, 20, 30, 40, 50))

scala&amp;gt; val test1 = test.flatMap( x =&amp;gt; x.map(_ * 2))
test1: List[Int] = List(2, 4, 6, 8, 10, 20, 40, 60, 80, 100)

// flat first, then mapping
scala&amp;gt; val tset2 = test.flatten
tset2: List[Int] = List(1, 2, 3, 4, 5, 10, 20, 30, 40, 50)

scala&amp;gt; val test3 = test.flatten.map(_*2)
test3: List[Int] = List(2, 4, 6, 8, 10, 20, 40, 60, 80, 100)

// mapping first, then flat
scala&amp;gt; val test4 = test.map( x =&amp;gt; x.map(_*2))
test4: List[List[Int]] = List(List(2, 4, 6, 8, 10), List(20, 40, 60, 80, 100))

scala&amp;gt; val test5 = test.map(x =&amp;gt; x.map(_*2)).flatten
test5: List[Int] = List(2, 4, 6, 8, 10, 20, 40, 60, 80, 100)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;3.2 RDD transfomation － mapPartitions&lt;/h2&gt;

&lt;p&gt;　　说明：&lt;br/&gt;
　　mapPartitions是map的一个变种。map的输入函数是应用于RDD中每个元素，而mapPartitions的输入函数是应用于每个分区，也就是把每个分区中的内容作为整体来处理的。&lt;/p&gt;

&lt;p&gt;　　定义：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def  
mapPartitions[U](f: (Iterator[T]) ⇒ Iterator[U], preservesPartitioning: Boolean = false)(implicit arg0: ClassTag[U]): RDD[U]
Return a new RDD by applying a function to each partition of this RDD.

preservesPartitioning indicates whether the input function preserves the partitioner, which should be false unless this is a pair RDD and the input function doesn&#39;t modify the keys.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　f即为输入函数，它处理每个分区里面的内容。每个分区中的内容将以Iterator[T]传递给输入函数f，f的输出结果是Iterator[U]。最终的RDD由所有分区经过输入函数处理后的结果合并起来的。&lt;/p&gt;

&lt;h2&gt;3.3 RDD transfomation － mapPartitionsWithIndex | mapPartitionsWithContext&lt;/h2&gt;

&lt;p&gt;　　说明： &lt;br/&gt;
　　mapPartitions还有些变种，比如mapPartitionsWithContext，它能把处理过程中的一些状态信息传递给用户指定的输入函数。还有mapPartitionsWithIndex，它能把分区的index传递给用户指定的输入函数。&lt;/p&gt;

&lt;p&gt;　　定义：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def
mapPartitionsWithContext[U](f: (TaskContext, Iterator[T]) ⇒ Iterator[U], preservesPartitioning: Boolean = false)(implicit arg0: ClassTag[U]): RDD[U]
Return a new RDD by applying a function to each partition of this RDD. This is a variant of mapPartitions that also passes the TaskContext into the closure.

preservesPartitioning indicates whether the input function preserves the partitioner, which should be false unless this is a pair RDD and the input function does not modify the keys.
Annotations

def
mapPartitionsWithIndex[U](f: (Int, Iterator[T]) ⇒ Iterator[U], preservesPartitioning: Boolean = false)(implicit arg0: ClassTag[U]): RDD[U]
Return a new RDD by applying a function to each partition of this RDD, while tracking the index of the original partition.

preservesPartitioning indicates whether the input function preserves the partitioner, which should be false unless this is a pair RDD and the input function doesn&#39;t modify the keys.
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;3.3 RDD transfomation －  sample&lt;/h2&gt;

&lt;p&gt;　　说明： &lt;br/&gt;
　　根据fraction指定的比例，对数据进行采样，可以选择是否用随机数进行替换，seed用于指定随机数生成器种子。这里我一直有一个疑问，当我的数据集里有100个元素是，设置fraction为0.1，按理应该是返回10个随机数的，可是就返回了6个。似乎返回的随机数会少于数据集元素数量*随机数的比例，晚点继续研究。&lt;/p&gt;

&lt;p&gt;　　定义：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def  sample(withReplacement: Boolean, fraction: Double, seed: Long = Utils.random.nextLong): RDD[T]
Return a sampled subset of this RDD.  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　示例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; val a = sc.parallelize(1 to 100)
a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[17] at parallelize at &amp;lt;console&amp;gt;:12

scala&amp;gt; val sample = a.sample(false, 0.1, 0)
sample: org.apache.spark.rdd.RDD[Int] = PartitionwiseSampledRDD[18] at sample at &amp;lt;console&amp;gt;:14

scala&amp;gt; sample.count
.
.
.
14/12/12 10:39:07 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool 
14/12/12 10:39:07 INFO SparkContext: Job finished: count at &amp;lt;console&amp;gt;:17, took 3.491275316 s
res29: Long = 6

scala&amp;gt; sample.collect
.
.
.
14/12/12 10:39:11 INFO DAGScheduler: Stage 24 (collect at &amp;lt;console&amp;gt;:17) finished in 0.142 s
14/12/12 10:39:11 INFO SparkContext: Job finished: collect at &amp;lt;console&amp;gt;:17, took 0.151221638 s
res30: Array[Int] = Array(22, 46, 48, 80, 87, 97)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　后话：&lt;br/&gt;
　　在看API DOC的时候发现有一个和sample很像的transformation，takeSample，但准确的说这并不是一个transformation，应该算是一个action吧。这个函数可以从数据集里返回固定数量的随机数，弥补上上面我提到的sample那个问题。但是需要注意的是，这个函数是直接在RDD上计算，返回计算结果，并不是一个transformation。看下面的例子就明白了。&lt;/p&gt;

&lt;p&gt;　　示例2：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; val takeSample = a.takeSample(false, 10, 0)
.
.
.
14/12/12 10:43:24 INFO DAGScheduler: Stage 26 (takeSample at &amp;lt;console&amp;gt;:14) finished in 0.068 s
14/12/12 10:43:24 INFO SparkContext: Job finished: takeSample at &amp;lt;console&amp;gt;:14, took 0.074973222 s
takeSample: Array[Int] = Array(68, 18, 97, 26, 61, 33, 67, 10, 2, 1)

scala&amp;gt; takeSample
res33: Array[Int] = Array(68, 18, 97, 26, 61, 33, 67, 10, 2, 1)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;3.4 RDD transfomation － union&lt;/h2&gt;

&lt;p&gt;　　说明：&lt;br/&gt;
　　这个把两个RDD合并为一个，很简单，可以理解成计算并集。但值得说明的是，union和++运算符是等价的，至少在函数定义上是完全一致的。先了解下，以后有需要的时候再看是否有细节上的区别。&lt;/p&gt;

&lt;p&gt;　　定义：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def
++(other: RDD[T]): RDD[T]
Return the union of this RDD and another one. Any identical elements will appear multiple times (use .distinct() to eliminate them).
def
union(other: RDD[T]): RDD[T]
Return the union of this RDD and another one. Any identical elements will appear multiple times (use .distinct() to eliminate them).
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　示例：&lt;br/&gt;
    scala&gt; a
    res37: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[17] at parallelize at &lt;console&gt;:12&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; val a = sc.parallelize(1 to 10)
a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[20] at parallelize at &amp;lt;console&amp;gt;:12

scala&amp;gt; val b = sc.parallelize(11 to 20)
b: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[21] at parallelize at &amp;lt;console&amp;gt;:12

scala&amp;gt; val c1 = a++b
c1: org.apache.spark.rdd.RDD[Int] = UnionRDD[22] at $plus$plus at &amp;lt;console&amp;gt;:16

scala&amp;gt; val c2 = a.union(b)

scala&amp;gt; c1.collect
.
.
.
14/12/12 11:01:13 INFO SparkContext: Job finished: collect at &amp;lt;console&amp;gt;:19, took 0.3475174 s
res38: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20)

scala&amp;gt; c2.collect
.
.
.
14/12/12 11:01:17 INFO SparkContext: Job finished: collect at &amp;lt;console&amp;gt;:19, took 0.122094635 s
res39: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;3.5 RDD transfomation － intersection&lt;/h2&gt;

&lt;p&gt;　　说明： &lt;br/&gt;
　　对比上面的union就很好理解了，上面是计算并集，这里是计算交集。这个transformation也有需要注意的地方，就是其有三种形式，根据是否提供第二个参数以及第二个参数的类型。具体可以参考函数定义。c++里这叫多态，现在对scala才是初学阶段，我想既然scala也是OO，那应该也有多态类似的概念吧，这里晚点继续补上。&lt;/p&gt;

&lt;p&gt;　　定义：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def
intersection(other: RDD[T], numPartitions: Int): RDD[T]
Return the intersection of this RDD and another one. The output will not contain any duplicate elements, even if the input RDDs did. Performs a hash partition across the cluster

Note that this method performs a shuffle internally.
numPartitions
How many partitions to use in the resulting RDD

def
intersection(other: RDD[T], partitioner: Partitioner)(implicit ord: Ordering[T] = null): RDD[T]
Return the intersection of this RDD and another one. The output will not contain any duplicate elements, even if the input RDDs did.

Note that this method performs a shuffle internally.
partitioner
Partitioner to use for the resulting RDD

def
intersection(other: RDD[T]): RDD[T]
Return the intersection of this RDD and another one. The output will not contain any duplicate elements, even if the input RDDs did.

Note that this method performs a shuffle internally.  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　示例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;val x = sc.parallelize(1 to 20)
val y = sc.parallelize(10 to 30)
val z = x.intersection(y)

z.collect
res74: Array[Int] = Array(16, 12, 20, 13, 17, 14, 18, 10, 19, 15, 11)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;3.6 RDD transfomation － distinct&lt;/h2&gt;

&lt;p&gt;　　说明： &lt;br/&gt;
　　即去重，相当于python里面的set。&lt;/p&gt;

&lt;p&gt;　　定义：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def
distinct(): RDD[T]
Return a new RDD containing the distinct elements in this RDD.
def
distinct(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T]
Return a new RDD containing the distinct elements in this RDD.
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;3.7 RDD transfomation － groupByKey&lt;/h2&gt;

&lt;p&gt;　　说明： &lt;br/&gt;
　　这是一个针对键值对结构来进行操作的转换方法，即你的RDD的结构是(key, value)类型，而其中key不是唯一性的。此时我们可以对这个RDD进行groupByKey的转换得到新的RDD，新的RDD的结构同样也是键值对，只是值改变了，并且键是唯一性的，即(key, iterator(value))。&lt;/p&gt;

&lt;p&gt;　　定义：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def
groupByKey(): RDD[(K, Iterable[V])]
Group the values for each key in the RDD into a single sequence. Hash-partitions the resulting RDD with the existing partitioner/parallelism level.

Note: This operation may be very expensive. If you are grouping in order to perform an aggregation (such as a sum or average) over each key, using PairRDDFunctions.aggregateByKey or PairRDDFunctions.reduceByKey will provide much better performance.
def
groupByKey(numPartitions: Int): RDD[(K, Iterable[V])]
Group the values for each key in the RDD into a single sequence. Hash-partitions the resulting RDD with into numPartitions partitions.

Note: This operation may be very expensive. If you are grouping in order to perform an aggregation (such as a sum or average) over each key, using PairRDDFunctions.aggregateByKey or PairRDDFunctions.reduceByKey will provide much better performance.
def
groupByKey(partitioner: Partitioner): RDD[(K, Iterable[V])]
Group the values for each key in the RDD into a single sequence. Allows controlling the partitioning of the resulting key-value pair RDD by passing a Partitioner.

Note: This operation may be very expensive. If you are grouping in order to perform an aggregation (such as a sum or average) over each key, using PairRDDFunctions.aggregateByKey or PairRDDFunctions.reduceByKey will provide much better performance.
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;3.8 RDD transfomation － reduceByKey&lt;/h2&gt;

&lt;p&gt;　　说明：  &lt;br/&gt;
　　和上面的groupByKey一样，这也是一个针对(key, value)型结构的RDD的转换函数。只是上面的groupByKey是将相同key对应的value组合成一个可迭代的对象；而reduceByKey是将相同key对应的value通过传入的函数func计算成一个新的value。&lt;/p&gt;

&lt;p&gt;　　定义：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def
reduceByKey(func: (V, V) ⇒ V): RDD[(K, V)]
Merge the values for each key using an associative reduce function. This will also perform the merging locally on each mapper before sending results to a reducer, similarly to a &quot;combiner&quot; in MapReduce. Output will be hash-partitioned with the existing partitioner/ parallelism level.
def
reduceByKey(func: (V, V) ⇒ V, numPartitions: Int): RDD[(K, V)]
Merge the values for each key using an associative reduce function. This will also perform the merging locally on each mapper before sending results to a reducer, similarly to a &quot;combiner&quot; in MapReduce. Output will be hash-partitioned with numPartitions partitions.
def
reduceByKey(partitioner: Partitioner, func: (V, V) ⇒ V): RDD[(K, V)]
Merge the values for each key using an associative reduce function. This will also perform the merging locally on each mapper before sending results to a reducer, similarly to a &quot;combiner&quot; in MapReduce.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　示例：　&lt;/p&gt;

&lt;h2&gt;3.9 RDD transfomation － aggregateByKey&lt;/h2&gt;

&lt;p&gt;　　说明：&lt;/p&gt;

&lt;p&gt;　　定义：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def
aggregateByKey[U](zeroValue: U)(seqOp: (U, V) ⇒ U, combOp: (U, U) ⇒ U)(implicit arg0: ClassTag[U]): RDD[(K, U)]
Aggregate the values of each key, using given combine functions and a neutral &quot;zero value&quot;. This function can return a different result type, U, than the type of the values in this RDD, V. Thus, we need one operation for merging a V into a U and one operation for merging two U&#39;s, as in scala.TraversableOnce. The former operation is used for merging values within a partition, and the latter is used for merging values between partitions. To avoid memory allocation, both of these functions are allowed to modify and return their first argument instead of creating a new U.
def
aggregateByKey[U](zeroValue: U, numPartitions: Int)(seqOp: (U, V) ⇒ U, combOp: (U, U) ⇒ U)(implicit arg0: ClassTag[U]): RDD[(K, U)]
Aggregate the values of each key, using given combine functions and a neutral &quot;zero value&quot;. This function can return a different result type, U, than the type of the values in this RDD, V. Thus, we need one operation for merging a V into a U and one operation for merging two U&#39;s, as in scala.TraversableOnce. The former operation is used for merging values within a partition, and the latter is used for merging values between partitions. To avoid memory allocation, both of these functions are allowed to modify and return their first argument instead of creating a new U.
def
aggregateByKey[U](zeroValue: U, partitioner: Partitioner)(seqOp: (U, V) ⇒ U, combOp: (U, U) ⇒ U)(implicit arg0: ClassTag[U]): RDD[(K, U)]
Aggregate the values of each key, using given combine functions and a neutral &quot;zero value&quot;. This function can return a different result type, U, than the type of the values in this RDD, V. Thus, we need one operation for merging a V into a U and one operation for merging two U&#39;s, as in scala.TraversableOnce. The former operation is used for merging values within a partition, and the latter is used for merging values between partitions. To avoid memory allocation, both of these functions are allowed to modify and return their first argument instead of creating a new U.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　示例：　&lt;/p&gt;

&lt;h2&gt;3.10 RDD transfomation － sortByKey&lt;/h2&gt;

&lt;p&gt;　　说明：  &lt;br/&gt;
　　按key来排序。但是在一些情况下，当RDD是(key, value)类型时，如果想对value来排序应该怎么处理呢？很简单，就是先把原来的(key, value)转换成(value, key)结构，然后对(value, key)进行sortByKey操作，最后再把已经排序了的(value, key)转换回(key, value)形式。&lt;/p&gt;

&lt;p&gt;　　定义：   &lt;br/&gt;
　　奇怪了，我再官方API DOC里没有找到这个转换函数的定义，晚一点再看一下。&lt;/p&gt;

&lt;p&gt;　　示例：　&lt;/p&gt;

&lt;h2&gt;3.11 RDD transfomation － join&lt;/h2&gt;

&lt;p&gt;　　说明：  &lt;br/&gt;
　　把两个(key, value)结构的RDD合成一个新的(key, value)结构的RDD。即(k1, v1).join((k1, v2)) =&gt; (k1, (v1, v2))，需要注意的是，为了让join操作成功，必须保证key是可以比较的。&lt;/p&gt;

&lt;p&gt;　　定义：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def
join[W](other: RDD[(K, W)], numPartitions: Int): RDD[(K, (V, W))]
Return an RDD containing all pairs of elements with matching keys in this and other. Each pair of elements will be returned as a (k, (v1, v2)) tuple, where (k, v1) is in this and (k, v2) is in other. Performs a hash join across the cluster.
def
join[W](other: RDD[(K, W)]): RDD[(K, (V, W))]
Return an RDD containing all pairs of elements with matching keys in this and other. Each pair of elements will be returned as a (k, (v1, v2)) tuple, where (k, v1) is in this and (k, v2) is in other. Performs a hash join across the cluster.
def
join[W](other: RDD[(K, W)], partitioner: Partitioner): RDD[(K, (V, W))]
Return an RDD containing all pairs of elements with matching keys in this and other. Each pair of elements will be returned as a (k, (v1, v2)) tuple, where (k, v1) is in this and (k, v2) is in other. Uses the given Partitioner to partition the output RDD.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　示例：　&lt;br/&gt;
　　&lt;/p&gt;

&lt;h2&gt;4，一些资源&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://blog.csdn.net/anzhsoft/article/details/39851421&quot;&gt;RDD&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://twitter.github.io/scala_school/zh_cn/collections.html#flatMap&quot;&gt;scala school from twitter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.zybuluo.com/jewes/note/35032&quot;&gt;RDD Reference 1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://homepage.cs.latrobe.edu.au/zhe/ZhenHeSparkRDDAPIExamples.html&quot;&gt;RDD API Examples&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD&quot;&gt;RDD API Docs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://spark.apache.org/docs/1.1.1/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions&quot;&gt;pairRDD API Docs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
   <entry>
     <title>［touch spark］4. spark RDD 之：什么是RDD</title>
     <link href="/spark-what-is-rdd"/>
     <updated>2014-12-08T00:00:00+08:00</updated>
     <id>/spark-what-is-rdd</id>
     <content type="html">&lt;h2&gt;&lt;/h2&gt;

&lt;h2&gt;1. 什么是RDD&lt;/h2&gt;

&lt;p&gt;　　先看下源码里是怎么描述RDD的。&lt;/p&gt;

&lt;blockquote&gt;&lt;blockquote&gt;&lt;p&gt;Internally, each RDD is characterized by five main properties:&lt;br/&gt;
A list of partitions&lt;br/&gt;
A function for computing each split &lt;br/&gt;
A list of dependencies on other RDDs&lt;br/&gt;
Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned) &lt;br/&gt;
Optionally, a list of preferred locations to compute each split on (e.g. block locations for an HDFS file)&lt;/p&gt;&lt;/blockquote&gt;&lt;/blockquote&gt;

&lt;p&gt;　　每个RDD有5个主要的属性：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;一组分片（partition），即数据集的基本组成单位&lt;/li&gt;
&lt;li&gt;一个计算每个分片的函数&lt;/li&gt;
&lt;li&gt;对parent RDD的依赖，这个依赖描述了RDD之间的lineage&lt;/li&gt;
&lt;li&gt;对于key-value的RDD，一个Partitioner，这是可选择的&lt;/li&gt;
&lt;li&gt;一个列表，存储存取每个partition的preferred位置。对于一个HDFS文件来说，存储每个partition所在的块的位置。这也是可选择的&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;　　把上面这5个主要的属性总结一下，可以得出RDD的大致概念。首先要知道，RDD大概是这样一种表示数据集的东西，它具有以上列出的一些属性。是spark项目组设计用来表示数据集的一种数据结构。而spark项目组为了让RDD能handle更多的问题，又规定RDD应该是只读的，分区记录的一种数据集合中。可以通过两种方式来创建RDD：一种是基于物理存储中的数据，比如说磁盘上的文件；另一种，也是大多数创建RDD的方式，即通过其他RDD来创建【以后叫做转换】而成。而正因为RDD满足了这么多特性，所以spark把RDD叫做Resilient Distributed Datasets，中文叫做弹性分布式数据集。很多文章都是先讲RDD的定义，概念，再来说RDD的特性。我觉得其实也可以倒过来，通过RDD的特性反过来理解RDD的定义和概念，通过这种由果溯因的方式来理解RDD也未尝不可。反正对我个人而言这种方式是挺好的。&lt;/p&gt;

&lt;h2&gt;2. 理解RDD的几个关键概念&lt;/h2&gt;

&lt;p&gt;　　本来我是想参考RDD的论文和自己的理解来整理这篇文章的，可是后来想想这样是不是有点过于细致了。我想，认识一个新事物，在时间、资源有限的情况下，不必锱铢必较，可以先focus on几个关键点，到后期应用的时候再步步深入。&lt;br/&gt;
　　所以，按照我个人的理解，我认为想用好spark，必须要理解RDD，而为了理解RDD，我认为只要了解下面几个RDD的几个关键点就能handle很多情况下的问题了。所以，下面所有列到的点，都是在我个人看来很重要的，但也许有所欠缺，大家如果想继续深入，可以看第三部分列出的参考资料，谢谢。
　　&lt;/p&gt;

&lt;h3&gt;2.1 RDD的背景及解决的痛点问题&lt;/h3&gt;

&lt;p&gt;　　按照RDD的paper来讲，RDD的设计是为了充分利用分布式系统中的内存资源，使得提升一些特定的应用的效率。这里所谓的特定的应用没有明确定义，但可以理解为一类应用到迭代算法，图算法等需要重复利用数据的应用类型；除此之外，RDD还可以应用在交互式大数据处理方面。所以，我们这里需要明确一下：RDD并不是万能的，也不是什么带着纱巾的少女那样神奇。简单的理解，就是一群大牛为了解决一个问题而设计的一个特定的数据结构，that&#39;s all。&lt;/p&gt;

&lt;h3&gt;2.2 What is DAG - 趣说有向无环图&lt;/h3&gt;

&lt;p&gt;　　DAG - Direct Acyclic Graph，有向五无图，好久没看图片了，先发个图片来理解理解吧。
&lt;img src=&quot;../../images/dag.jpg&quot; alt=&quot;DAG&quot; /&gt;&lt;br/&gt;
　　要理解DAG，只需弄明白三个概念就可以毕业了，首先，我们假设上图图二中的A,B,C,D,E都代表spark里不同的RDD：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;图：图是表达RDD Lineage信息的一个结构，在spark中，大部分RDD都是通过其他RDD进行转换而来的，比如说上图图二中，B和D都是通过A转换而来的，而C是通过B转换而来，E的话是通过B和D一起转换来的。&lt;/li&gt;
&lt;li&gt;有向：有向就更容易理解了，简单来说就是linage是一个top-down的结构，而且是时间序列上的top-down结构，这里不是很好理解，我们在下面讲“无环”这个概念是一起说明。&lt;/li&gt;
&lt;li&gt;&lt;p&gt;无环：这里就是重点要理解的地方了，我猜想spark的优化器在这里也发挥了很大的作用。首先，我们先理解一下无环的概念，假设有图三中左下B,D,E这样一个RDD转换图，那当我们的需要执行D.collect操作的时候，就会引发一个死循环了。不过，仔细想过的话，就会知道，“无环”这个问题其实已经在“有向”这个概念中提现了，上面说的“有向”，其实更详细的说是一个时间上的先来后到，即祖先与子孙的关系，是不可逆的。举个例子，我们按照时间序列分析一下图下左下的B,D,E三个RDD：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt; B通过某种方式初始化了第一个RDD【这里我们抛却A,C不谈】；&lt;/li&gt;
&lt;li&gt; D通过某种转换从B生成第二个RDD；&lt;/li&gt;
&lt;li&gt; E通过某种转换从D生成第三个RDD；&lt;/li&gt;
&lt;li&gt; 现在B这个RDD已经存在了，所以根本无从说明B是从E通过转换生成的，为啥，因为B已经存在了；&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;　　够清楚了吗，啥，还不够清楚，好，那我告诉你，B是小明他爷爷，D是小明他爸爸，E是小明自己，你说小明他爷爷能是小明通过某种方式转换出现在这个世界上的吗？&lt;/p&gt;

&lt;h3&gt;2.3 What is Data Locality - RDD的位置可见性&lt;/h3&gt;

&lt;p&gt;　　这个问题就不重复造轮子了，直接引用Quora上的一个&lt;a href=&quot;https://www.quora.com/How-do-I-make-clear-the-concept-of-RDD-in-Spark&quot;&gt;问答了&lt;/a&gt;。&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;RDD is a dataset which is distributed, that is, it is divided into &quot;partitions&quot;. Each of these partitions can be present in the memory or disk of different machines. If you want Spark to process the RDD, then Spark needs to launch one task per partition of the RDD. Its best that each task be sent to the machine have the partition that task is supposed to process. In that case, the task will be able to read the data of the partition from the local machine. Otherwise, the task would have to pull the partition data over the network from a different machine, which is less efficient. This scheduling of tasks (that is, allocation of tasks to machines) such that the tasks can read data &quot;locally&quot; is known as &quot;locality aware scheduling&quot;.&lt;/p&gt;&lt;/blockquote&gt;

&lt;h3&gt;2.4 What is Lazy Evaluation - 神马叫惰性求值&lt;/h3&gt;

&lt;p&gt;　　本来不想叫“惰性求值”的，看到“惰”这个字实在是各种不爽，实际上，我觉得应该叫&quot;后续求值&quot;，&quot;按需计算&quot;，&quot;晚点搞&quot;这类似的，哈哈。这几天一直在想应该怎么简单易懂地来表达Lazy Evaluation这个概念，本来打算引用MongoDB的Cursor来类比一下的，可总觉得还是小题大做了。这个概念就懒得解释了，主要是觉得太简单了，没有必要把事情搞得这么复杂，哈哈。&lt;/p&gt;

&lt;h3&gt;2.5 What is Narrow/Wide Dependency - RDD的宽依赖和窄依赖&lt;/h3&gt;

&lt;p&gt;　　首先，先从原文看看宽依赖和窄依赖各自的定义。&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;　　&lt;strong&gt;narrow dependencies&lt;/strong&gt;: where each partition of the parent RDD is used by at most one partition of the child RDD, &lt;strong&gt;wide dependencis&lt;/strong&gt;, where multiple child partitions may depend on it.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;　　按照&lt;a href=&quot;http://shiyanjun.cn/archives/744.html&quot;&gt;这篇RDD论文中文译文&lt;/a&gt;的解释，窄依赖是指子RDD的每个分区依赖于常数个父分区（即与数据规模无关）；宽依赖指子RDD的每个分区依赖于所有父RDD分区。暂且不说这样理解是否有偏差，我们先来从两个方面了解下计算一个窄依赖的子RDD和一个宽依赖的RDD时具体都有什么区别，然后再回顾这个定义。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;计算方面：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;计算窄依赖的子RDD：可以在某一个计算节点上直接通过父RDD的某几块数据（通常是一块）计算得到子RDD某一块的数据；&lt;/li&gt;
&lt;li&gt;计算宽依赖的子RDD：子RDD某一块数据的计算必须等到它的父RDD所有数据都计算完成之后才可以进行，而且需要对父RDD的计算结果进行hash并传递到对应的节点之上；&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;容错恢复方面：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;窄依赖：当父RDD的某分片丢失时，只有丢失的那一块数据需要被重新计算；&lt;/li&gt;
&lt;li&gt;宽依赖：当父RDD的某分片丢失时，需要把父RDD的所有分区数据重新计算一次，计算量明显比窄依赖情况下大很多；&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;3. 尚未提到的一些重要概念&lt;/h2&gt;

&lt;p&gt;　　还有一些基本概念上面没有提到，一些是因为自己还没怎么弄清楚，一些是觉得重要但是容易理解的，所以就先不记录下来了。比如说：粗粒度、细粒度；序列化和反序列化等。
　　&lt;/p&gt;

&lt;h2&gt;4. 参考资料&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://blog.csdn.net/anzhsoft/article/details/39851421&quot;&gt;Spark技术内幕：究竟什么是RDD&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf&quot;&gt;Resilient Distributed Datasets: A Fault-Tolerant Abstraction for
In-Memory Cluster Computing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://shiyanjun.cn/archives/744.html&quot;&gt;RDD 论文中文版&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;　　&lt;/p&gt;
</content>
   </entry>
   
   <entry>
     <title>［touch spark］3. 使用Spark分析wikipedia流量数据</title>
     <link href="/using-amazon-aws-2"/>
     <updated>2014-12-02T00:00:00+08:00</updated>
     <id>/using-amazon-aws-2</id>
     <content type="html">&lt;p&gt;　　本文是接上一篇的，所以序号就延续下来了。上一篇主要记录一些EC2配置和启动的问题，有兴趣请移步&lt;a href=&quot;../using-amazon-aws-1&quot;&gt;Amazon AWS EC2 入门&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;&lt;/h2&gt;

&lt;h2&gt;&lt;/h2&gt;

&lt;h2&gt;4. 利用spark来分析wikipedia流量数据&lt;/h2&gt;

&lt;p&gt;　　启动spark shell。路径在/root/spark/spark-shell。&lt;/p&gt;

&lt;h3&gt;4.1  热身&lt;/h3&gt;

&lt;p&gt;　　创建一个RDD，在spark-shell中，可以用sc代替SparkContext来创建RDD。这里需要注意一点，在Scala中有两种变量类型var和val，其中var是variable的缩写，val是value的缩写。顾名思义，var是可变的，val是不可变的。简单的可以把val理解成C/C++里的常量，或者Erlang里的变量【Erlang里的变量具有单次赋值的特征】。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;&quot;&gt;scala&amp;gt; var a =&quot;aaa&quot;
a: java.lang.String = aaa

scala&amp;gt; a = &quot;a&quot;
a: java.lang.String = a

scala&amp;gt; val b = &quot;aaa&quot;
b: java.lang.String = aaa

scala&amp;gt; b = &quot;a&quot;
&amp;lt;console&amp;gt;:12: error: reassignment to val
       b = &quot;a&quot;
         ^
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　这里我们需要用val来指定一个新建的RDD，原因有2：第一，我们不需要对RDD做in place的改变，所以可以采用val来指定；其次，我们不应该对RDD做in place的改变，所以必须采用val来指定。下面，我们新建一个val型pagecounts变量，读取wikipedia 20GB的流量数据，并以两种方式打印前3条数据。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;&quot;&gt;scala&amp;gt; val pagecounts = sc.textFile(&quot;/wiki/pagecounts&quot;)
14/12/04 05:58:35 INFO mapred.FileInputFormat: Total input paths to process : 74
pagecounts: spark.RDD[String] = spark.MappedRDD@2fddef87

scala&amp;gt; pagecounts.take(3)
14/12/04 05:58:49 INFO spark.SparkContext: Starting job...
14/12/04 05:58:49 INFO spark.CacheTracker: Registering RDD ID 1 with cache
14/12/04 05:58:49 INFO spark.CacheTrackerActor: Registering RDD 1 with 177 partitions
14/12/04 05:58:49 INFO spark.CacheTracker: Registering RDD ID 0 with cache
14/12/04 05:58:49 INFO spark.CacheTrackerActor: Registering RDD 0 with 177 partitions
14/12/04 05:58:49 INFO spark.CacheTrackerActor: Asked for current cache locations
14/12/04 05:58:49 INFO spark.MesosScheduler: Final stage: Stage 0
14/12/04 05:58:49 INFO spark.MesosScheduler: Parents of final stage: List()
14/12/04 05:58:49 INFO spark.MesosScheduler: Missing parents: List()
14/12/04 05:58:49 INFO spark.MesosScheduler: Computing the requested partition locally
14/12/04 05:58:49 INFO spark.SparkContext: Job finished in 0.098193078 s
14/12/04 05:58:49 INFO spark.SparkContext: Starting job...
14/12/04 05:58:49 INFO spark.CacheTrackerActor: Asked for current cache locations
14/12/04 05:58:49 INFO spark.MesosScheduler: Final stage: Stage 1
14/12/04 05:58:49 INFO spark.MesosScheduler: Parents of final stage: List()
14/12/04 05:58:49 INFO spark.MesosScheduler: Missing parents: List()
14/12/04 05:58:49 INFO spark.MesosScheduler: Computing the requested partition locally
14/12/04 05:58:49 INFO spark.SparkContext: Job finished in 0.026119526 s
res1: Array[String] = Array(20090505-000000 aa.b ?71G4Bo1cAdWyg 1 14463, 20090505-000000 aa.b Special:Statistics 1 840, 20090505-000000 aa.b Special:Whatlinkshere/MediaWiki:Returnto 1 1019)

scala&amp;gt; pagecounts.take(3).foreach(println)
14/12/04 05:59:16 INFO spark.SparkContext: Starting job...
14/12/04 05:59:16 INFO spark.CacheTrackerActor: Asked for current cache locations
14/12/04 05:59:16 INFO spark.MesosScheduler: Final stage: Stage 2
14/12/04 05:59:16 INFO spark.MesosScheduler: Parents of final stage: List()
14/12/04 05:59:16 INFO spark.MesosScheduler: Missing parents: List()
14/12/04 05:59:16 INFO spark.MesosScheduler: Computing the requested partition locally
14/12/04 05:59:16 INFO spark.SparkContext: Job finished in 0.004355182 s
14/12/04 05:59:16 INFO spark.SparkContext: Starting job...
14/12/04 05:59:16 INFO spark.CacheTrackerActor: Asked for current cache locations
14/12/04 05:59:16 INFO spark.MesosScheduler: Final stage: Stage 3
14/12/04 05:59:16 INFO spark.MesosScheduler: Parents of final stage: List()
14/12/04 05:59:16 INFO spark.MesosScheduler: Missing parents: List()
14/12/04 05:59:16 INFO spark.MesosScheduler: Computing the requested partition locally
14/12/04 05:59:16 INFO spark.SparkContext: Job finished in 0.016392708 s
20090505-000000 aa.b ?71G4Bo1cAdWyg 1 14463
20090505-000000 aa.b Special:Statistics 1 840
20090505-000000 aa.b Special:Whatlinkshere/MediaWiki:Returnto 1 1019
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;4.2 初试RDD Transfomation 和 RDD Action&lt;/h3&gt;

&lt;p&gt;　　下面，我们来演示一个RDD Transformation的例子。关于RDD Transformation，这篇有详细介绍和示例&lt;a href=&quot;../spark-transformers/&quot;&gt;spark RDD transformation 学习&lt;/a&gt;。首先，我们先看看这20GB的文件里有多少条数据，然后查询一下看所有流量数据中，有多少条是浏览的英文wiki。&lt;br/&gt;
　　首先，执行pagecounts.count来查看有多少条数据。这个动作会产生177个spark任务，这里是从HDFS读书数据，所以这个任务的瓶颈实在I/O这块，整个任务执行下来大概2~3分钟。这里我执行了几次，大概花了2分钟左右的时间，执行结果如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; pagecounts.count
.
.
.
14/12/05 01:19:58 INFO spark.SimpleJob: Finished TID 173 (progress: 177/177)
14/12/05 01:19:58 INFO spark.MesosScheduler: Completed ResultTask(0, 174)
14/12/05 01:19:58 INFO spark.SparkContext: Job finished in 95.251659404 s
res0: Long = 329641466
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　在任务运行的时候，可以打开web窗口访问：http://&lt;master_node_hostname&gt;:8080 来实时观察执行进度。下面是我的一个截图示例：&lt;br/&gt;
&lt;img src=&quot;../../images/mesos-cluster.png&quot; alt=&quot;mesos-cluster&quot; /&gt;&lt;/p&gt;

&lt;p&gt;　　现在，我们来利用现在这个RDD来trasform出另外一个RDD，用于记录英文wiki的数据。也通过把英文wiki的流量数据写到内存里，来比较一下数据在内存中和不在内存中两种情况下一些操作的耗时。这个测试需要下面4步：&lt;br/&gt;
　　1. 通过trasformation生成一个RDD[enPages]，记录英文wiki流量数据，因为这个步骤也需要遍历一边所有数据，所以这个步骤耗时也应该和上一个 pagecounts.count 耗时相当。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;&quot;&gt;scala&amp;gt; val enPages = pagecounts.filter(_.split(&quot; &quot;)(1) == &quot;en&quot;)
enPages: spark.RDD[String] = spark.FilteredRDD@1b8f2e35

scala&amp;gt; enPages.count
.
.
.
14/12/05 01:51:01 INFO spark.SparkContext: Job finished in 114.035390332 s
res1: Long = 122352588
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　2. 把enPages缓存到内存中&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;&quot;&gt;scala&amp;gt; enPages.cache
res0: spark.RDD[String] = spark.FilteredRDD@78bf34f4
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　3. 执行enPages.count，看看执行速度有神马区别，what happened? 按照原计划，现在不应该是神速吗？仔细看看下面的执行log，是不是有一种恍然大悟的赶脚啊？&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;&quot;&gt;scala&amp;gt; enPages.count
.
.
.
14/12/05 01:59:33 INFO spark.SimpleJob: Size of task 0:176 is 10680 bytes and took 4 ms to serialize by spark.JavaSerializerInstance
14/12/05 01:59:33 INFO spark.CacheTrackerActor: Cache entry added: (2, 176) on ip-172-31-25-137.ec2.internal (size added: 16.0B, available: 6.0GB)
14/12/05 01:59:33 INFO spark.SimpleJob: Finished TID 176 (progress: 172/177)
14/12/05 01:59:33 INFO spark.MesosScheduler: Completed ResultTask(0, 176)
14/12/05 01:59:34 INFO spark.CacheTrackerActor: Cache entry added: (2, 170) on ip-172-31-25-139.ec2.internal (size added: 10.3MB, available: 5.0GB)
14/12/05 01:59:34 INFO spark.SimpleJob: Finished TID 169 (progress: 173/177)
14/12/05 01:59:34 INFO spark.MesosScheduler: Completed ResultTask(0, 170)
14/12/05 01:59:35 INFO spark.CacheTrackerActor: Cache entry added: (2, 172) on ip-172-31-25-137.ec2.internal (size added: 183.3MB, available: 5.8GB)
14/12/05 01:59:35 INFO spark.SimpleJob: Finished TID 171 (progress: 174/177)
14/12/05 01:59:35 INFO spark.MesosScheduler: Completed ResultTask(0, 172)
14/12/05 01:59:35 INFO spark.CacheTrackerActor: Cache entry added: (2, 175) on ip-172-31-25-138.ec2.internal (size added: 16.0B, available: 6.1GB)
14/12/05 01:59:35 INFO spark.SimpleJob: Finished TID 174 (progress: 175/177)
14/12/05 01:59:35 INFO spark.MesosScheduler: Completed ResultTask(0, 175)
14/12/05 01:59:36 INFO spark.CacheTrackerActor: Cache entry added: (2, 173) on ip-172-31-25-138.ec2.internal (size added: 16.0B, available: 6.1GB)
14/12/05 01:59:36 INFO spark.SimpleJob: Finished TID 172 (progress: 176/177)
14/12/05 01:59:36 INFO spark.MesosScheduler: Completed ResultTask(0, 173)
14/12/05 01:59:36 INFO spark.CacheTrackerActor: Cache entry added: (2, 174) on ip-172-31-25-139.ec2.internal (size added: 178.3MB, available: 4.8GB)
14/12/05 01:59:36 INFO spark.SimpleJob: Finished TID 173 (progress: 177/177)
14/12/05 01:59:36 INFO spark.MesosScheduler: Completed ResultTask(0, 174)
14/12/05 01:59:36 INFO spark.SparkContext: Job finished in 130.727017576 s
res0: Long = 122352588
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　4. 好，现在我们再次执行enPages.count，看看是不是有神马神奇的事情发生了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; enPages.count
.
.
.
14/12/05 02:12:01 INFO spark.SparkContext: Job finished in 2.492567199 s
res2: Long = 122352588
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　
　　哇，130秒和2.5秒的对决，心算一下，52倍啊，如果visualize一下这个数据，估计会更让人吃惊吧。擅于YY的我不禁用echarts画了个图，感受一下内存计算的神速。画图代码如下，直接把代码粘贴到&lt;a href=&quot;http://echarts.baidu.com/doc/example/bar1.html#macarons&quot;&gt;echarts bar&lt;/a&gt;，在再点击刷新就可以看到图了。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;&quot;&gt;option = {
    title : {
        text: &#39;enPages.count&#39;,
        subtext: &#39;by taotao.li&#39;
    },
    tooltip : {
        trigger: &#39;axis&#39;
    },
    legend: {
        data:[&#39;no cache&#39;,&#39;cache&#39;]
    },
    toolbox: {
        show : true,
        feature : {
            mark : {show: true},
            dataView : {show: true, readOnly: false},
            magicType : {show: true, type: [&#39;line&#39;, &#39;bar&#39;]},
            restore : {show: true},
            saveAsImage : {show: true}
        }
    },
    calculable : true,
    xAxis : [
        {
            type : &#39;category&#39;,
            data : [&#39;one time&#39;]
        }
    ],
    yAxis : [
        {
            type : &#39;value&#39;
        }
    ],
    series : [
        {
            name:&#39;no cache&#39;,
            type:&#39;bar&#39;,
            data:[130.727017576]
        },
        {
            name:&#39;cache&#39;,
            type:&#39;bar&#39;,
            data:[2.492567199]
        }
    ]
};
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&quot;../../images/enPages-pic.png&quot; alt=&quot;enPages-pic&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;4.3 分析wikipedia的每日PV&lt;/h3&gt;

&lt;p&gt;　　重新温习一下&lt;a href=&quot;../using-amazon-aws-1&quot;&gt;上一篇末尾&lt;/a&gt;分析的wiki流量数据的格式如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;date_time: 以YYYYMMDD-HHMMSS格式表示的访问时间，且以小时为单位；&lt;/li&gt;
&lt;li&gt;project_code：表示对应的页面所使用的语言；&lt;/li&gt;
&lt;li&gt;page_title：表示访问的wiki标题；&lt;/li&gt;
&lt;li&gt;num_hits：表示从date_time起一小时内的浏览量；&lt;/li&gt;
&lt;li&gt;page_size： 表示以字节为单位，这个页面的大小；&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;　　OK，现在我们如果想要分析wiki流量的日PV，在上面5个字段中应该最关注的是date_time和num_hits吧。所以这里我们针对每一行数据创建一个key-value对，其中key是date_time，value是num_hits，在相加上相同的key对应的value就可以了。 下面我们把这些步骤拆开，一步一步分析，其中有些输出我就省略了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; val enTuples = enPages.map(line =&amp;gt; line.split(&quot; &quot;))
enTuples: spark.RDD[Array[java.lang.String]] = spark.MappedRDD@34ba89c5

scala&amp;gt; enTuples.take(5)
.
.
.
14/12/08 03:05:26 INFO spark.SparkContext: Job finished in 7.956625757 s
res3: Array[Array[java.lang.String]] = Array(Array(20090505-000000, en, !, 4, 170494), Array(20090505-000000, en, !!!, 21, 306957), Array(20090505-000000, en, !!!Fuck_You!!!, 9, 87025), Array(20090505-000000, en, !!!Fuck_You!!!_And_Then_Some, 2, 18249), Array(20090505-000000, en, !!!Fuck_You!!!_and_Then_Some, 2, 17960))


scala&amp;gt; val enKeyValuePairs = enTuples.map(line =&amp;gt; (line(0).substring(0, 8), line(3).toInt))
enKeyValuePairs: spark.RDD[(java.lang.String, Int)] = spark.MappedRDD@5e62a8d2

scala&amp;gt; enKeyValuePairs.take(5).foreach(println)
.
.
.
14/12/08 03:07:58 INFO spark.SparkContext: Job finished in 0.001414429 s
(20090505,4)
(20090505,21)
(20090505,9)
(20090505,2)
(20090505,2)

scala&amp;gt; val dailyPv = enKeyValuePairs.reduceByKey(_+_, 1)
dailyPv: spark.RDD[(java.lang.String, Int)] = spark.ShuffledRDD@50a934ec

scala&amp;gt; dailyPv.take(5).foreach(println)
.
.
.
14/12/08 03:13:18 INFO spark.SparkContext: Job finished in 26.776559986 s
(20090506,204190442)
(20090507,202617618)
(20090505,207698578)

scala&amp;gt; dailyPv.collect
.
.
.
14/12/08 03:13:45 INFO spark.SparkContext: Job finished in 0.145675681 s
res8: Array[(java.lang.String, Int)] = Array((20090506,204190442), (20090507,202617618), (20090505,207698578))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　最后的collect方法会把RDD 转换成scala里的数组。take(n)方法是取出前n条，因为这里我们就分析3天的数据，所以最多也只能取钱3天的，这里take(5)是看看这样会不会有什么错误提示呢。&lt;br/&gt;
　　上面我们大概用了3-4行语句来完成这个统计，这已经很强大了。而spark更强大的地方是它提供的编程模型，即transformation和action，虽然这些行为也就寥寥数十个，但已经足够处理大多数常见的问题了。比如说上面这个统计日PV的查询，在spark里其实完全可以把上面3-4行语句组合成一行语句，也就是说，在spark里，只有一行语句就可以统计当前wiki数据集下的日PV了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; enPages.map(line =&amp;gt; line.split(&quot; &quot;)).map(line =&amp;gt; (line(0).substring(0,8), line(3).toInt)).reduceByKey(_+_, 1).collect
.
.
.
14/12/08 03:25:03 INFO spark.SparkContext: Job finished in 27.144072883 s
res12: Array[(java.lang.String, Int)] = Array((20090506,204190442), (20090507,202617618), (20090505,207698578))
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;&lt;p&gt;　　&lt;strong&gt;可是老湿，你上面不是说只用一行语句就可以统计当前wiki数据集下的日PV的吗？可你这里用的是enPages啊！enPages不也是结果转换的吗，得把前几句加上吧？老湿，你骗我！！！&lt;/strong&gt;&lt;br/&gt;
&lt;img src=&quot;../../images/wawawa.gif&quot; alt=&quot;wawawa&quot; /&gt;&lt;/p&gt;

&lt;p&gt;　　&lt;strong&gt;同学，你问这个问题是不是刚才又写情书去了？既然enPages也是由其他RDD转换而来的，那这里不也可以把enPages替换成其他的RDD与与对应的transformation吗？&lt;/strong&gt;&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;../../images/laoshi.gif&quot; alt=&quot;laoshi&quot; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; pagecounts.filter(_.split(&quot; &quot;)(1) == &quot;en&quot;).map(line =&amp;gt; line.split(&quot; &quot;)).map(line =&amp;gt; (line(0).substring(0,8), line(3).toInt)).reduceByKey(_+_, 1).collect
 .
 .
 .
 14/12/08 04:33:52 INFO spark.SparkContext: Job finished in 151.78660518 s
res14: Array[(java.lang.String, Int)] = Array((20090506,204190442), (20090507,202617618), (20090505,207698578))
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;4.4 做点有趣的事情，看看哪些网页浏览次数最多&lt;/h3&gt;

&lt;p&gt;　　OK，其实分析每日PV已经是一个很有用的分析案例了。特别是长时间段的，比如说一周，一月，一季等，这些数据会让公司在容灾容错方面有很大启发。同样有用的是分析热点数据，即哪些页面是用户最常访问的，这个在缓存系统建立方面是绝对的关键啊。想一想，要是你把一个用户很少访问的页面放到缓存系统里，是不是既浪费了昂贵的缓存空间，又费力不讨好，简直是事倍功半啊。所以，现在我们就来做一件事，根据wiki这几日的流量数据，分析一下用户最常访问的wiki页面。&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;　　&lt;strong&gt;要不，我们先预测一下。我个人觉得，怎么说至少也应该有主页，帮助页面吧。&lt;/strong&gt;&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;　　当然，首先还是需要继续温习一下数据流量的格式啊：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;date_time: 以YYYYMMDD-HHMMSS格式表示的访问时间，且以小时为单位；&lt;/li&gt;
&lt;li&gt;project_code：表示对应的页面所使用的语言；&lt;/li&gt;
&lt;li&gt;page_title：表示访问的wiki标题；&lt;/li&gt;
&lt;li&gt;num_hits：表示从date_time起一小时内的浏览量；&lt;/li&gt;
&lt;li&gt;page_size： 表示以字节为单位，这个页面的大小；&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;　　既然我们要找到最常访问的热点数据，那就应该关注page_title和num_hits了。so，用分析日PV同样的思路，我们来分析一下热点数据：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; enPages.take(5)
.
.
.
14/12/08 04:59:48 INFO spark.SparkContext: Job finished in 9.6166E-4 s
res15: Array[String] = Array(20090505-000000 en ! 4 170494, 20090505-000000 en !!! 21 306957, 20090505-000000 en !!!Fuck_You!!! 9 87025, 20090505-000000 en !!!Fuck_You!!!_And_Then_Some 2 18249, 20090505-000000 en !!!Fuck_You!!!_and_Then_Some 2 17960)

scala&amp;gt; val enPageArray = enPages.map( l=&amp;gt;l.split(&quot; &quot;))
enPageArray: spark.RDD[Array[java.lang.String]] = spark.MappedRDD@27174693

scala&amp;gt; enPageArray.take(5)
.
.
.
14/12/08 05:02:28 INFO spark.SparkContext: Job finished in 0.001180471 s
res16: Array[Array[java.lang.String]] = Array(Array(20090505-000000, en, !, 4, 170494), Array(20090505-000000, en, !!!, 21, 306957), Array(20090505-000000, en, !!!Fuck_You!!!, 9, 87025), Array(20090505-000000, en, !!!Fuck_You!!!_And_Then_Some, 2, 18249), Array(20090505-000000, en, !!!Fuck_You!!!_and_Then_Some, 2, 17960))

scala&amp;gt; val enPageKeyValue = enPageArray.map(l =&amp;gt;(l(2), l(3).toInt))
enPageKeyValue: spark.RDD[(java.lang.String, Int)] = spark.MappedRDD@5b68b32

scala&amp;gt; enPageKeyValue.take(5)
.
.
.
14/12/08 05:03:54 INFO spark.SparkContext: Job finished in 0.00113825 s
res17: Array[(java.lang.String, Int)] = Array((!,4), (!!!,21), (!!!Fuck_You!!!,9), (!!!Fuck_You!!!_And_Then_Some,2), (!!!Fuck_You!!!_and_Then_Some,2))

scala&amp;gt; val keyValueUnion = enPageKeyValue.reduceByKey(_+_, 40)
keyValueUnion: spark.RDD[(java.lang.String, Int)] = spark.ShuffledRDD@7843f53

scala&amp;gt; keyValueUnion.take(5).foreach(println)
.
.
.
14/12/08 05:17:35 INFO spark.SparkContext: Job finished in 98.416456363 s
(Einst%C3%83%C2%BCrzende_Neubauten,2)
(Maxemail,1)
(Michael_Carl,4)
(Boothe_Homestead,1)
(File:The_Photographer.jpg,20)

scala&amp;gt; val valueKey = keyValueUnion.map(x=&amp;gt;(x._2, x._1))
valueKey: spark.RDD[(Int, java.lang.String)] = spark.MappedRDD@47a82a6a

scala&amp;gt; valueKey.take(5).foreach(println)

14/12/08 05:19:42 INFO spark.SparkContext: Job finished in 4.196323691 s
(2,Einst%C3%83%C2%BCrzende_Neubauten)
(4,Michael_Carl)
(1,Maxemail)
(1,Boothe_Homestead)
(20,File:The_Photographer.jpg)

scala&amp;gt; valueKey.sortByKey(false).take(5).foreach(println)
.
.
.
14/12/08 05:21:59 INFO spark.SparkContext: Job finished in 41.025906283 s
(43822489,404_error/)
(18730347,Main_Page)
(17657352,Special:Search)
(5816953,Special:Random)
(3521336,Special:Randompage)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　思路依然和分析每日PV是一样的，当然也可以组织成一行语句：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;val hotPage = enPages.map(l =&amp;gt; l.split(&quot; &quot;)).map(l =&amp;gt; (l(2), l(3).toInt)).reduceByKey(_+_, 40).map(x =&amp;gt; (x._2, x._1)).sortByKey(false).take(10).foreach(println)
.
.
.
14/12/08 09:57:30 INFO spark.SparkContext: Job finished in 41.232081196 s
(43822489,404_error/)
(18730347,Main_Page)
(17657352,Special:Search)
(5816953,Special:Random)
(3521336,Special:Randompage)
(695817,Cinco_de_Mayo)
(534253,Swine_influenza)
(464935,Wiki)
(396776,Dom_DeLuise)
(382510,Deadpool_(comics))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　好，这篇我们就先实践到这里。接下来体会一下shark的power，有兴趣的同志请移步&lt;a href=&quot;../using-amazon-aws-3-shark&quot;&gt;Spark Shark使用&lt;/a&gt;。&lt;/p&gt;
</content>
   </entry>
   
   <entry>
     <title>［touch spark］2. Amazon AWS EC2 入门</title>
     <link href="/using-amazon-aws-1"/>
     <updated>2014-12-01T00:00:00+08:00</updated>
     <id>/using-amazon-aws-1</id>
     <content type="html">&lt;h2&gt;&lt;/h2&gt;

&lt;h2&gt;1. 申请Amazon AWS账号&lt;/h2&gt;

&lt;p&gt;　　申请Amazon AWS需要绑定信用卡，无奈兄弟我从来没用过信用卡，所以只能跑到&lt;a href=&quot;https://www.globalcash.hk/&quot;&gt;global cash&lt;/a&gt;申请一张虚拟信用卡了。有关申请虚拟信用卡的教程&lt;a href=&quot;http://www.freehao123.com/globalcash/&quot;&gt;这里&lt;/a&gt;已经有了，我就不重复了。&lt;/p&gt;

&lt;h2&gt;2. 在EC2上创建一个spark集群&lt;/h2&gt;

&lt;h3&gt;2.1 前期准备&lt;/h3&gt;

&lt;p&gt;　　本文中用到的所有脚本都是基于python 2.x写的，且在Linux和0S X上测试通过。&lt;/p&gt;

&lt;h3&gt;2.2 创建EC2 keys&lt;/h3&gt;

&lt;p&gt;　　首先确保你的地区是US EAST，在右上角可以选择区域，即帐号名右侧。还没找到的请看下图：&lt;br/&gt;
&lt;img src=&quot;../../images/choose_ec2_region.png&quot; alt=&quot;choose_ec2_region&quot; /&gt;&lt;/p&gt;

&lt;p&gt;　　然后在帐号名-&gt;Security Credentials-&gt;Dashboard 下的 Details-&gt;Security Status-&gt;Manage Security Credentials-&gt;Access Keys-&gt;Create New Access Key创建keys，这里最好把keys记录下来，以后好用。&lt;br/&gt;
　　设置变量，下面的KEY_ID, ACCESS_KEY是在你创建keys的时候产生的：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;&quot;&gt;export AWS_ACCESS_KEY_ID=&amp;lt;ACCESS_KEY_ID&amp;gt;
export AWS_SECRET_ACCESS_KEY=&amp;lt;SECRET_ACCESS_KEY&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;2.3 创建key pair&lt;/h3&gt;

&lt;p&gt;　　在EC2 Dashboard左侧边栏-&gt;Network &amp;amp; Security-&gt;Key Pairs-&gt;Create Key Pair。这里会需要你输入一个key pair name，最好搞一个简单好记的，因为以后也会用到。创建成功后会自动下载一个用于后期验证登录的文件，下载该文件把其复制到用户家目录下，确保其权限至少是600，保险起见执行 chmod 600 key_pair_file。&lt;/p&gt;

&lt;h3&gt;2.4 下载启动脚本&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;&quot;&gt;git clone git://github.com/amplab/ampcamp.git  
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;2.5 建立并启动集群&lt;/h3&gt;

&lt;p&gt;　　若上面的启动脚本下载成功后，本地会有一个ampcamp的文件夹，cd 到ampcamp文件夹里，执行下面命令启动集群。其中key_file是刚刚下载并复制到家目录下的验证文件，name_of_key_pair是你创建key_pair的时候自己命名的。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;&quot;&gt;./spark-ec2 -i &amp;lt;key_file&amp;gt; -k &amp;lt;name_of_key_pair&amp;gt; --copy launch ampcamp
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　上面这个过程大约会持续15-20分钟，耐心等待一下。如果期间出现下面这个问题，那是因为没有把key_pair文件复制到家目录下去。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;rsync: connection unexpectedly closed (0 bytes received so far) [sender]
rsync error: unexplained error (code 255) at io.c(605) [sender=3.0.9]
Traceback (most recent call last):
  File &quot;./spark_ec2.py&quot;, line 759, in &amp;lt;module&amp;gt;
    main()
  File &quot;./spark_ec2.py&quot;, line 648, in main
    setup_cluster(conn, master_nodes, slave_nodes, zoo_nodes, opts, True)
  File &quot;./spark_ec2.py&quot;, line 363, in setup_cluster
    deploy_files(conn, &quot;deploy.generic&quot;, opts, master_nodes, slave_nodes, zoo_nodes)
  File &quot;./spark_ec2.py&quot;, line 604, in deploy_files
    subprocess.check_call(command, shell=True)
  File &quot;/root/anaconda/lib/python2.7/subprocess.py&quot;, line 540, in check_call
    raise CalledProcessError(retcode, cmd)
subprocess.CalledProcessError: Command &#39;rsync -rv -e &#39;ssh -o StrictHostKeyChecking=no -i ../company.pem&#39; &#39;/tmp/tmp6YpLzV/&#39; &#39;root@ec2-54-172-219-206.compute-1.amazonaws.com:/&#39;&#39; returned non-zero exit status 255
root@ubuntu2:~/Desktop/spark/ampcamp# cp ../company.pem .
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　如果一切顺利（但愿），最后会有消息提示创建成功：SUCCESS: Cluster successfully launched! You can login to the master at ***&lt;/p&gt;

&lt;h3&gt;2.6 其他相关命令&lt;/h3&gt;

&lt;p&gt;　　第一个命令获取ampcamp集群的master节点，这个需要在集群启动成功后执行一次，因为后续也要用到这个节点地址，所以最好把master 节点地址记录下来。第二个命令是删除集群。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;&quot;&gt;./spark-ec2 -i &amp;lt;key_file&amp;gt; -k &amp;lt;key_pair&amp;gt; get-master ampcamp   
./spark-ec2 -i &amp;lt;key_file&amp;gt; -k &amp;lt;key_pair&amp;gt; destroy ampcamp  
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;3. 查看集群设置和数据准备&lt;/h2&gt;

&lt;h3&gt;3.1 获取master节点地址&lt;/h3&gt;

&lt;p&gt;　　在这个练习中，我们会用从&lt;a href=&quot;http://aws.amazon.com/datasets/4182&quot;&gt;http://aws.amazon.com/datasets/4182&lt;/a&gt;拿到的wikipedia的流量数据来做分析。&lt;br/&gt;
　　方便起见，AMP Camp已经提前把(May 5 to May 7, 2009; roughly 20G and 329 million entries)的数据准备好，并且预加载到集群里一个HDFS机器上了。这样我们就不用准备数据了，可以专注在体验spark特性的这件事上。&lt;/p&gt;

&lt;h3&gt;3.1 获取master节点地址&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;&quot;&gt;./spark-ec2 -i &amp;lt;key_file&amp;gt; -k &amp;lt;key_pair&amp;gt; get-master ampcamp  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　此时成功的话应该会提示你当前有一个master，3个slave，0个ZooKeeper。&lt;/p&gt;

&lt;h3&gt;3.2 使用ssh登录master节点&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&quot;&quot;&gt;ssh -i &amp;lt;key_file&amp;gt; -l root &amp;lt;master_node_hostname&amp;gt;
or
ssh -i &amp;lt;key_file&amp;gt; root &amp;lt;master_node_hostname&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　需要注意的是，这里虽然你是登录到一个机器上，但实际是一个集群中。集群里有一个master节点，3个slave节点。其中你登录的地方是master节点，master节点主要负责任务分配和管理HDFS的元数据。其他的3个slave节点是计算节点，也就是真正运行任务的节点。&lt;br/&gt;
　　在master里，执行ls可以看到以下几个文件夹，下面列出比较重要的几个文件夹：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ephemeral-hdfs: Hadoop installation&lt;/li&gt;
&lt;li&gt;hive: Hive installation&lt;/li&gt;
&lt;li&gt;java-app-template: Some stand-alone Spark programs in java&lt;/li&gt;
&lt;li&gt;mesos: Mesos installation&lt;/li&gt;
&lt;li&gt;mesos-ec2: A suite of scripts to manage Mesos on EC2&lt;/li&gt;
&lt;li&gt;scala-2.9.1.final: Scala installation&lt;/li&gt;
&lt;li&gt;scala-app-template: Some stand-alone Spark programs in scala&lt;/li&gt;
&lt;li&gt;spark: Spark installation&lt;/li&gt;
&lt;li&gt;shark: Shark installation&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;　　可以在mesos-ec2/slaves文件里看到自己的3个slave节点地址：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[root@ip-172-31-22-240 ~]# cat mesos-ec2/slaves
ec2-54-174-175-127.compute-1.amazonaws.com
ec2-54-174-183-88.compute-1.amazonaws.com
ec2-54-174-124-52.compute-1.amazonaws.com
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　你的HDFS集群应该已经提前载入20GB的wikipedia数据文件了，可以到ephemeral-hdfs/bin/下执行hadoop fs -ls /wiki/pagecounts查看，这里应该是有74个文件，其中2个是空的。其中每一个文件是以小时为单位来保存的。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;&quot;&gt;[root@ip-172-31-22-240 ~]# ephemeral-hdfs/bin/hadoop fs -ls /wiki/pagecounts
Found 74 items
-rw-r--r--   3 root supergroup          0 2014-12-03 02:18 /wiki/pagecounts/part-00095
-rw-r--r--   3 root supergroup  244236879 2014-12-03 02:18 /wiki/pagecounts/part-00096
-rw-r--r--   3 root supergroup  233905016 2014-12-03 02:18 /wiki/pagecounts/part-00097
-rw-r--r--   3 root supergroup  225825888 2014-12-03 02:19 /wiki/pagecounts/part-00098
-rw-r--r--   3 root supergroup  225164279 2014-12-03 02:18 /wiki/pagecounts/part-00099
-rw-r--r--   3 root supergroup  228145848 2014-12-03 02:19 /wiki/pagecounts/part-00100
.            
.
.
-rw-r--r--   3 root supergroup  327382691 2014-12-03 02:26 /wiki/pagecounts/part-00163
-rw-r--r--   3 root supergroup  325471268 2014-12-03 02:27 /wiki/pagecounts/part-00164
-rw-r--r--   3 root supergroup  288288841 2014-12-03 02:27 /wiki/pagecounts/part-00165
-rw-r--r--   3 root supergroup  266179174 2014-12-03 02:29 /wiki/pagecounts/part-00166
-rw-r--r--   3 root supergroup  243451716 2014-12-03 02:18 /wiki/pagecounts/part-00167
-rw-r--r--   3 root supergroup          0 2014-12-03 02:19 /wiki/pagecounts/part-00168
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　其中，每个文件都以一行为单位记录，每行都符合模式：&lt;code&gt;&amp;lt;date_time&amp;gt; &amp;lt;project_code&amp;gt; &amp;lt;page_title&amp;gt; &amp;lt;num_hits&amp;gt; &amp;lt;page_size&amp;gt;&lt;/code&gt;。其中&lt;code&gt;&amp;lt;date_time&amp;gt;&lt;/code&gt;字段以YYYYMMDD-HHMMSS为时间格式，表示访问时间，且以小时为单位，所以只有YYYYMMDD-HH为有效数据，MMSS都为0，&lt;code&gt;&amp;lt;project_code&amp;gt;&lt;/code&gt;字段表示对应的页面所使用的语言，如&quot;en&quot;则表示英文；&lt;code&gt;&amp;lt;page_title&amp;gt;&lt;/code&gt;字段表示该页面在wiki上的标题，&lt;code&gt;&amp;lt;num_hits&amp;gt;&lt;/code&gt;表示从&lt;code&gt;&amp;lt;date_time&amp;gt;&lt;/code&gt;时间起一小时内的浏览量，&lt;code&gt;&amp;lt;page_size&amp;gt;&lt;/code&gt;表示以字节为单位，这个页面的大小。&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;&quot;&gt;20090507-040000 aa Main_Page 7 51309
20090507-040000 aa Special:Boardvote 1 11631
20090507-040000 aa Special:Imagelist 1 931
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　下一篇会记录在EC2上用spark分析wikipedia流量的过程，请移步&lt;a href=&quot;../using-amazon-aws-2&quot;&gt;使用Spark分析wikipedia流量数据&lt;/a&gt;&lt;/p&gt;
</content>
   </entry>
   

</feed>


</body>
</html>
