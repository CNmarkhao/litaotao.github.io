<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8" />
  <title>Taotao's Zone</title>
  <meta name="renderer" content="webkit">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="mobile-web-app-capable" content="yes">
  <link rel="stylesheet" href="/css/font-awesome/css/font-awesome.min.css" type="text/css" />
  <link rel="stylesheet" href="/css/default.css" type="text/css" />
  <link rel="stylesheet" href="/css/desktop.css" type="text/css" />
  <link rel="stylesheet" href="/css/mobile.css" type="text/css" />
  <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
  <link rel="alternate" type="application/atom+xml" title="Recent Entries" href="/atom.xml" />
  <script src="/js/jquery-1.11.0.min.js" type="text/javascript"></script>
  <script src="/js/jquery-migrate-1.2.1.js" type="text/javascript"></script>
  <script src="/js/jquery.transit.min.js" type="text/javascript"></script>
  <script src="/js/common.js" type="text/javascript"></script>
</head>
<body>
  <?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

   <title>Taotao's Zone</title>
   <link href="http://litaotao.github.io/atom.xml" rel="self" type="application/atom+xml"/>
   <link href="http://litaotao.github.io" rel="alternate" type="text/html" />
   <updated>2015-03-12T10:21:35+08:00</updated>
   <id>http://litaotao.github.io</id>
   <author>
     <name></name>
     <email></email>
   </author>

   
   <entry>
     <title>book-3. 设计模式总结之创建型模式</title>
     <link href="/design-pattern-create-pattern"/>
     <updated>2015-03-11T00:00:00+08:00</updated>
     <id>/design-pattern-create-pattern</id>
     <content type="html">&lt;h2&gt;&lt;/h2&gt;

&lt;h2&gt;1. 抽象工厂模式&lt;/h2&gt;

&lt;p&gt;　　抽象工厂模式（英语：Abstract factory pattern）是一种软件开发设计模式。抽象工厂模式提供了一种方式，可以将一组具有同一主题的单独的工厂封装起来。在正常使用中，客户端程序需要创建抽象工厂的具体实现，然后使用抽象工厂作为接口来创建这一主题的具体对象。客户端程序不需要知道（或关心）它从这些内部的工厂方法中获得对象的具体类型，因为客户端程序仅使用这些对象的通用接口。抽象工厂模式将一组对象的实现细节与他们的一般使用分离开来。&lt;/p&gt;

&lt;p&gt;　　举个例子来说，比如一个抽象工厂类叫做DocumentCreator（文档创建器），此类提供创建若干种产品的接口，包括createLetter()（创建邮件）和createResume()（创建简历）。其中，createLetter()返回一个Letter（邮件），createResume()返回一个Resume（简历）。系统中还有一些DocumentCreator的具体实现类，包括FancyDocumentCreator和ModernDocumentCreator。这两个类对DocumentCreator的两个方法分别有不同的实现，用来创建不同的“邮件”和“简历”（用FancyDocumentCreator的实例可以创建FancyLetter和FancyResume，用ModernDocumentCreator的实例可以创建ModernLetter和ModernResume）。这些具体的“邮件”和“简历”类均继承自抽象类，即Letter和Resume类。客户端需要创建“邮件”或“简历”时，先要得到一个合适的DocumentCreator实例，然后调用它的方法。一个工厂中创建的每个对象都是同一个主题的（“fancy”或者“modern”）。客户端程序只需要知道得到的对象是“邮件”或者“简历”，而不需要知道具体的主题，因此客户端程序从抽象工厂DocumentCreator中得到了Letter或Resume类的引用，而不是具体类的对象引用。&lt;/p&gt;

&lt;p&gt;　　“工厂”是创建产品（对象）的地方，其目的是将产品的创建与产品的使用分离。抽象工厂模式的目的，是将若干抽象产品的接口与不同主题产品的具体实现分离开。这样就能在增加新的具体工厂的时候，不用修改引用抽象工厂的客户端代码。&lt;/p&gt;

&lt;p&gt;　　使用抽象工厂模式，能够在具体工厂变化的时候，不用修改使用工厂的客户端代码，甚至是在运行时。然而，使用这种模式或者相似的设计模式，可能给编写代码带来不必要的复杂性和额外的工作。正确使用设计模式能够抵消这样的“额外工作”。&lt;/p&gt;

&lt;h3&gt;1.1 Python 源码示例&lt;/h3&gt;

&lt;p&gt;　　源码来自&lt;a href=&quot;https://github.com/faif/python-patterns/&quot;&gt;github:python-patterns&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/usr/bin/env python
# -*- coding: utf-8 -*-

# http://ginstrom.com/scribbles/2007/10/08/design-patterns-python-style/

&quot;&quot;&quot;Implementation of the abstract factory pattern&quot;&quot;&quot;

import random

class PetShop:

    &quot;&quot;&quot;A pet shop&quot;&quot;&quot;

    def __init__(self, animal_factory=None):
        &quot;&quot;&quot;pet_factory is our abstract factory.  We can set it at will.&quot;&quot;&quot;

        self.pet_factory = animal_factory

    def show_pet(self):
        &quot;&quot;&quot;Creates and shows a pet using the abstract factory&quot;&quot;&quot;

        pet = self.pet_factory.get_pet()
        print(&quot;We have a lovely {}&quot;.format(pet))
        print(&quot;It says {}&quot;.format(pet.speak()))
        print(&quot;We also have {}&quot;.format(self.pet_factory.get_food()))


# Stuff that our factory makes

class Dog:

    def speak(self):
        return &quot;woof&quot;

    def __str__(self):
        return &quot;Dog&quot;


class Cat:

    def speak(self):
        return &quot;meow&quot;

    def __str__(self):
        return &quot;Cat&quot;


# Factory classes

class DogFactory:

    def get_pet(self):
        return Dog()

    def get_food(self):
        return &quot;dog food&quot;


class CatFactory:

    def get_pet(self):
        return Cat()

    def get_food(self):
        return &quot;cat food&quot;

# Create the proper family
def get_factory():
    &quot;&quot;&quot;Let&#39;s be dynamic!&quot;&quot;&quot;
    return random.choice([DogFactory, CatFactory])()


# Show pets with various factories
if __name__ == &quot;__main__&quot;:
    for i in range(3):
        shop = PetShop(get_factory())
        shop.show_pet()
        print(&quot;=&quot; * 20)

### OUTPUT ###
# We have a lovely Dog
# It says woof
# We also have dog food
# ====================
# We have a lovely Dog
# It says woof
# We also have dog food
# ====================
# We have a lovely Cat
# It says meow
# We also have cat food
# ====================
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;2.工厂方法模式&lt;/h2&gt;

&lt;p&gt;　　工厂方法模式（英语：Factory method pattern）是一种实现了“工厂”概念的面向对象设计模式。就像其他创建型模式一样，它也是处理在不指定对象具体类型的情况下创建对象的问题。工厂方法模式的实质是“定义一个创建对象的接口，但让实现这个接口的类来决定实例化哪个类。工厂方法让类的实例化推迟到子类中进行。”&lt;/p&gt;

&lt;p&gt;　　创建一个对象常常需要复杂的过程，所以不适合包含在一个复合对象中。创建对象可能会导致大量的重复代码，可能会需要复合对象访问不到的信息，也可能提供不了足够级别的抽象，还可能并不是复合对象概念的一部分。工厂方法模式通过定义一个单独的创建对象的方法来解决这些问题。由子类实现这个方法来创建具体类型的对象。&lt;/p&gt;

&lt;p&gt;　　对象创建中的有些过程包括决定创建哪个对象、管理对象的生命周期，以及管理特定对象的创建和销毁的概念。&lt;/p&gt;

&lt;p&gt;　　如果抛开设计模式的范畴，“工厂方法”这个词也可以指作为“工厂”的方法，这个方法的主要目的就是创建对象，而这个方法不一定在单独的工厂类中。这些方法通常作为静态方法，定义在方法所实例化的类中。&lt;/p&gt;

&lt;p&gt;　　每个工厂方法都有特定的名称。在许多面向对象的编程语言中，构造方法必须和它所在的类具有相同的名称，这样的话，如果有多种创建对象的方式（重载）就可能导致歧义。工厂方法没有这种限制，所以可以具有描述性的名称。举例来说，根据两个实数创建一个复数，而这两个实数表示直角坐标或极坐标，如果使用工厂方法，方法的含义就非常清晰了。当工厂方法起到这种消除歧义的作用时，构造方法常常被设置为私有方法，从而强制客户端代码使用工厂方法创建对象。&lt;/p&gt;

&lt;h3&gt;2.1 Python 源码示例&lt;/h3&gt;

&lt;p&gt;　　源码来自&lt;a href=&quot;https://github.com/faif/python-patterns/&quot;&gt;github:python-patterns&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/usr/bin/env python
# -*- coding: utf-8 -*-

&quot;&quot;&quot;http://ginstrom.com/scribbles/2007/10/08/design-patterns-python-style/&quot;&quot;&quot;


class GreekGetter:

    &quot;&quot;&quot;A simple localizer a la gettext&quot;&quot;&quot;

    def __init__(self):
        self.trans = dict(dog=&quot;σκύλος&quot;, cat=&quot;γάτα&quot;)

    def get(self, msgid):
        &quot;&quot;&quot;We&#39;ll punt if we don&#39;t have a translation&quot;&quot;&quot;
        try:
            return self.trans[msgid]
        except KeyError:
            return str(msgid)


class EnglishGetter:

    &quot;&quot;&quot;Simply echoes the msg ids&quot;&quot;&quot;

    def get(self, msgid):
        return str(msgid)


def get_localizer(language=&quot;English&quot;):
    &quot;&quot;&quot;The factory method&quot;&quot;&quot;
    languages = dict(English=EnglishGetter, Greek=GreekGetter)
    return languages[language]()

# Create our localizers
e, g = get_localizer(language=&quot;English&quot;), get_localizer(language=&quot;Greek&quot;)
# Localize some text
for msgid in &quot;dog parrot cat bear&quot;.split():
    print(e.get(msgid), g.get(msgid))

### OUTPUT ###
# dog σκύλος
# parrot parrot
# cat γάτα
# bear bear
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;3. 单例模式&lt;/h2&gt;

&lt;p&gt;　　单例模式，也叫单子模式，是一种常用的软件设计模式。在应用这个模式时，单例对象的类必须保证只有一个实例存在。许多时候整个系统只需要拥有一个的全局对象，这样有利于我们协调系统整体的行为。比如在某个服务器程序中，该服务器的配置信息存放在一个文件中，这些配置数据由一个单例对象统一读取，然后服务进程中的其他对象再通过这个单例对象获取这些配置信息。这种方式简化了在复杂环境下的配置管理。&lt;/p&gt;

&lt;p&gt;　　实现单例模式的思路是：一个类能返回对象一个引用(永远是同一个)和一个获得该实例的方法（必须是静态方法，通常使用getInstance这个名称）；当我们调用这个方法时，如果类持有的引用不为空就返回这个引用，如果类保持的引用为空就创建该类的实例并将实例的引用赋予该类保持的引用；同时我们还将该类的构造函数定义为私有方法，这样其他处的代码就无法通过调用该类的构造函数来实例化该类的对象，只有通过该类提供的静态方法来得到该类的唯一实例。&lt;/p&gt;

&lt;p&gt;　　单例模式在多线程的应用场合下必须小心使用。如果当唯一实例尚未创建时，有两个线程同时调用创建方法，那么它们同时没有检测到唯一实例的存在，从而同时各自创建了一个实例，这样就有两个实例被构造出来，从而违反了单例模式中实例唯一的原则。 解决这个问题的办法是为指示类是否已经实例化的变量提供一个互斥锁(虽然这样会降低效率)。&lt;/p&gt;

&lt;h3&gt;3.1 Python源码示例&lt;/h3&gt;

&lt;p&gt;　　源码来自&lt;a href=&quot;http://blog.csdn.net/ghostfromheaven/article/details/7671853&quot;&gt;csdn.net&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#-*- encoding=utf-8 -*-
print &#39;----------------------方法1--------------------------&#39;
#方法1,实现__new__方法
#并在将一个类的实例绑定到类变量_instance上,
#如果cls._instance为None说明该类还没有实例化过,实例化该类,并返回
#如果cls._instance不为None,直接返回cls._instance
class Singleton(object):
    def __new__(cls, *args, **kw):
        if not hasattr(cls, &#39;_instance&#39;):
            orig = super(Singleton, cls)
            cls._instance = orig.__new__(cls, *args, **kw)
        return cls._instance

class MyClass(Singleton):
    a = 1

one = MyClass()
two = MyClass()

two.a = 3
print one.a
#3
#one和two完全相同,可以用id(), ==, is检测
print id(one)
#29097904
print id(two)
#29097904
print one == two
#True
print one is two
#True

print &#39;----------------------方法2--------------------------&#39;
#方法2,共享属性;所谓单例就是所有引用(实例、对象)拥有相同的状态(属性)和行为(方法)
#同一个类的所有实例天然拥有相同的行为(方法),
#只需要保证同一个类的所有实例具有相同的状态(属性)即可
#所有实例共享属性的最简单最直接的方法就是__dict__属性指向(引用)同一个字典(dict)
#可参看:http://code.activestate.com/recipes/66531/
class Borg(object):
    _state = {}
    def __new__(cls, *args, **kw):
        ob = super(Borg, cls).__new__(cls, *args, **kw)
        ob.__dict__ = cls._state
        return ob

class MyClass2(Borg):
    a = 1

one = MyClass2()
two = MyClass2()

#one和two是两个不同的对象,id, ==, is对比结果可看出
two.a = 3
print one.a
#3
print id(one)
#28873680
print id(two)
#28873712
print one == two
#False
print one is two
#False
#但是one和two具有相同的（同一个__dict__属性）,见:
print id(one.__dict__)
#30104000
print id(two.__dict__)
#30104000

print &#39;----------------------方法3--------------------------&#39;
#方法3:本质上是方法1的升级（或者说高级）版
#使用__metaclass__（元类）的高级python用法
class Singleton2(type):
    def __init__(cls, name, bases, dict):
        super(Singleton2, cls).__init__(name, bases, dict)
        cls._instance = None
    def __call__(cls, *args, **kw):
        if cls._instance is None:
            cls._instance = super(Singleton2, cls).__call__(*args, **kw)
        return cls._instance

class MyClass3(object):
    __metaclass__ = Singleton2

one = MyClass3()
two = MyClass3()

two.a = 3
print one.a
#3
print id(one)
#31495472
print id(two)
#31495472
print one == two
#True
print one is two
#True

print &#39;----------------------方法4--------------------------&#39;
#方法4:也是方法1的升级（高级）版本,
#使用装饰器(decorator),
#这是一种更pythonic,更elegant的方法,
#单例类本身根本不知道自己是单例的,因为他本身(自己的代码)并不是单例的
def singleton(cls, *args, **kw):
    instances = {}
    def _singleton():
        if cls not in instances:
            instances[cls] = cls(*args, **kw)
        return instances[cls]
    return _singleton

@singleton
class MyClass4(object):
    a = 1
    def __init__(self, x=0):
        self.x = x

one = MyClass4()
two = MyClass4()

two.a = 3
print one.a
#3
print id(one)
#29660784
print id(two)
#29660784
print one == two
#True
print one is two
#True
one.x = 1
print one.x
#1
print two.x
#1
&lt;/code&gt;&lt;/pre&gt;
</content>
   </entry>
   
   <entry>
     <title>book-3. 设计模式总结</title>
     <link href="/design-pattern"/>
     <updated>2015-03-09T00:00:00+08:00</updated>
     <id>/design-pattern</id>
     <content type="html">&lt;h2&gt;&lt;/h2&gt;

&lt;h2&gt;1. 设计模式概念&lt;/h2&gt;

&lt;p&gt;　　设计模式这个术语是由Erich Gamma等人在1990年代从建筑设计领域引入到计算机科学的。它是对软件设计中普遍存在（反复出现）的各种问题，所提出的解决方案。
　　设计模式并不直接用来完成代码的编写，而是描述在各种不同情况下，要怎么解决问题的一种方案。面向对象设计模式通常以类或对象来描述其中的关系和相互作用，但不涉及用来完成应用程序的特定类或对象。设计模式能使不稳定依赖于相对稳定、具体依赖于相对抽象，避免会引起麻烦的紧耦合，以增强软件设计面对并适应变化的能力。
　　并非所有的软件模式都是设计模式，设计模式特指软件“设计”层次上的问题。还有其它非设计模式的模式，如架构模式。同时，算法不能算是一种设计模式，因为算法主要是用来解决计算上的问题，而非设计上的问题。&lt;/p&gt;

&lt;h2&gt;2. 模式分类&lt;/h2&gt;

&lt;p&gt;　　《设计模式》一书把设计模式分为创建型、结构性、行为型三大模式。把它们通过授权，聚合，诊断的概念来描述。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;创建型模式：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;抽象工厂：为一个产品族提供了统一的创建接口，当需要这个产品族的某一系列的时候，可以从抽象工厂中选出相应的系列创建一个具体的工厂类。&lt;/li&gt;
&lt;li&gt;工厂方法：定义一个接口用于创建对象，但是让子类觉得初始化那个类。工厂方法把一个类的初始化下方到子类。&lt;/li&gt;
&lt;li&gt;生成器：讲一个复杂对象的构建与它的表示分离，使得同样的构建过程可以创建不同的表示。&lt;/li&gt;
&lt;li&gt;惰性初始：推迟对象的创建，数据的计算等需要耗费较多资源的操作，只有在第一次访问的时候才执行。&lt;/li&gt;
&lt;li&gt;对象池：通过回收利用对象避免获取和释放资源所需的昂贵成本。&lt;/li&gt;
&lt;li&gt;原型：用原型实例指定创建对象的种类，并且通过拷贝这些原型创建新的对象。&lt;/li&gt;
&lt;li&gt;单例：确保一个类只有一个实例，并提供对该实例的全局访问。&lt;/li&gt;
&lt;li&gt;多例：确保一个类只有命名的实例，并提供对这些实例的全局访问。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;结构型模式：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;适配器：将某个类的接口转化为客户端期望的另一个接口表示，适配器模式可以消除由于接口不匹配所造成的类兼容性问题。&lt;/li&gt;
&lt;li&gt;桥接：将一个抽象与实现解耦，以便两者可以独立地变化。&lt;/li&gt;
&lt;li&gt;组合：把多个对象组成树状结构来表示局部与整体，这样用户可以一样地对待单个对象和对象的组合。&lt;/li&gt;
&lt;li&gt;修饰：向某个对象动态地添加更多的功能，修饰模式是除类继承之外另一种扩展功能的方法。&lt;/li&gt;
&lt;li&gt;外观模式：为子系统的一组接口提供一个一致的界面，外观模式定义了一个高层接口，这个接口使得这一子系统更加容易使用。&lt;/li&gt;
&lt;li&gt;享元：通过共享以便有效地支持大量小颗粒对象。&lt;/li&gt;
&lt;li&gt;代理：为其他对象提供一个代理以控制这个对象的访问。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;行为型：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;责任链：为接触请求的发送者和接收者之间耦合，而使多个对象都有机会处理这个请求。将这些对象连成一条链，并沿着这条链传递该请求，直到有一个对象处理它。&lt;/li&gt;
&lt;li&gt;命令：将一个请求封装为一个对象，从而使你可用不同的请求对客户端进行参数化，对请求排队或者记录请求日志，以及支持可取消的操作。&lt;/li&gt;
&lt;li&gt;解释器：给定一个语言，定义它的文法的一种表示，并定义一个解释器，该解释器使用该表示来解释语言中的句子。&lt;/li&gt;
&lt;li&gt;迭代器：提供一种方法顺序访问一个聚合对象中的各个元素，而又不需要暴露该对象的内部表示。&lt;/li&gt;
&lt;li&gt;备忘录：备忘录对象是一个用来存储另外一个对象内部状态的快照的对象，备忘录模式的用意是在不破坏封装的条件下，讲一个对象的状态捉住，并外部化，存储起来，从而可以在将来合适的时候把这个对象还原到存储起来的状态。&lt;/li&gt;
&lt;li&gt;观察者：在对象间定义一个一对多的联系性，由此当一个对象改变了状态，所有其他相关的对象会被通知并且自动更新。&lt;/li&gt;
&lt;li&gt;策略：定义一个算法的系列，将其各个分装，并且使他们有交互性，策略模式使得算法在用户使用的时候能独立地改变。&lt;/li&gt;
&lt;li&gt;模板方法：模板方法模式准备一个抽象类，将部分逻辑以具体的方法及具体构造子类的形式实现，然后声明一些抽象方法来迫使子类实现剩余的逻辑。不同的子类可以以不同的方式实现这些抽象方法，从而对剩余的逻辑有不同的实现。先构建一个顶级逻辑框架，而将逻辑的细节留给具体的子类去实现。&lt;/li&gt;
&lt;li&gt;访问者：封装一些施加于某种数据结构元素之上的操作。一旦这些操作需要修改，接受这个操作的数据结构可以保持不变。访问者模式适用于数据结构相对未定的系统，它把数据结果和作用于结构上的操作之间的耦合解开，使得操作集合可以相对自由的演化。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
   <entry>
     <title>book-2. 苏黎世投机定律</title>
     <link href="/zurich-invest-principles"/>
     <updated>2015-03-01T00:00:00+08:00</updated>
     <id>/zurich-invest-principles</id>
     <content type="html">&lt;h2&gt;&lt;/h2&gt;

&lt;h2&gt;引言&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;瑞士，贫瘠而多岩，国土面积相当于美国缅因州的一半，自然资源缺乏，没有石油，没有煤炭，甚至气候和地形也不太适合大多数农作物。但是三百多年来没有卷入任何一场欧洲战争，而且瑞士人是世界上最富有的国民之一，瑞士法郎也是世界上最强势的货币之一。&lt;/li&gt;
&lt;li&gt;本书讲述瑞士人如何投机赚钱，并总结其中的一些经验。&lt;/li&gt;
&lt;li&gt;人的一生不能逃避风险，而应当谨慎地投入风险。小心地去赌，保持收益大于损失的原则，去赌，去赢。&lt;/li&gt;
&lt;li&gt;不要整天想着薪水，没有一个人能靠薪水发财，倒是有不少人因此而一贫如洗。所以你必须放手一搏，投机才是你所需要的。&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;定律一：论冒险&lt;/h2&gt;

&lt;p&gt;如果你对从事的投机不感到忧虑，那么你冒的风险肯定不够。&lt;br/&gt;
该定律要求你把钱投入到风险事业中，不要怕遭受损失。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;大多数人紧紧抓着安全，仿佛他是世界上最重要的东西。&lt;/li&gt;
&lt;li&gt;任何投机家都会告诉你，如果你一生的主要目标是避免忧虑，那么你将永远贫困。&lt;/li&gt;
&lt;li&gt;生命应当是一场冒险活动，而不是一种单调的生活。&lt;/li&gt;
&lt;li&gt;冒险会使生命更有价值，冒险的途径是把自己暴露于风险之中。&lt;/li&gt;
&lt;li&gt;每种职业都有它的渴望和痛苦。&lt;/li&gt;
&lt;li&gt;一切的投资都是投机，唯一的差异是有人承认，有人不承认。&lt;/li&gt;
&lt;li&gt;苏黎世投机定律是讨论投机的，这一点毋庸置疑。他们绝不是要你疯狂地去冒险，只不过话说的比较直率而已。&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;strong&gt;&lt;em&gt;次要定律一&lt;/em&gt;&lt;/strong&gt;：始终要下有意义的赌注。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;只下你负担得起损失的赌注。&lt;/li&gt;
&lt;li&gt;在投机的过程中，一开始你就要有甘心承受损失的心理准备。&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;strong&gt;&lt;em&gt;次要定律二&lt;/em&gt;&lt;/strong&gt;：避免过分分散风险&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;分散风险会降低你的风险，但这也同时减少了你可能致富的希望。&lt;/li&gt;
&lt;li&gt;你所投入的机会越多，就越需要更多的时间去研究，这额能会把你弄得焦头烂额。&lt;/li&gt;
&lt;li&gt;千万不要为了多样化而分散资金，这样，你会变得像在超级市场里参加购物比赛一样，竞赛的目的只是快速装满篮子，回到家时却发现带了一堆昂贵但又不真正需要的物品。在投机中，你应该把钱放在值得冒险的事业里，永远不要为了分散风险的投资组合而从事太多项目的投机。&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;定律二：论贪婪&lt;/h2&gt;

&lt;p&gt;尽早获利了结。 &lt;br/&gt;
该定律推荐不要等到涨势的顶峰，不要希望获利会一直持续下去，不要希望好运气会连续不断，要有“好运气是短暂的”心理。在你达到预定的目标时，立即获利了结。即使你周围的所有人都说暴涨将持续下去，你也要坚持自己的做法。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;如果你能战胜贪婪，这种自制的行为将使你比其他99%的追逐财富的人更能成为优秀的投机者。&lt;/li&gt;
&lt;li&gt;减少贪婪，你将有更多致富的机会。&lt;/li&gt;
&lt;li&gt;一旦渴望变得杂乱无章甚至失去控制，并达到一定程度时，它就会战胜你的意志，这就是贪婪。&lt;/li&gt;
&lt;li&gt;不要过度压榨你的运气。&lt;/li&gt;
&lt;li&gt;你总会有短暂的连续盈利的时候，千万不要让贪婪抓住了你。&lt;/li&gt;
&lt;li&gt;遵守贪婪定律，对某些人而言非常困难，主要困难可能就是怕后悔。&lt;/li&gt;
&lt;li&gt;永远不要去核对已经卖掉股票的股价。&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;strong&gt;&lt;em&gt;次要定律三&lt;/em&gt;&lt;/strong&gt;：预先决定利润目标，目标达成后，立即获利了结。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;到底赚多少才算够？贪婪是这个问题如此难解的主要原因，不管一个人已经拥有多少，他总是还要更多。&lt;/li&gt;
&lt;li&gt;钱来得太快会让你感觉这笔钱本来就应该是你的。&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;定律三：论希望&lt;/h2&gt;

&lt;p&gt;船开始下沉时，不要祷告，赶紧脱身。
定律三告诉我，当困难到来时，不要等待，果断地离开它。
定律二告诉我们当事情顺利时该怎么办，而定律三要说的是，当情况不好时该如何拯救自己。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;当你知道自己是怎么失败的时候，你就离成功不远了。&lt;/li&gt;
&lt;li&gt;一个业余的赌徒只会希望或祈祷好牌发到自己手里，但是职业赌徒研究的却是如何在逆境中生存。&lt;/li&gt;
&lt;li&gt;知道如何从困境中解脱，可能是最可贵的投机天赋。&lt;/li&gt;
&lt;li&gt;记住，当船刚开始下沉时，而不是等到已经下沉一半时，镇定地逃离这艘船，保全自己。&lt;/li&gt;
&lt;li&gt;拒绝认错本身就是最大的错误。&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;strong&gt;&lt;em&gt;次要定律四&lt;/em&gt;&lt;/strong&gt;：欣然接受小的损失，这是生活的一部分；尽管经历多次失败，但要设法一次振作。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;如果你能坦然地停止损失，你将会很好地保护自己，你可能永远不会受到严重的伤害。&lt;/li&gt;
&lt;li&gt;只要等待，一切终会发生。&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;定律四：论预测&lt;/h2&gt;

&lt;p&gt;人类的行为不能预测，不要相信任何未卜先知。&lt;br/&gt;
定律四告诉我们不要依靠预测来决定你的投机计划，因为它起不了作用。&lt;/p&gt;

&lt;h2&gt;定律五：论模式&lt;/h2&gt;

&lt;p&gt;在没有显示秩序之前，混乱并不危险。
定律五警告你，不要去设想一种模式，模式并不存在。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;当你感觉到一个有秩序的世界正在形成时，你已经处于危险之中了。&lt;/li&gt;
&lt;li&gt;真实情况是：金钱世界从未有过秩序，甚至是混乱得没有条理的世界。&lt;/li&gt;
&lt;li&gt;为什么人们不可能找到投资的公式，原因是这个公式根本不存在。&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;strong&gt;&lt;em&gt;次要定律五&lt;/em&gt;&lt;/strong&gt;：警惕历史会重演的心理暗示。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;历史的缺陷是一种特殊的对模式的幻想。&lt;/li&gt;
&lt;li&gt;不要跌进这个陷阱，历史有时候的确会重演，但大多数时候它不是这样的。&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;strong&gt;&lt;em&gt;次要定律六&lt;/em&gt;&lt;/strong&gt;：谨防图表分析的幻觉。
&lt;strong&gt;&lt;em&gt;次要定律七&lt;/em&gt;&lt;/strong&gt;：不要妄作因果关系。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;出发你能确切地看到一个原因在起作用，是真的看到它，否则你就要以最怀疑的态度看待所有的因果设想。&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;strong&gt;&lt;em&gt;次要定律八&lt;/em&gt;&lt;/strong&gt;：勿犯赌徒的荒谬心理。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;你持续扔硬币，迟早会得到正面。但是在这个过程中并无规律可言，你不能预先知道它将何时开始，当它开始后，你也不会知道它将持续多久。&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;定律六：论灵活&lt;/h2&gt;

&lt;p&gt;避免扎根，他们会妨碍你的灵活。
定律六要你保持灵活性，它告诫你，要避免因为扎根而丧失投机机会，比如忠诚的情感、持续等待回报的行为。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;次要定律九&lt;/em&gt;&lt;/strong&gt;：不要因为一起活练就的情绪而驻足于已经没有希望的投资活动。
&lt;strong&gt;&lt;em&gt;次要定律十&lt;/em&gt;&lt;/strong&gt;：如果有更吸引人的机会出现，要毫不迟疑放弃原来的事物。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;如果你根植于某些投机活动，它将迟早有损于你赚钱的目标。&lt;/li&gt;
&lt;li&gt;永远不要留恋旧事物，但是别忘了老朋友。&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;定律七：论直觉&lt;/h2&gt;

&lt;p&gt;如果一个预感可以被解释，那么它就是指的依赖的。
定律七认为，嘲笑和一味相信预感都是错误的。虽然直觉不会都是正确的，但如果以谨慎和怀疑的态度对待它，它可能就是一个有利的投机工具。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;次要定律十一&lt;/em&gt;&lt;/strong&gt;：绝对不要把希望和直觉混为一谈。&lt;/p&gt;

&lt;h2&gt;定律八：论神秘主义&lt;/h2&gt;

&lt;p&gt;上帝创造世界的计划中，未必包括使你发财。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;次要定律十二&lt;/em&gt;&lt;/strong&gt;：占星术要是灵的话，占星学家们早就富裕了。
&lt;strong&gt;&lt;em&gt;次要定律十三&lt;/em&gt;&lt;/strong&gt;：不要过度迷信，如果你能泰然处之，也可以从中享受到一些乐趣。&lt;/p&gt;

&lt;h2&gt;定律九：论乐观与悲观&lt;/h2&gt;

&lt;p&gt;乐观就是预期最好的情况会发生，信心则是知道如何处理最坏的状况。绝不要仅仅因为乐观而采取投机活动。
定律九警告，乐观可能是投机者的敌人，它会使人感觉良好，因此它是危险的。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;将乐观应用于金钱冒险时，你就需要警惕了。这是一种危险的心理状态。&lt;/li&gt;
&lt;li&gt;知道如何处理最坏的情况，那就是信心。&lt;/li&gt;
&lt;li&gt;在你感到乐观时，判断一下，这种愉快的感觉是不是真正见过了事实的证明。&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;定律十：论舆论&lt;/h2&gt;

&lt;p&gt;藐视大多数人的意见，因为他们很有可能是错的。
定律十教会我们，大多数人虽然不会一直主动滴犯错，但是犯错的机会还是比较大滴。
务必警惕不假思索地采取与大多数人相同或不同的立场，尤其是前者。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;真理通常由少数人掌握，而不是由多数人发现的。&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;strong&gt;&lt;em&gt;次要定律十四&lt;/em&gt;&lt;/strong&gt;：不要追逐投机的风潮，最好的购买机会常常是没有人注意到他的时候。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;当你周围的人都在冲你喊“不”的时候，你要想“是”是一件很困难的事情。&lt;/li&gt;
&lt;li&gt;当还没有一时到事情将如何发生的时候，新手早已被大多数人推动了。&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;定律十一：论固执&lt;/h2&gt;

&lt;p&gt;如果第一次没能赚到钱，忘掉他，重新再来。
毅力是一种很好的品质，但不可随便运用到投机活动中。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;毅力对我们日常生活的很多方面都有帮助。在投机活动中，它有时可以引导你走向成功，但有时也会使你陷入困境。&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;strong&gt;&lt;em&gt;次要定律十五&lt;/em&gt;&lt;/strong&gt;：绝对不要用摊平法挽救失败的投资。&lt;/p&gt;
</content>
   </entry>
   
   <entry>
     <title>［touch spark］10. Spark 学习资源集锦</title>
     <link href="/spark-resources"/>
     <updated>2015-03-01T00:00:00+08:00</updated>
     <id>/spark-resources</id>
     <content type="html">&lt;h2&gt;&lt;/h2&gt;

&lt;h2&gt;1. 写在前面&lt;/h2&gt;

&lt;p&gt;　　实际上，学习任何一门技术，最好的学习资料肯定是官网。但是，在我学习spark的过程中，我发现有两个理由告诉我为什么学习一门技术仍然需要一些官网之外的资料：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;广度：官网上的资料大都focus在技术本身的实现和用法，而有很多资料会提及这门技术的应用，以及实践、应用这门技术中遇到的坑，还有由这门技术衍生出来的其他辅助技术。&lt;/li&gt;
&lt;li&gt;深度：很多资料都会解析这门技术的实现以及原理，反应到官网上，大多都是API文档或技术架构；我发现，很多在官网上理解不了的细节，可能在一些资料上描述起来更明确。&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;　　基于上面这两个原因，我准备把自己在学习Spark过程中有幸遇到的好的资料搜集起来，并从广度和深度两个方面来整理这些资料，希望对大家有所帮助。还有，我会整理一些和spark相关的第三方开源工具，相信这些工具在帮助大家构建基于spark的应用时会助一臂之力。&lt;/p&gt;

&lt;p&gt;　　所有这些我都会同步到github上：&lt;a href=&quot;https://github.com/litaotao/spark-materials&quot;&gt;link&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;2. 资源列表&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;广度&lt;/strong&gt;：&lt;a href=&quot;http://blog.cloudera.com/blog/2014/08/how-to-use-ipython-notebook-with-apache-spark/&quot;&gt;spark和ipython notebook结合&lt;/a&gt;&lt;br/&gt;
&lt;strong&gt;备注&lt;/strong&gt;：我参考这篇文章搭建了自己基于ipython notebook的分析平台，详细记录在这篇博客上了：&lt;a href=&quot;../ipython-notebook-server-spark&quot;&gt;当Ipython Notebook遇见Spark&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;广深&lt;/strong&gt;：&lt;a href=&quot;http://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf&quot;&gt;RDD论文英文版&lt;/a&gt; &lt;br/&gt;
&lt;strong&gt;备注&lt;/strong&gt;：这篇论文绝对值得多读和深读，中文版在&lt;a href=&quot;https://code.csdn.net/CODE_Translation/spark_matei_phd&quot;&gt;这里&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;一个对Spark 2分钟的介绍视频，如果有人要你介绍Spark，按照里面的说就perfect了。&lt;a href=&quot;https://www.youtube.com/watch?v=cs3_3LdCny8&quot;&gt;YouTube&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;blockquote&gt;&lt;blockquote&gt;&lt;p&gt;&lt;strong&gt;Apache Spark is：&lt;/strong&gt;  &lt;br/&gt;
    + A data analytics, cluster computing framework;   &lt;br/&gt;
    + Fits into Hadoop open-source community, and builds on top of Hadoop     Distributed File System(HDFS);  &lt;br/&gt;
    + Not tied to two-stage MapReduce paradigm, it&#39;s performance up to 100 times faster than Hadoop MapReduce for certain applications;         &lt;br/&gt;
    + Provides primitives for in-memeory cluster computing;         &lt;br/&gt;
    + In-memory cluster computing allows user load data to cluster&#39;s memory and queried repeatedly;        &lt;br/&gt;
    + Suited for machine learning algorithms;&lt;/p&gt;&lt;/blockquote&gt;&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;广深&lt;/strong&gt;：&lt;a href=&quot;http://down.51cto.com/tag-spark%E4%B8%93%E5%88%8A.html&quot;&gt;Spark专刊之：spark最佳学习路径&lt;/a&gt;    &lt;br/&gt;
&lt;strong&gt;备注&lt;/strong&gt;：国人整理和翻译的spark学习资料，质量不错哦~&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;3. Spark相关的第三方开源工具&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Zeppelin&lt;/strong&gt;：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;官网：&lt;a href=&quot;http://zeppelin-project.org/&quot;&gt;http://zeppelin-project.org/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Hue&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;官网：&lt;a href=&quot;http://gethue.com/&quot;&gt;http://gethue.com/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;IPyhon&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;官网：&lt;a href=&quot;http://ipython.org/&quot;&gt;http://ipython.org/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;ISpark&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Github：&lt;a href=&quot;https://github.com/tribbloid/ISpark&quot;&gt;https://github.com/tribbloid/ISpark&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;scala-notebook&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Github: &lt;a href=&quot;https://github.com/Bridgewater/scala-notebook&quot;&gt;https://github.com/Bridgewater/scala-notebook&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;spark-notebook&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Github: &lt;a href=&quot;https://github.com/andypetrella/spark-notebook&quot;&gt;https://github.com/andypetrella/spark-notebook&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Ipython-sql&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Github: &lt;a href=&quot;https://github.com/catherinedevlin/ipython-sql&quot;&gt;https://github.com/catherinedevlin/ipython-sql&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;sparknotebook&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Github: &lt;a href=&quot;https://github.com/hohonuuli/sparknotebook&quot;&gt;https://github.com/hohonuuli/sparknotebook&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;spark-kernel&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Github: &lt;a href=&quot;https://github.com/ibm-et/spark-kernel&quot;&gt;https://github.com/ibm-et/spark-kernel&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;spark-jobserver&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Github: &lt;a href=&quot;https://github.com/spark-jobserver/spark-jobserver&quot;&gt;https://github.com/spark-jobserver/spark-jobserver&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;spark-packages&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;官网：&lt;a href=&quot;http://spark-packages.org/&quot;&gt;http://spark-packages.org/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
   <entry>
     <title>今天发生了一件坏事，也发生了一件好事</title>
     <link href="/the-badest-thing-in-my-life"/>
     <updated>2015-02-06T00:00:00+08:00</updated>
     <id>/the-badest-thing-in-my-life</id>
     <content type="html">&lt;h2&gt;&lt;/h2&gt;

&lt;h2&gt;1. 写在前面&lt;/h2&gt;

&lt;p&gt;　　以前读过两篇文章，一片叫《为什么从现在起你应该写博客》，第二篇记不得了，但其中有一个中心思想，说的是你写的东西都是给别人看的，应该都是对别人有用的。看了第一篇文章，我从不久之前终于借助Github建立了自己的一个博客，记录自己的技术成长经历；看了第二篇文章，我几乎每周都会对自己有一个总结，而从不把这些总结发到博客上。因为我认为这些总结都是未来回忆自己年轻时的年少轻狂用的，不应该发到博客上了。&lt;br/&gt;
　　但是今天，发生了一件坏事，算是从小到大最丑的一件事了，但同时也是一件好事。至少以后别人再问我最丑的经历时，我有话可说了。&lt;/p&gt;

&lt;h2&gt;2. 人生最丑的事&lt;/h2&gt;

&lt;p&gt;　　今天在小组内做第二次关于spark的技术分享，讲得很烂，算是人生的一个污点了，具体经历就不讲了。但我保证这是历史和未来所有presentation中最烂的一次。我将从以下几个方面来记录这次事件，以后每次活动之前，我都要自己看看今天的这篇文章，告诫自己，不要在同一个地方跌倒两次。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;没有事先准备好&lt;br/&gt;
　　以后进行presentation，一定要提前半小时入场，把电脑环境配置好。今天失败的根本原因是一个自定义的配置文件在断网重连后没有进行source操作，导致IPython Notebook Server和Spark集群之间连接不上。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;紧急情况下不会冷静分析问题&lt;br/&gt;
　　在启动IPython Notebook后，发现 &lt;code&gt;from pyspark import SparkContext, SparkConf&lt;/code&gt; 不能成功，提示没有pyspark这个包。当时我就慌了，没有沿着本质去分析问题，然后在老板的催促和心慌当中就开始讲了，在大家的迷茫和我不知所以的回答中，我就这样浑浑噩噩的度过了几十分钟，中间还幸得一位同事解围。在最后几分钟的时候，我实在忍不住了，心想反正今天已经完蛋了，就不管别人在那里讨论什么了。我开始研究到底是什么原因导致Notebook Server连接不上spark集群。我首先查看了下IPython的配置，在 &lt;code&gt;~/.ipython/profile_pyspark/&lt;/code&gt; 目录下面，我先看ipython_notebook_config.py文件，发现没有什么异常的，心想也应该不会出现在这里吧，因为IPython notebook还是能用的，只是找不到pyspark这个包。我突然回忆起来，在当初配置spark+IPython的时候，需要设置一个启动脚本文件。接着，我查看了那个启动脚本，在 &lt;code&gt;/root/.ipython/profile_pyspark/startup/00-pyspark-setup.py&lt;/code&gt; 下，恍然大悟，我发现问题了，这个脚本是这样写的：&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;pre&gt;&lt;code&gt;import os
import sys

spark_home = os.environ.get(&#39;SPARK_HOME&#39;, None)

if not spark_home:
    raise ValueError(&#39;SPARK_HOME environment variable is not set&#39;)
sys.path.insert(0, os.path.join(spark_home, &#39;python&#39;))
sys.path.insert(0, os.path.join(spark_home, &#39;python/lib/py4j-0.8.2.1-src.zip&#39;))
#execfile(os.path.join(spark_home, &#39;python/pyspark/shell.py&#39;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　Say，关键就是这句 &lt;code&gt;spark_home = os.environ.get(&#39;SPARK_HOME&#39;, None)&lt;/code&gt; 因为没有定义一个环境变量，因此不能在IPython Notebook的环境变量sys.path里加入pyspark包所在的地址。再想想自己在哪里定义了SPARK_HOME这个变量，明明就在Desktop下的&lt;code&gt;ipython_notebook_spark.bashrc&lt;/code&gt; 里，看看里面是怎么写的：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;&quot;&gt;export SPARK_HOME=&quot;/usr/local/spark-1.2.0-bin-cdh4/&quot;
export PYSPARK_SUBMIT_ARGS=&quot;--master spark://10.21.208.21:7077 --deploy-mode client&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　这下知道了，原因是自己定义的.bashrc文件在登出linux后会失效，而刚才把电脑带到会议室后重新连接的网络，linux console重新登出了，所以必须重新source这个文件。 &lt;br/&gt;
　　现在回想，其实应该在看到错误提示没有pyspark这个包的时候就应该发现这个问题了，之所以一开始没有推测出这个问题，有几个原因：一是在这种情况下发现错误，头脑发热发慌，不能冷静处理；二是自己对这部分技术没有掌握彻底；3是在学习一门新技术时，没有详细地把学习过程和一些操作过程记录下来，导致出问题后不能及时参考。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;表达不清 &lt;br/&gt;
　　还有一条经验就是表达不清楚，思路不清楚，回答别人的问题不够明确。在别人提问没有表达清晰时，自己没有和别人进行沟通就贸然回答，导致交流效率极低。曾经看过很多文章，都讲做技术的一定要努力提高自己的表达能力，我满不在乎，因为平时和大家感觉交流起来还算流畅的，没想到这次碰了瓷。得到一条宝贵的经验，以后再做分享的时候，一定要准备好详细讲稿和提纲。别人在提问时，一定要先把别人提问的问题搞清楚，理解了再进行回答。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;一些细节问题 &lt;br/&gt;
　　这次分享出来本组人员，还有鹏哥和他们组的两个人员，这种情况下在开始前应该由主讲人[我]来互相介绍一下，介绍双方的人员和职责。 &lt;br/&gt;
　　在讲稿，PPT，程序准备的时候，要注意配色和字体，字体要大，颜色要和谐，让听众能清晰地看到投影仪的东西。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;最后&lt;br/&gt;
　　最后，我想说，这真的是人生最糟糕的一次经历，以后绝对不能出现同样的问题，在以后每次做分享的时候，我要多回头看看这篇文章，想想今天尴尬的我。今天的分享，远远没有达到预期效果，还耽误了大家不少时间，也影响了自己的形象。这种情况下不能逃避，我应该给参会的每个人员发封邮件表示歉意，这也是对别人的尊重。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;3. 感悟&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;机会，永远只给有所准备，并且已经准备好了的人；&lt;/li&gt;
&lt;li&gt;从每一件小事做起，把每一件小事做到极致；
&lt;img src=&quot;../images/make_little_thing_best.jpg&quot; alt=&quot;make_little_thing_best&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
   <entry>
     <title>［touch spark］9. 编译Zeppelin</title>
     <link href="/compile-zeppelin"/>
     <updated>2015-01-29T00:00:00+08:00</updated>
     <id>/compile-zeppelin</id>
     <content type="html">&lt;h2&gt;&lt;/h2&gt;

&lt;h2&gt;1. 写在前面&lt;/h2&gt;

&lt;p&gt;　　这篇记录是我按照&lt;a href=&quot;http://zeppelin-project.org/docs/install/install.html&quot;&gt;官网&lt;/a&gt;的步骤来写的，主要是记录在编译Zeppelin过程中的一些经验。&lt;br/&gt;
　　对于这类没有发布特别稳定版本的项目，我倾向于这样一种实践方法，在本地建两个文件夹：project, project-build，把project文件夹作为一个本地的repo，可以随时update到最新的源码，然后在project-build里构建，就算失败了也可以保证不会污染项目源文件。
　　再具体一点就是我在实践Zeppelin的时候使用的方法，如下所示，可以随时保证zeppelin文件夹里的源码最新，然后在zeppelin-build里用最新的源码构建。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@kali:~/Desktop# mkdir zeppelin zeppelin-build
root@kali:~/Desktop# cd zeppelin
root@kali:~/Desktop/zeppelin# git init
Initialized empty Git repository in /root/Desktop/zeppelin/.git/
root@kali:~/Desktop/zeppelin# git remote add origin git@github.com:NFLabs/zeppelin.git
root@kali:~/Desktop/zeppelin# git pull origin master
root@kali:~/Desktop/zeppelin# cd ../zeppelin-build/
root@kali:~/Desktop/zeppelin-build# cp -R ../zeppelin/* .
root@kali:~/Desktop/zeppelin-build# mvn clean package -DskipTests
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;2. 详细步骤及错误解决&lt;/h2&gt;

&lt;h3&gt;2.1 按照默认配置编译 : mvn clean package -DskipTests&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;root@ubuntu2[15:30:17]:~/Desktop/zeppelin-build#mvn clean package -DskipTests
.
.
.
[INFO] npm WARN deprecated grunt-ngmin@0.0.3: use grunt-ng-annotate instead
[INFO] npm ERR! 
[INFO] npm ERR! Additional logging details can be found in:
[INFO] npm ERR!     /root/Desktop/zeppelin-build/zeppelin-web/npm-debug.log
[INFO] npm ERR! not ok code 0
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Zeppelin ........................................... SUCCESS [ 37.320 s]
[INFO] Zeppelin: Zengine .................................. SUCCESS [  9.899 s]
[INFO] Zeppelin: Spark .................................... SUCCESS [ 12.115 s]
[INFO] Zeppelin: Markdown interpreter ..................... SUCCESS [  2.257 s]
[INFO] Zeppelin: Shell interpreter ........................ SUCCESS [  2.239 s]
[INFO] Zeppelin: web Application .......................... FAILURE [04:10 min]
[INFO] Zeppelin: Server ................................... SKIPPED
[INFO] Zeppelin: Packaging distribution ................... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 05:15 min
[INFO] Finished at: 2015-02-11T15:30:16+08:00
[INFO] Final Memory: 61M/409M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal com.github.eirslett:frontend-maven-plugin:0.0.20:npm (npm install) on project zeppelin-web: Failed to run task: &#39;npm install --color=false&#39; failed. (error code 1) -&amp;gt; [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn &amp;lt;goals&amp;gt; -rf :zeppelin-web
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;2.1 出错: npm install --color=false&lt;/h3&gt;

&lt;p&gt;　　这个错误在&lt;a href=&quot;https://groups.google.com/forum/#!searchin/zeppelin-developers/npm$20install&quot;&gt;mailing list&lt;/a&gt;里提到了，我们按照里面的解决方案来尝试一下。注意，以前的mailing list的维护在google groups里面的，但今年2月份之后groups就只读，不可以发帖了，新的mailing list移植到apache旗下，地址在&lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/incubator-zeppelin-users/&quot;&gt;这里&lt;/a&gt;。
　　解决方案：先在zeppelin-web文件夹里运行 &lt;code&gt;npm install&lt;/code&gt;，然后再回到zeppelin-build目录构建。&lt;/p&gt;

&lt;h3&gt;2.2 启动zeppelin服务器错误，提示Unsupported major.minor&lt;/h3&gt;

&lt;p&gt;　　在运行 bin/zeppelin-daemon.sh start 的时候提示错误如下。一开始没有发现什么原因，后来google 关键字 &lt;code&gt;nsupported major.minor&lt;/code&gt;后在&lt;a href=&quot;http://www.oecp.cn/hi/yangtaoorange/blog/1168263&quot;&gt;这里&lt;/a&gt;知道问题应该和JAVA编译器的版本有问题，因为zeppelin需要java 1.7以上，我本机是java 1.6的，当时为了编译zeppelin我自己下了java 1.7和设置了一个java 1.7的环境变量文件，应该是没有source这个环境变量吧，测试一下发现$JAVA_HOME这个环境变量为空。这里再次source一下java_1.7_path.bashrc这个文件就可以了，这个文件是这样写的：
&lt;code&gt;
root@ubuntu2[10:59:24]:~/Desktop#cat java_1.7_path.bashrc
export PATH=/usr/local/jdk1.7.0_71/bin:$PATH
export CLASSPATH=&quot;/usr/local/jdk1.7.0_71/lib:.&quot;
export JAVA_HOME=&quot;/usr/local/jdk1.7.0_71/&quot;
&lt;/code&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@ubuntu2[10:51:15]:~/Desktop/zeppelin-build#vi logs/zeppelin-root-ubuntu2.out 
Exception in thread &quot;main&quot; java.lang.UnsupportedClassVersionError: com/nflabs/zeppelin/
server/ZeppelinServer : Unsupported major.minor version 51.0
        at java.lang.ClassLoader.defineClass1(Native Method)
        at java.lang.ClassLoader.defineClass(ClassLoader.java:643)
        at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
        at java.net.URLClassLoader.defineClass(URLClassLoader.java:277)
        at java.net.URLClassLoader.access$000(URLClassLoader.java:73)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:212)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:205)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:323)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:294)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:268)
Could not find the main class: com.nflabs.zeppelin.server.ZeppelinServer. Program will
exit.
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;2.3 运行出错，日志提示：java.net.UnknownHostException: &lt;em&gt;&lt;your hostname&gt;&lt;/em&gt;: nodename nor servname provided, or not known&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;../images/zeppelin-run-error.jpg&quot; alt=&quot;run-error&quot; /&gt;&lt;/p&gt;

&lt;p&gt;解决办法[我的机器host名叫mac007]，修改/etc/hosts，新增一项 127.0.0.1   mac007&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/zeppelin-hosts.jpg&quot; alt=&quot;revise-hosts&quot; /&gt;
　　&lt;/p&gt;
</content>
   </entry>
   
   <entry>
     <title>［touch spark］8. 当Ipython Notebook遇见Spark</title>
     <link href="/ipython-notebook-server-spark"/>
     <updated>2015-01-27T00:00:00+08:00</updated>
     <id>/ipython-notebook-server-spark</id>
     <content type="html">&lt;p&gt;注：和本文相关的资料和文件都放到Github上了：&lt;a href=&quot;https://github.com/litaotao/ipython-notebook-spark&quot;&gt;ipython-notebook-spark&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;&lt;/h2&gt;

&lt;h2&gt;1. 致谢&lt;/h2&gt;

&lt;p&gt;　　首先我忠心地感谢Ipython，Spark的开源作者，真心谢谢你们开发这么方便，好用，功能强大的项目，而且还无私地奉献给大众使用。刚刚很轻松地搭建了一个机遇Ipython Notebook的Spark客户端，真的感受到 The power of technology, the power of open source.&lt;br/&gt;
　　下面是这两个项目的github地址：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/ipython/ipython&quot;&gt;Ipython&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://github.com/apache/spark&quot;&gt;Spark&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;　　同时，这篇文章在刚开始的部分，参考了很多 &lt;a href=&quot;http://blog.cloudera.com/blog/2014/08/how-to-use-ipython-notebook-with-apache-spark/&quot;&gt;这篇博客&lt;/a&gt;的内容，感谢这么多人能无私分享如此高质量的内容。 &lt;br/&gt;
　　但是，这篇文章不是简单记录怎么做，我尽量做到量少质高，所以有些地方会说得比较详细，其中也会提到在解决遇到的问题上的一些方法和思路。&lt;/p&gt;

&lt;h2&gt;2. 路线规划&lt;/h2&gt;

&lt;p&gt;　　基于 &lt;a href=&quot;http://www.databricks.com/&quot;&gt;Databricks&lt;/a&gt;，&lt;a href=&quot;zeppelin-project.org&quot;&gt;Zeppelin&lt;/a&gt; 和 &lt;a href=&quot;www.gethue.com&quot;&gt;Hue&lt;/a&gt; 的启发，我也想尝试搭建一个丰富可用的在线大数据REPL分析平台，正好用此机会好好实践一下spark，毕竟都学习spark几个月了呢。 &lt;br/&gt;
　　不说废话，同&lt;a href=&quot;../weibo-api-in-action&quot;&gt;使用spark分析微博数据那篇博文一样&lt;/a&gt;，我们也要有一个路线规划：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;搭建一个可多用户使用的，底层接入了spark集群的Ipython Notebook Server；&lt;/li&gt;
&lt;li&gt;完善 Weibo Message Driver，使用户可在Notebook里获取、分析微博数据，as simple as possible；&lt;/li&gt;
&lt;li&gt;研究Zeppelin和Hue项目，把其中一个嫁接在Notebook的上层，实现准产品级的大数据实时ETL，Analytic，Sharing平台；这一步可能需要较长时间，可根据自己的时间安排灵活调整；&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;　　Dream：在年前完成上面三步，that&#39;s really full or chanllenge, but more funny. &lt;strong&gt;Anyway, we need dreams, and I can&#39;t wait to make this dream into reality.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/dreams.jpg&quot; alt=&quot;dreams&quot; /&gt;&lt;/p&gt;

&lt;p&gt;　　这篇主要记录我在实现第一步的过程中遇到的主要步骤，遇到的问题和解决方法：搭建一个可多用户使用的，底层接入了spark集群的Ipython Notebook Server。&lt;/p&gt;

&lt;h2&gt;3. 配置Ipython&lt;/h2&gt;

&lt;h3&gt;3.1: ipython 配置名profile介绍&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;profile 命令说明&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;　　profile是ipython的一个子命令，其中profile又有两个子命令，分别是create和list，顾名思义，create就是创建一个配置文件，list就是列出当前配置文件。如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@ubuntu2[13:54:01]:~/Desktop#ipython profile 
No subcommand specified. Must specify one of: [&#39;create&#39;, &#39;list&#39;]

Manage IPython profiles

Profile directories contain configuration, log and security related files and
are named using the convention &#39;profile_&amp;lt;name&amp;gt;&#39;. By default they are located in
your ipython directory.  You can create profiles with `ipython profile create
&amp;lt;name&amp;gt;`, or see the profiles you already have with `ipython profile list`

To get started configuring IPython, simply do:

$&amp;gt; ipython profile create

and IPython will create the default profile in &amp;lt;ipython_dir&amp;gt;/profile_default,
where you can edit ipython_config.py to start configuring IPython.

Subcommands
-----------

Subcommands are launched as `ipython cmd [args]`. For information on using
subcommand &#39;cmd&#39;, do: `ipython cmd -h`.

create
    Create an IPython profile by name
list
    List available IPython profiles
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;profile子命令list说明&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;　　本想list命令应该很简单的，和linux下的ls差不多嘛，但我自己看了下，其中还是有些细节值得推敲的。其中这项 &lt;code&gt;Available profiles in /root/.config/ipython:&lt;/code&gt; 是说目前有两个配置文件在那个目录下面，pyspark是我自己创建的了。在参考的&lt;a href=&quot;http://blog.cloudera.com/blog/2014/08/how-to-use-ipython-notebook-with-apache-spark/&quot;&gt;这篇文章&lt;/a&gt;中，作者说创建的配置文件会放到 &lt;code&gt;~/.ipython/profile_pyspark/&lt;/code&gt; 下，其实这并不是一定的，具体放在哪个目录下面，可以根据profile list的命令来查看。如此看来，我们在这台机器上创建的配置文件应该是放在目录 &lt;code&gt;/root/.config/ipython&lt;/code&gt; 下面的。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@ubuntu2[14:09:12]:~/Desktop#ipython profile list

Available profiles in IPython:
    pysh
    math
    sympy
    cluster

    The first request for a bundled profile will copy it
    into your IPython directory (/root/.config/ipython),
    where you can customize it.

Available profiles in /root/.config/ipython:
    default
    pyspark

To use any of the above profiles, start IPython with:
    ipython --profile=&amp;lt;name&amp;gt;  
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;profile子命令create说明&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;　　简单介绍下create子命令的用法。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@ubuntu2[09:25:57]:~/Desktop#ipython profile help create
Create an IPython profile by name

Create an ipython profile directory by its name or profile directory path.
Profile directories contain configuration, log and security related files and
are named using the convention &#39;profile_&amp;lt;name&amp;gt;&#39;. By default they are located in
your ipython directory. Once created, you will can edit the configuration files
in the profile directory to configure IPython. Most users will create a profile
directory by name, `ipython profile create myprofile`, which will put the
directory in `&amp;lt;ipython_dir&amp;gt;/profile_myprofile`.
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;3.2 创建新的Ipython配置文件&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;创建配置文件&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;　　因为我之前已经配置过一个pyspark的配置文件了，这里我们创建一个测试用的配置文件，pytest。运行一下命令后，会在 &lt;code&gt;/root/.config/ipython&lt;/code&gt; 下生成一个 pytest的目录。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@ubuntu2[14:54:14]:~/Desktop#ipython profile create pytest
[ProfileCreate] Generating default config file: u&#39;/root/.config/ipython/profile_pytest/ipython_config.py&#39;
[ProfileCreate] Generating default config file: u&#39;/root/.config/ipython/profile_pytest/ipython_notebook_config.py&#39;

root@ubuntu2[15:00:57]:~/Desktop#ls ~/.config/ipython/profile_pytest/
ipython_config.py  ipython_notebook_config.py  log  pid  security  startup 
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;3.3 编辑配置文件&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;编辑ipython_notebook_config.py&lt;/li&gt;
&lt;/ul&gt;


&lt;pre&gt;&lt;code&gt;    c = get_config()

    # about line 15, the ip address the notebook server will listen on. Set it to * means that any IP/Machine which can connect to the server can connect to the notebook server.
    c.NotebookApp.ip = &#39;*&#39;
    # about line 37, whether to open a browser or not. cause what we want to build is a backend server, so we set it False, no need to open a browser.
    c.NotebookApp.open_browser = False
    # about line 54, the port which the notebook server will listen on
    c.NotebookApp.port = 8880 # or whatever you want, make sure the port is available  
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;设置访问密码 &lt;br/&gt;
　　如果你的notebook server是需要访问控制的，简单的话可以设置一个访问密码。听说Ipython 2.x 版本有用户访问控制，这里我还没有接触过，晚点会看看是否有成熟的可用的用户控制方案。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;生成密码文件&lt;br/&gt;
这里我们用python自带的密码包生成一个密码，然后再把这个密码重定向到nvpasswd.txt文件里。注意这里重定向的路径哦。&lt;/li&gt;
&lt;li&gt;编辑配置文件，设置读取密码文件配置项
这里有一个需要注意的，就是PWDFILE的设置，一开始我设置为 &lt;code&gt;~/.config/ipython/profile_pytest/nbpasswd.txt&lt;/code&gt;，但是启动ipython notebook server的时候老师报错，说找不到密码文件nbpasswd.txt，很奇怪，明明文件就是在的，可就是提示找不到。无奈我到nbpasswd.txt路径下用 pwd 打印当前路径，显示为 &lt;code&gt;root/.config/ipython/profile_pytest/nbpasswd.txt&lt;/code&gt;，可是这两个路径应该是一样的啊。无奈之下，死马当作活马医，我就把PWDFILE设置成为 &lt;code&gt;root/.config/ipython/profile_pytest/nbpasswd.txt&lt;/code&gt;，没想到这样还成功了。关于这点为什么会有效，目前我还不是很清楚，等我请教了公司大神后再补上这一个tip吧。&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;  示例如下：&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;pre&gt;&lt;code&gt;root@ubuntu2[09:40:29]:~/Desktop#python -c &#39;from IPython.lib import passwd; print passwd()&#39; &amp;gt; ~/.config/ipython/profile_pytest/nbpasswd.txt
Enter password: 
Verify password: 
root@ubuntu2[09:43:35]:~/Desktop#vi /root/.config/ipython/profile_pytest/nbpasswd.txt 
sha1:c6b748a8e1e2:4688f91ccfb9a8e0afd041ec77cdda99d0e1fb8f  

root@ubuntu2[09:49:09]:~/Desktop#vi /root/.config/ipython/profile_pytest/ipython_notebook_config.py 
# about line 95
PWDFILE=&#39;root/.config/ipython/profile_pytest/nbpasswd.txt&#39;
c.NotebookApp.password = open(PWDFILE).read().strip()
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;设置启动文件&lt;br/&gt;
　　这一步算是比较重要的了，也是我在配置这个notebook server中遇到的比较难解的问题。这里我们首先需要创建一个启动文件，并在启动文件里设置一些spark的启动参数。如下：&lt;/li&gt;
&lt;/ul&gt;


&lt;pre&gt;&lt;code&gt;root@ubuntu2[09:52:14]:~/Desktop#touch ~/.config/ipython/profile_pytest/startup/00-pytest-setup.py 
root@ubuntu2[10:08:44]:~/Desktop#vi ~/.config/ipython/profile_pytest/startup/00-pytest-setup.py   

import os
import sys

spark_home = os.environ.get(&#39;SPARK_HOME&#39;, None)
if not spark_home:
    raise ValueError(&#39;SPARK_HOME environment variable is not set&#39;)
sys.path.insert(0, os.path.join(spark_home, &#39;python&#39;))
sys.path.insert(0, os.path.join(spark_home, &#39;python/lib/py4j-0.8.1-src.zip&#39;))
# execfile(os.path.join(spark_home, &#39;python/pyspark/shell.py&#39;))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　上面的启动配置文件也还简单，即拿到spark_home路径，并在系统环境变量path里加上两个路径，然后再执行一个shell.py文件。不过，在保存之前还是先确认下配置文件写对了，比如说你的SPARK_HOME配置对了，并且下面有python这个文件夹，并且python/lib下有py4j-0.8.1这个文件。我在检查的时候就发现我的包版本是py4j-0.8.2.1的，所以还是要改得和自己的包一致才行。 &lt;br/&gt;
　　这里得到一个经验，在这种手把手，step by step的教程中，一定要注意版本控制，毕竟各人的机器，操作系统，软件版本等都不可能完全一致，也许在别人机器上能成功，在自己的机器上不成功也是很正常的事情，毕竟细节决定成败啊！所以在我这里，这句我是这样写的： &lt;code&gt;sys.path.insert(0, os.path.join(spark_home, &#39;python/lib/py4j-0.8.2.1-src.zip&#39;))&lt;/code&gt;  &lt;br/&gt;
　　注意，上面的最后一行 &lt;code&gt;execfile(os.path.join(spark_home, &#39;python/pyspark/shell.py&#39;))&lt;/code&gt; 被注释掉了，表示在新建或打开一个notebook时并不去执行shell.py这个文件，这个文件是创建SparkContext的，即如果执行改行语句，那在启动notebook时就会初始化一个sc，但这个sc的配置都是写死了的，在spark web UI监控里的appName也是一样的，很不方便。而且考虑到并不是打开一个notebook就要用到spark的资源，所以最好是要用户自己定义sc了。 &lt;br/&gt;
　　python/pyspark/shell.py的核心代码： &lt;br/&gt;
&lt;code&gt;
sc = SparkContext(appName=&quot;PySparkShell&quot;, pyFiles=add_files)
atexit.register(lambda: sc.stop())
&lt;/code&gt;&lt;/p&gt;

&lt;h2&gt;4. Ok，here we go　　&lt;/h2&gt;

&lt;p&gt;　　到这里差不多大功告成了，可以启动notebook server了。不过在启动之前，需要配置两个环境变量参数，同样，这两个环境变量参数在也是根据个人配置而定的。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# for the CDH-installed Spark
export SPARK_HOME=&#39;/usr/local/spark-1.2.0-bin-cdh4/&#39;

# this is where you specify all the options you wou
ld normally add after bin/pyspark
  export PYSPARK_SUBMIT_ARGS=&#39;--master spark://10.21.208.21:7077 --deploy-mode client&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　ok，万事具备，只欠东风了。让我们来尝尝鲜吧：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;root@ubuntu2[10:40:50]:~/Desktop#ipython notebook --profile=pyspark
2015-02-01 10:40:54.850 [NotebookApp] Using existing profile dir: u&#39;/root/.config/ipython/profile_pyspark&#39;
2015-02-01 10:40:54.858 [NotebookApp] Using MathJax from CDN: http://cdn.mathjax.org/mathjax/latest/MathJax.js
2015-02-01 10:40:54.868 [NotebookApp] CRITICAL | WARNING: The notebook server is listening on all IP addresses and not using encryption. This is not recommended.
2015-02-01 10:40:54.869 [NotebookApp] Serving notebooks from local directory: /root/Desktop
2015-02-01 10:40:54.869 [NotebookApp] The IPython Notebook is running at: http://[all ip addresses on your system]:8880/
2015-02-01 10:40:54.869 [NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　在浏览器输入driver:8880即可访问notebook server了，首先会提示输入密码，密码正确后就可以使用了。
&lt;img src=&quot;../images/notebook-spark-1.jpg&quot; alt=&quot;notebook-spark-1&quot; /&gt;
&lt;img src=&quot;../images/notebook-spark-2.jpg&quot; alt=&quot;notebook-spark-2&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;5. 总结&lt;/h2&gt;

&lt;p&gt;　　下面是简单的步骤总结：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;建立环境变量配置文件：ipython_notebook_spark.bashrc&lt;/li&gt;
&lt;/ul&gt;


&lt;pre&gt;&lt;code&gt;export SPARK_HOME=&quot;/usr/local/spark-1.2.0-bin-cdh4/&quot;
export PYSPARK_SUBMIT_ARGS=&quot;--master spark://10.21.208.21:7077 --deploy-mode client&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;配置Ipython notebook server

&lt;ul&gt;
&lt;li&gt;ipython profile create pyspark&lt;/li&gt;
&lt;li&gt;编辑ipython_notebook_config.py&lt;/li&gt;
&lt;li&gt;[可选]配置ipython notebook登录密码&lt;/li&gt;
&lt;li&gt;设置启动文件&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;设置启动脚本&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;6. 启动脚本和文件&lt;/h2&gt;

&lt;p&gt;　　为了方便，都放到Github上了。&lt;/p&gt;
</content>
   </entry>
   
   <entry>
     <title>［touch spark］6. 终于等到你，spark streaming + 新浪微博数据</title>
     <link href="/weibo-api-in-action"/>
     <updated>2015-01-05T00:00:00+08:00</updated>
     <id>/weibo-api-in-action</id>
     <content type="html">&lt;p&gt;注：与本文相关的所有源代码已放在最最喜爱的 &lt;a href=&quot;https://github.com/litaotao/weibostreaming&quot;&gt;Github&lt;/a&gt; 上。&lt;/p&gt;

&lt;h2&gt;&lt;/h2&gt;

&lt;h2&gt;1. 写在前面&lt;/h2&gt;

&lt;p&gt;　　从最开始想尝试streaming的时候，我一心想实现databricks在spark summit 2014上的那个演示---实时获取twiiter数据，并做分析展示。无奈，在折腾twitter几天后只得放弃。刚开始注册twitter得需要手机号，可是twitter居然不支持中国内地的号码，这里不知道是twitter不支持，还是中国内地的运营商不支持twitter，我想应该是后者吧。当时纠结了好久，一种心碎的感觉。后来终于突然有一天无需手机号也能注册了，我欣喜若狂地注册了一个twitter号，后来发现尼玛新建一个twitter app需要手机验证，这下我就完全down机了，无奈，twitter这条路是完全走不通了。这段时间的心情，就像过山车一样，从心碎到兴奋再到最后的心死。&lt;br/&gt;
&lt;img src=&quot;../../images/guoshanche.gif&quot; alt=&quot;过山车&quot; /&gt;
　　可是我不死心啊，还行想玩玩streaming，想体验体验streaming的power，怎么办怎么办？我左思右想，想要做到databricks的效果，唯一的办法就是利用咱们天朝的新浪微博了。在调研了一下新浪微博的API后，其原理和twitter API的原理也是一样的，但是新浪微博的streaming API被取消了。好吧，硬骨头挺多，还得自己啃了。没办法，我想了想，就2个选择，要么不做，然后天天后悔；要么做，然后天天折腾。好吧，我承认我还是义无返顾地选择了第二条路，sigh。&lt;br/&gt;
　　在写这篇博客的时候看到徐静蕾的新片《有一个地方只有我们知道》，里面一句话让我深有感触---&lt;strong&gt;没有在一起的，就是不对的人，对的人，你是不会失去他的&lt;/strong&gt;，我也想说，没有学到的技术，就是你不喜爱的技术，喜欢的技术，你是不可能学不会的。&lt;/p&gt;

&lt;h2&gt;2. 实验目的&lt;/h2&gt;

&lt;p&gt;　　这次实验是想尝试一下spark streaming的效果，预期是这样的：通过每隔 &lt;strong&gt;几秒&lt;/strong&gt; 从新浪微博拿到 &lt;strong&gt;一些&lt;/strong&gt; 公开的微博数据，然后实时 &lt;strong&gt;处理&lt;/strong&gt; 一下这些数据并 &lt;strong&gt;展现&lt;/strong&gt; 出来。&lt;br/&gt;
　　ok，这里的关键上面已经用粗体标记出来了。有两个方面，一是时间间隔的设置，数据流量的设置，这关系到streaming的稳定性，比如说若处理速度小于数据流入的速度的话，那数据会慢慢堆积起来；若数据流入速度小于处理速度的话，展现处理结果肯定也不好看。这里是属于tuning的环境，可以在spark官网上仔细瞧瞧，不过具体还是要根据应用需求来定。第二个方面是数据处理和展示，这应该是应用的核心了。这里我们做得很初级，简单做一些TF-IDF的测试或者是更简单的包含性测试。 &lt;br/&gt;
　　既然是第一次，那就不要太那啥，还是温柔一点比较好。暂时定一个目标，我们想看看实时微博中哪些是包含某某字段的，然后输出这些微博的信息。比如说，我想知道实时微博中，哪些是包含 &lt;strong&gt;建设银行&lt;/strong&gt;，&lt;strong&gt;涨&lt;/strong&gt; 这样两个关键字的微博，并实时打印出来。&lt;/p&gt;

&lt;h2&gt;3. streaming，我的目标&lt;/h2&gt;

&lt;p&gt;　　既然要玩，那就玩得痛快点，下面是我准备在微博streaming这块做的一些各个版本的安排：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;数据方面：

&lt;ul&gt;
&lt;li&gt;第一步，能够获取微博伪实时数据即可【微博API请求有限制】；&lt;/li&gt;
&lt;li&gt;第二步，自己设计一个搜集系统，能获取近实时的微博数据，希望能媲美原来微博的streaming API；&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;应用方面：

&lt;ul&gt;
&lt;li&gt;第一步，啥都不干，确认spark   cluster能连到我的数据源，把所有接收到的数据简单打印出来；&lt;/li&gt;
&lt;li&gt;第二步，简单处理，对实时数据流进行一个简单的filter操作，比如说，看看哪些消息是提到了某人，或某支股票；&lt;/li&gt;
&lt;li&gt;第三步，复杂一点，用用MLib来对每条消息学习一下，其实就是高级的filter；&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;　　目前我打算从上面几个小目标一点一点上，最近发现一个NB项目，Zeppelin，底层可以集成spark，届时看看是否有需要，可以尝试下
eppelin+spark。【最近我尝试编译过Zeppelin，遇到很多问题，目前这个项目还不是很成熟，不过项目组说了，他们正在迁移到Apache的孵化器中，完成迁移后会专心发布新版本。Good，看来的确是一个NB项目】 &lt;br/&gt;
　　冲啊，每天进步一点点。
&lt;img src=&quot;../../images/stepbystep.jpg&quot; alt=&quot;每天进步一点点咯&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;4. 步骤规划&lt;/h2&gt;

&lt;p&gt;　　我们第一次的目标很简单，我准备在数据方面，简单地完成第一步；在应用方面，也是简单地完成第一步，算是一个最小的MVP了，暂定为MVP 0.1.0
吧，哈哈。好，现在简单地分析下，大概有下面几个步骤：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;获取数据，通过新浪微博API，这里需要我们设计一个数据收集器&lt;/li&gt;
&lt;li&gt;发送/接收数据，因为我改用Python来玩spark了，目前spark 1.2版的python streaming只支持socket包，当然socket包也是最简单的了，所以我准备用socket方式进行数据的收发。So，这里我们需要写一个简单的Socket Server&lt;/li&gt;
&lt;li&gt;展示数据，简单的打印下来即可&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;　　下面，我们就按照MVP 0.1.0 的步骤规划，一步一步来搞定咱们这个小系统。&lt;/p&gt;

&lt;h2&gt;5. 获取数据：新浪微博API使用&lt;/h2&gt;

&lt;p&gt;　　微博官方已经有详细的新手引导了，这里就不重复造轮子了，大家可以直接参考 &lt;a href=&quot;http://open.weibo.com/wiki/%E6%96%B0%E6%89%8B%E6%8C%87%E5%8D%97&quot;&gt;这里&lt;/a&gt;。我用的是&lt;a href=&quot;https://github.com/michaelliao/sinaweibopy&quot;&gt;Python SDK&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;6. 收发数据：Socket Server&lt;/h2&gt;

&lt;p&gt;　　好久没有接触网络编程这块了，这里为了快速完成MVP的效果，我用了最简单的多线程socket server模型，即新建一个线程用于处理一个新的连接。整个socket server的模型如下：&lt;br/&gt;
&lt;img src=&quot;../../images/socket-server.jpg&quot; alt=&quot;simple-socket-server&quot; /&gt;&lt;/p&gt;

&lt;p&gt;　　socket server的核心代码如下，完整代码请查看github。这里send_data应该是到去拿新浪微博的数据的，但是我在测试的时候为了方便起见，先简单地用了一条测试数据： data = &#39;hello, I am litaotao&#39;。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def send_data(conn, client):
    # data = get_data(client)
    data = &#39;hello, I am litaotao&#39;
    conn.sendall(data.encode(&#39;utf-8&#39;))
    print &#39;\nIN THREAD: send to {}, data length: {}&#39;.format(str(conn), str(len(data)))
    conn.close()

def socket_server(HOST, PORT):
    client = get_local_weibo_client() or get_weibo_client()

    # s = socket.socket(socket.AF_INET,socket.SOCK_STREAM)  
    s = socket.socket()    
    s.bind((HOST, PORT))
    s.listen(10)

    while  True:
        print &#39;wait for connection ...&#39;
        conn, addr = s.accept()
        print &#39;connect with {} : {}&#39;.format(addr[0], str(addr[1]))
        thread.start_new_thread(send_data, (conn, client))      
    s.shutdown()

if __name__ == &#39;__main__&#39;:
    HOST, PORT = &#39;&#39;, 9999
    socket_server(HOST, PORT)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;7. 展示数据: Just print&lt;/h2&gt;

&lt;p&gt;　　现在socket server已经准备就绪了，接下来准备一下spark端的任务逻辑。在这之前，我先简单介绍一下目前我的spark集群环境，为了方便理解，我也把环境的IP地址列出来了，这样在以后启动命令的时候也比较清楚。&lt;br/&gt;
　　可以清楚的看到，目前spark cluster测试环境里一共有10台机器，其中一台&lt;code&gt;10.21.208.21&lt;/code&gt;作为cluster manager，即master使用，其他9台作为worker使用。而我们写spark任务程序以及提交任务，拿到任务运行结果，都在一台driver机器上，driver机器ip是&lt;code&gt;10.20.70.80&lt;/code&gt;。
&lt;img src=&quot;../../images/spark-cluster.jpg&quot; alt=&quot;spark-cluster&quot; /&gt;
　　ok，现在可以来写spark的任务程序了，很简单，就是一个print语句，我是用python写的，代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# -*- coding: utf-8 -*-
import sys

from pyspark import SparkContext
from pyspark.streaming import StreamingContext


def change_nothing(lines):
    return lines

if __name__ == &quot;__main__&quot;:
    if len(sys.argv) != 3:
        print &amp;gt;&amp;gt; sys.stderr, &quot;Usage: weibo_message.py &amp;lt;hostname&amp;gt; &amp;lt;port&amp;gt;&quot;
        exit(-1)
    sc = SparkContext(appName=&quot;PythonStreamingWeiboMessage&quot;)
    ssc = StreamingContext(sc, 5)

    lines = ssc.socketTextStream(sys.argv[1], int(sys.argv[2]))
    lines = change_nothing(lines)
    lines.pprint()
    ssc.start()
    ssc.awaitTermination()
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;8. Opps, 为神马只有一个worker接收到数据了&lt;/h2&gt;

&lt;p&gt;　　原本以为这样就大功告成，可是当我兴奋地运行程序的时候，突然发现一个极为严重的问题---只有一个worker会到socket server这里来获取数据，并&lt;strong&gt;处理&lt;/strong&gt;后返回给driver，而且有时候是第一台worker来拿数据，有时候却又是另外一台worker来拿数据，anyway，问题就是：9太worker中，一直只有1台worker来拿数据，处理并返回，且这台worker并不是固定的。 &lt;br/&gt;
　　下面是日志：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;socket server的运行日志&lt;/li&gt;
&lt;/ul&gt;


&lt;pre&gt;&lt;code&gt;C:\Users\taotao.li\Desktop\weibostreaming (master)                                        
λ python socket_server_1.py                                                               
wait for connection ...                                                                   
-----------------------                                                                   
connect with 10.21.208.30 : 48927                                                         
wait for connection ...                                                                   
-----------------------                                                                   

IN THREAD: send to &amp;lt;socket._socketobject object at 0x0000000002C45E18&amp;gt;, data length: 20   
connect with 10.21.208.30 : 48929                                                         
wait for connection ...                                                                   
-----------------------                                                                   
IN THREAD: send to &amp;lt;socket._socketobject object at 0x0000000002C45E80&amp;gt;, data length: 20   

connect with 10.21.208.30 : 48931                                                         
wait for connection ...                                                                   
 -----------------------                                                                  
IN THREAD: send to &amp;lt;socket._socketobject object at 0x0000000002C45E18&amp;gt;, data length: 20   

connect with 10.21.208.30 : 48933                                                         
wait for connection ...                                                                   
-----------------------                                                                   

IN THREAD: send to &amp;lt;socket._socketobject object at 0x0000000002C45E80&amp;gt;, data length: 20   
connect with 10.21.208.30 : 48935                                                         
wait for connection ...                                                                   
IN THREAD: send to &amp;lt;socket._socketobject object at 0x0000000002C45E18&amp;gt;, data length: 20   

-----------------------                                                                   
connect with 10.21.208.30 : 48937                                                         
wait for connection ...                                                                   
IN THREAD: send to &amp;lt;socket._socketobject object at 0x0000000002C45E80&amp;gt;, data length: 20   

-----------------------                                                                   
connect with 10.21.208.30 : 48938                                                         
wait for connection ...                                                                   
 -----------------------                                                                  
IN THREAD: send to &amp;lt;socket._socketobject object at 0x0000000002C45E18&amp;gt;, data length: 20   

connect with 10.21.208.30 : 48940                                                         
wait for connection ...                                                                   
IN THREAD: send to &amp;lt;socket._socketobject object at 0x0000000002C45E80&amp;gt;, data length: 20   

-----------------------                                                                   
connect with 10.21.208.30 : 48942                                                         
wait for connection ...                                                                   
IN THREAD: send to &amp;lt;socket._socketobject object at 0x0000000002C45E18&amp;gt;, data length: 20   

-----------------------                                                                   
connect with 10.21.208.30 : 48944                                                         
wait for connection ...                                                                   
-----------------------                                                                   
IN THREAD: send to &amp;lt;socket._socketobject object at 0x0000000002C45E80&amp;gt;, data length: 20   

connect with 10.21.208.30 : 48946                                                         
wait for connection ...                                                                   
 -----------------------                                                                  
IN THREAD: send to &amp;lt;socket._socketobject object at 0x0000000002C45E18&amp;gt;, data length: 20   

connect with 10.21.208.30 : 48948                                                         
wait for connection ...                                                                   
 -----------------------                                                                  
IN THREAD: send to &amp;lt;socket._socketobject object at 0x0000000002C45E80&amp;gt;, data length: 20   

connect with 10.21.208.30 : 48952                                                         
wait for connection ...                                                                   
IN THREAD: send to &amp;lt;socket._socketobject object at 0x0000000002C45E18&amp;gt;, data length: 20   
-----------------------                                                                   

connect with 10.21.208.30 : 48953                                                         
wait for connection ...                                                                   
IN THREAD: send to &amp;lt;socket._socketobject object at 0x0000000002C45E80&amp;gt;, data length: 20   

-----------------------                                                                   
connect with 10.21.208.30 : 48955                                                         
wait for connection ...                                                                   
IN THREAD: send to &amp;lt;socket._socketobject object at 0x0000000002C45E18&amp;gt;, data length: 20   

-----------------------                                                                   
connect with 10.21.208.30 : 48956                                                         
wait for connection ...                                                                   
IN THREAD: send to &amp;lt;socket._socketobject object at 0x0000000002C45E80&amp;gt;, data length: 20   

-----------------------                                                                   
connect with 10.21.208.30 : 48957                                                         
wait for connection ...                                                                   
 -----------------------                                                                  
IN THREAD: send to &amp;lt;socket._socketobject object at 0x0000000002C45E18&amp;gt;, data length: 20   

-----------------------
Traceback (most recent call last):
  File &quot;socket_server_1.py&quot;, line 24, in &amp;lt;module&amp;gt;
    socket_server(HOST, PORT)
  File &quot;socket_server_1.py&quot;, line 16, in socket_server
    conn, addr = s.accept()
  File &quot;C:\Anaconda\lib\socket.py&quot;, line 202, in accept
    sock, addr = self._sock.accept()
KeyboardInterrupt
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;spark 任务的启动命令，我把处理返回的结果重定向到log.txt里，方便查看&lt;/li&gt;
&lt;/ul&gt;


&lt;pre&gt;&lt;code&gt;root@ubuntu2[17:41:01]:~/Desktop/streaming#spark-submit --master spark://10.21.208.21:7077 weibo_message.py 10.20.102.52 9999 &amp;gt; log.txt
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;spark 任务运行日志，太多了，完整的日志可以到 &lt;a href=&quot;../../files/spark-console.log&quot;&gt;这里下载 spark-console.log&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;spark 任务运行结果日志 log.txt，完整的日志可以到 &lt;a href=&quot;../../files/log.txt&quot;&gt;这里下载 log.txt&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;pre&gt;&lt;code&gt;-------------------------------------------
Time: 2015-01-21 18:50:05
-------------------------------------------

-------------------------------------------
Time: 2015-01-21 18:50:10
-------------------------------------------
hello, I am litaotao

-------------------------------------------
Time: 2015-01-21 18:50:15
-------------------------------------------
hello, I am litaotao

-------------------------------------------
Time: 2015-01-21 18:50:20
-------------------------------------------
hello, I am litaotao
hello, I am litaotao

-------------------------------------------
Time: 2015-01-21 18:50:25
-------------------------------------------

-------------------------------------------
Time: 2015-01-21 18:50:30
-------------------------------------------

-------------------------------------------
Time: 2015-01-21 18:50:35
-------------------------------------------

-------------------------------------------
Time: 2015-01-21 18:50:40
-------------------------------------------

-------------------------------------------
Time: 2015-01-21 18:50:45
-------------------------------------------

-------------------------------------------
Time: 2015-01-21 18:50:50
-------------------------------------------

-------------------------------------------
Time: 2015-01-21 18:50:55
-------------------------------------------

-------------------------------------------
Time: 2015-01-21 18:51:00
-------------------------------------------
hello, I am litaotao

-------------------------------------------
Time: 2015-01-21 18:51:05
-------------------------------------------
hello, I am litaotao

-------------------------------------------
Time: 2015-01-21 18:51:10
-------------------------------------------

-------------------------------------------
Time: 2015-01-21 18:51:15
-------------------------------------------

-------------------------------------------
Time: 2015-01-21 18:51:20
-------------------------------------------

-------------------------------------------
Time: 2015-01-21 18:51:25
-------------------------------------------

-------------------------------------------
Time: 2015-01-21 18:51:30
-------------------------------------------
hello, I am litaotao
hello, I am litaotao

-------------------------------------------
Time: 2015-01-21 18:51:35
-------------------------------------------
hello, I am litaotao
hello, I am litaotao
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;Web UI 监控截图，显示只有一个receiver
&lt;img src=&quot;../../images/only-one-receiver.jpg&quot; alt=&quot;only-one-receiver&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;9. Why! What happened?&lt;/h2&gt;

&lt;p&gt;　　百思不得其解，这是为什么呢，这里我有两个疑点：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;难道我对这个任务的理解有误吗。我的理解是，当运行spark-submit提交任务后，master应该会把这个weibo_message代码分发到9个worker上，然后9个worker分别在自己的机器上新建TCP连接到socket server，并从这个socket server上获取数据，然后处理后各自独立返回给driver。难道是我理解错误了吗。&lt;/li&gt;
&lt;li&gt;我仔细研读了官方&lt;a href=&quot;http://spark.apache.org/docs/latest/streaming-programming-guide.html#reducing-the-processing-time-of-each-batch&quot;&gt;spark streaming的教程&lt;/a&gt;，在里面发现这样一个主题 &lt;a href=&quot;http://spark.apache.org/docs/latest/streaming-programming-guide.html#reducing-the-processing-time-of-each-batch&quot;&gt;Level of Parallelism in Data Receiving&lt;/a&gt;，似乎对比起上图的Web UI监控图来看，难道是要自己根据worker的数据自定义receiver的数量。即目前我有9太worker，那我必须手动定义9个receiver？难道真的应该是这样的吗，我怎么觉得这样设计会很不灵活呢？why？&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
   <entry>
     <title>记录MongoDB一些优化方法</title>
     <link href="/mongodb-optimizing"/>
     <updated>2014-12-24T00:00:00+08:00</updated>
     <id>/mongodb-optimizing</id>
     <content type="html">&lt;h2&gt;&lt;/h2&gt;

&lt;h2&gt;1. 决定是否优化&lt;/h2&gt;

&lt;h3&gt;1.1 使用MongoDB自带的explain命令查看查询性能&lt;/h3&gt;

&lt;p&gt;　　下面是我在本地测试的一个例子。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; db.user.find({&#39;user&#39;:&#39;taotao.li@datayes.com&#39;}).explain()         
{             
        // 返回游标类型(BasicCursor | BtreeCursor | GeoSearchCursor | Complex Plan | multi)                                                         
        &quot;cursor&quot; : &quot;BasicCursor&quot;,
        // 是否使用多重索引(true | false)                                     
        &quot;isMultiKey&quot; : false,    
        // 返回的文档数量   (number)                                      
        &quot;n&quot; : 1,    
        // 被扫描的文档数量，这个值小于或等于nscanned   (number)                                                   
        &quot;nscannedObjects&quot; : 9,
        // 被扫描的文档数量，我们应该尽量使这个值和上面提到的n的值相近    (number)                                          
        &quot;nscanned&quot; : 9,    
        // 这个值表明在进行一次查询时，数据库计划扫描的文档数量 (number)                                            
        &quot;nscannedObjectsAllPlans&quot; : 9,                                
        &quot;nscannedAllPlans&quot; : 9,    
        // 若为true，表示查询不能利用文档在索引里的排序来返回结果，用户需要手动对返回进行排序操作，反之(true | false)                                     
        &quot;scanAndOrder&quot; : false,    
        // 若为true，表示查询是充分利用了现有的索引的，在设计索引的时候，应该尽量确保热点查询都利用到了已有的索引(true | false)                                     
        &quot;indexOnly&quot; : false,   
        // 表示查询语句执行时写锁的等待时间  (ms)                                       
        &quot;nYields&quot; : 0,          
        // 在分片的时候可以通过这个字段看分片的效果 (number)                                       
        &quot;nChunkSkips&quot; : 0, 
        // 耗时 (ms)                                               
        &quot;millis&quot; : 65,    
        // 所使用的索引 (dict of dict)                                             
        &quot;indexBounds&quot; : {                                             

        },        
        // mongo所在的服务器地址                                            
        &quot;server&quot; : &quot;SHN1408GPVG612:27017&quot;                             
}                                                                
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;1.2 使用MongoDB自带的profile优化器查看查询性能&lt;/h3&gt;

&lt;p&gt;　　MongoDB Database Profiler是一种慢查询日志功能,可以作为我们优化数据库的依据.
开启Profiling功能,有2种方式可以控制Profiling的开关盒级别。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;启动MonggoDB时加上 &lt;code&gt;-profile=级别&lt;/code&gt; 即可&lt;/li&gt;
&lt;li&gt;在客户端调用db.setProfilingLevel(级别)命令来实时配置&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;　　Profiler信息保存在system.profile中.我们可以通过db.getProfilingLevel()命令来获取当前的Profile级别。profile的级别有4个，分别是-1、0、1、2，默认没有开启。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;-1: 返回当前设置的级别&lt;/li&gt;
&lt;li&gt;0： 表示不开启&lt;/li&gt;
&lt;li&gt;1： 表示记录慢命令(默认为&gt;100ms)&lt;/li&gt;
&lt;li&gt;2： 表示记录所有命令&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;　　下面是我运行profile的一个示例及各个字段的解释：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; db.system.profile.findOne()
{
    //操作类型：insert | query | update | remove | getmore | command
    &quot;op&quot; : &quot;query&quot;,
    //进行op操作的地方，比如现在是说在community数据库的system集合的indexes中进行了一次查询操作
    &quot;ns&quot; : &quot;community.system.indexes&quot;,
    &quot;query&quot; : {
            &quot;expireAfterSeconds&quot; : {
                    &quot;$exists&quot; : true
            }
    },
    &quot;ntoreturn&quot; : 0,
    &quot;ntoskip&quot; : 0,
    &quot;nscanned&quot; : 2,
    &quot;keyUpdates&quot; : 0,
    &quot;numYield&quot; : 0,
    // 此次查询花在处理锁上的时间；其中R/W代表全局读/写锁，r/w代表数据库层面的读/写锁；
    &quot;lockStats&quot; : {
            &quot;timeLockedMicros&quot; : {
                    &quot;r&quot; : NumberLong(79),
                    &quot;w&quot; : NumberLong(0)
            },
            &quot;timeAcquiringMicros&quot; : {
                    &quot;r&quot; : NumberLong(1),
                    &quot;w&quot; : NumberLong(2)
            }
    },
    // 返回的文档数量
    &quot;nreturned&quot; : 0,
    // 返回字节长度，如果这个数字很大，考虑值返回所需字段
    &quot;responseLength&quot; : 20,
    // 查询所耗时间，这个时间是在mongo服务器端，从这个查询开始到查询结束；类似于一般程序执行的CPU时间；
    &quot;millis&quot; : 0,
    &quot;ts&quot; : ISODate(&quot;2014-11-19T09:33:58.965Z&quot;),
    // 发起此次查询的远程地址
    &quot;client&quot; : &quot;0.0.0.0&quot;,
    &quot;allUsers&quot; : [ ],
    // 执行此查询语句的用户，不知道这里为什么是空的
    &quot;user&quot; : &quot;&quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3&gt;1.3 使用MongoDB自带的mongostat优化器查看查询性能&lt;/h3&gt;

&lt;p&gt;　　下面是我本地测试的一个mongostat示例即相关字段含义：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;D:\mongodb-2.4.10\bin
λ mongostat.exe
connected to: 127.0.0.1
insert  query update delete getmore command flushes mapped  vsize    res faults  locked db idx miss %     qr|qw   ar|aw  netIn netOut  conn       time
    *0     *0     *0     *0       0     1|0       0   608m   1.4g    14m      1  test:3.3%          0       0|0     0|0    62b     3k    60   10:42:44
    *0     *0     *0     *0       0     1|0       0   608m   1.4g    14m      0  test:0.0%          0       0|0     0|0    62b     3k    60   10:42:45
    *0     *0     *0     *0       0     1|0       0   608m   1.4g    14m      0  test:0.0%          0       0|0     0|0    62b     3k    60   10:42:46
    *0     *0     *0     *0       0     1|0       0   608m   1.4g    14m      0  test:0.0%          0       0|0     0|0    62b     3k    60   10:42:47
    *0     *0     *0     *0       0     3|0       0   608m   1.4g    14m      0  test:0.0%          0       0|0     0|0   174b     3k    60   10:42:48
    *0     *0     *0     *0       0     1|0       0   608m   1.4g    14m      0  test:0.0%          0       0|0     0|0    62b     3k    60   10:42:49
    *0     *0     *0     *0       0     3|0       0   608m   1.4g    14m      0 local:0.0%          0       0|0     0|0   178b     3k    60   10:42:50
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　mongostat各个字段解释：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;insert/query/update/delete/getmore：每秒执行这5个操作的次数；&lt;/li&gt;
&lt;li&gt;command：每秒执行指令的次数，在从数据库中，这个字段的值是一个以“|”分开的两个值，表示 local|replicated 数量；&lt;/li&gt;
&lt;li&gt;flushes：每秒fsync操作的次数；&lt;/li&gt;
&lt;li&gt;mapped：按照官方解释，这个字段表示上一次执行mongostat指令是所有数据的大小，应该是指所有数据占磁盘的大小，但似乎不是很对；&lt;/li&gt;
&lt;li&gt;vsize：mongod服务占用的虚拟内存大小；&lt;/li&gt;
&lt;li&gt;res：mongod所占用的物理内存；&lt;/li&gt;
&lt;li&gt;faluts：page faults次数；&lt;/li&gt;
&lt;li&gt;index miss：索引缺失的数量；&lt;/li&gt;
&lt;li&gt;qr/qw：表示在队列中等待的客户端，rw表示读写；&lt;/li&gt;
&lt;li&gt;ar/aw：表示正在进行请求的客户端；&lt;/li&gt;
&lt;li&gt;netIn/netOut表示网络流量，单位是字节；&lt;/li&gt;
&lt;li&gt;conn：表示连接数；&lt;/li&gt;
&lt;li&gt;repl：表示同步状态；&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;1.4 使用MongoDB自带的db.serverStatus查看服务器状态&lt;/h3&gt;

&lt;p&gt;　　这里输出的信息太多了，看一个Robomongo的截图吧，里面有几个字段也是很重要的。 &lt;img src=&quot;../../images/mongo-db-serverStats.jpg&quot; alt=&quot;mongo-db-serverStatus&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;1.5 使用MongoDB自带的db.stats查看服务器状态&lt;/h3&gt;

&lt;p&gt;　　说到这里，我突然发现监控MongoDB performance的工具真的挺多的，完全可以自己给予这些命令和工具来开发后台管理的工具啊。比如说stats这个命令，也提供了挺多的信息：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; db.stats()                                        
{                                                   
        &quot;db&quot; : &quot;community&quot;,                         
        &quot;collections&quot; : 7,   
        // 记录在数据库中的所有文档总数                       
        &quot;objects&quot; : 73,                
        // 数据库中所有文档的平均大小，等于 dataSize/objects             
        &quot;avgObjSize&quot; : 24965.424657534248,          
        // 数据库所有文档的总大小，以字节为单位
        &quot;dataSize&quot; : 1822476,                  
        // 分配给每一个文档的磁盘空间，奇怪这里为什么不是16MB     
        &quot;storageSize&quot; : 11943936,                   
        &quot;numExtents&quot; : 12,                          
        &quot;indexes&quot; : 4,                              
        &quot;indexSize&quot; : 32704,                        
        &quot;fileSize&quot; : 201326592,                     
        &quot;nsSizeMB&quot; : 16,                            
        &quot;dataFileVersion&quot; : {                       
                &quot;major&quot; : 4,                        
                &quot;minor&quot; : 5                         
        },                                          
        &quot;ok&quot; : 1                                    
}                                                   
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;2. 优化Schema&lt;/h2&gt;

&lt;p&gt;　　解决一个问题永远都有多种方法，且在产品的不同时期也会有不同的解决办法。但核心观点都不变：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;理解产品的核心应用；&lt;/li&gt;
&lt;li&gt;合理平衡数据库的读写，读写比例很大程度上决定你的Schema设计；&lt;/li&gt;
&lt;li&gt;避免随机性IO操作；&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;　　即使是NoSQL，也无法避免一些数据库字段在关系上的建立。所以在设计NoSQL Schema的时候不可避免地要进行一些关系的处理。在MongoDB方面，处理关系有以下三种方法：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;数据库引用&lt;/li&gt;
&lt;li&gt;集合间的应用&lt;/li&gt;
&lt;li&gt;文档嵌套&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;　　其中数据库引用很少用到，并且官方也不推荐这种用法。相对于集合间引用和文档嵌套，这个需要看具体设计。特别是在应用文档嵌套方式的时候，需要注意一个文档的最大容量是&lt;a href=&quot;http://docs.mongodb.org/manual/core/document/&quot;&gt;16MB&lt;/a&gt;。文章末尾的&lt;strong&gt;6 Rules of Thumb for MongoDB Schema Design&lt;/strong&gt;详细介绍、对比了一些集合引用和嵌套文档方面的案例，&lt;strong&gt;MongoDB Schema Design: Four Real-World Examples&lt;/strong&gt;介绍了MongoDB 4个真实的应用设计案例，有比较大的参考价值。&lt;/p&gt;

&lt;h2&gt;2.1 MongoDB 和 RDBMS的一些概念联系&lt;/h2&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt; RDBMS &lt;/th&gt;
&lt;th&gt; MongoDB &lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt; Database &lt;/td&gt;
&lt;td&gt; Database &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; Table &lt;/td&gt;
&lt;td&gt; Collection &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; Row &lt;/td&gt;
&lt;td&gt; Document &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; Index &lt;/td&gt;
&lt;td&gt; Index &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; Join &lt;/td&gt;
&lt;td&gt; Embedded Document &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; Foreign Key &lt;/td&gt;
&lt;td&gt; Reference &lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;


&lt;h2&gt;3. 查询优化&lt;/h2&gt;

&lt;h3&gt;3.1 建立索引&lt;/h3&gt;

&lt;p&gt;　　首先要明确的是，mongo里会自动根据&lt;em&gt;id来创建一个唯一性索引，所以如果你是以&lt;/em&gt;id为key来进行查询的话都会很快的。比如下面这个截图，nscanned为1。
&lt;img src=&quot;../../images/mongo-index-id.jpg&quot; alt=&quot;mongo-explain&quot; /&gt;&lt;br/&gt;
　　database.collection.ensureIndex( { key : 1 } , { background : true } ); &lt;br/&gt;
　　说明：在数据库database里，对collection中的字段key建立索引，按照升序方式建立索引background参数设置为true时表示后台创建索引【建立索引略耗时】。  &lt;br/&gt;
　　索引是一把双刃剑啊，用得好不好，全看也许需求和数据库设计了，在设计索引前最好参考参考&lt;a href=&quot;http://docs.mongodb.org/manual/core/indexes-introduction/&quot;&gt;官方文档&lt;/a&gt;，而且最好要有一个建索引前后的performance的一些对比，talk is cheap, show me the data.  &lt;br/&gt;
　　总而言之，当需要建立索引的时候，一定要仔细思考下面几个方面：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;热点查询的数据是什么，可否用缓存替代；&lt;/li&gt;
&lt;li&gt;建立索引的顺序也会影响查询速度，参考: 10gen工程师谈MongoDB组合索引的优化；&lt;/li&gt;
&lt;li&gt;索引的更新周期，更新索引是一件很tricky的事情；&lt;/li&gt;
&lt;li&gt;索引建立后，是否能跟上后期系统扩展的脚步；&lt;/li&gt;
&lt;/ul&gt;


&lt;h3&gt;3.2 限制返回数据&lt;/h3&gt;

&lt;p&gt;　　使用limit，skip方式返回查询数据，只返回需要返回的数据。&lt;/p&gt;

&lt;h2&gt;4. 总结&lt;/h2&gt;

&lt;p&gt;　　在记录这篇文章的过程中，我发现监控mongo性能的工具还真的挺多的。但是现在我参与的产品中数据量还很少，还没有涉及到数据库这方面的优化，所以上面提到的这些都是自己在官网和一些优秀博客收集的资料。在后续涉及到自己优化这些查询的时候，我会再把实际经验和优化对比记录下来。&lt;/p&gt;

&lt;h2&gt;5. 一些资料&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://caizi.blog.51cto.com/5234706/1542480&quot;&gt;mongodb性能优化&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://blog.mongodb.org/post/87200945828/6-rules-of-thumb-for-mongodb-schema-design-part-1&quot;&gt;6 Rules of Thumb for MongoDB Schema Design&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.slideshare.net/friedo/data-modeling-examples&quot;&gt;MongoDB Schema Design: Four Real-World Examples&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://docs.mongodb.org/v2.4/administration/optimization/&quot;&gt;Optimization Strategies for MongoDB&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://docs.mongodb.org/v2.4/reference/database-profiler/&quot;&gt;Database Profiler Output&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://www.csdn.net/article/2012-11-09/2811690-optimizing-mongodb-compound&quot;&gt;10gen工程师谈MongoDB组合索引的优化&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://docs.mongodb.org/v2.4/reference/command/dbStats/&quot;&gt;dbStats&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   
   <entry>
     <title>［touch spark］5. spark RDD 之：RDD Transformation</title>
     <link href="/rdd-transformations"/>
     <updated>2014-12-20T00:00:00+08:00</updated>
     <id>/rdd-transformations</id>
     <content type="html">&lt;p&gt;&lt;img src=&quot;../../images/transfomers.jpg&quot; alt=&quot;transfomers&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;&lt;/h2&gt;

&lt;h2&gt;1. 什么是RDD&lt;/h2&gt;

&lt;p&gt;　　关于什么是RDD，可以参考上一篇 &lt;a href=&quot;../spark-what-is-rdd&quot;&gt;4. spark RDD 之：什么是RDD&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;2. RDD transfomation 一览&lt;/h2&gt;

&lt;p&gt;　　ok，了解了RDD的含义，我们可以来看看神马叫transformation了，中文叫转换。上一篇提到RDD可以由两种方式创建，而在实际应用中，一般第一种方式都是用于新建一个RDD的时候，大多数时候都是通过第二种方式来生成一个新的RDD。so，想想这里我们应该怎么来根据一个已存在的RDD来transform出另一个新的RDD呢？当然就是根据一些规则，比如说筛选，映射，分组等等，而用于支撑这些规则的函数，就叫做RDD的transformation。&lt;br/&gt;
　　下面我们通过下面这张表来看看RDD都支持哪些规则的transformation吧。 这些是&lt;a href=&quot;http://spark.apache.org/docs/latest/programming-guide.html#transformations&quot;&gt;官方&lt;/a&gt;列出的一些常用的transformation，我原本想把所有transformation列出来的，可考虑到2/8原则，想想下面这些在实际应用中应该足够了。如果真的需要其他transformation的时候，相比彼时你的功力应该已到阅读、贡献源码的级别了。也就不需要再参考我下面将要写的东西了。 &lt;br/&gt;
　　这些transformation中有一些是我不太熟悉的，所以这里记录一下我不太熟悉的那些转换函数的用法，仅供个人参考哦。&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt; No &lt;/th&gt;
&lt;th&gt; Transformation  &lt;/th&gt;
&lt;th&gt;  Meaning &lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt; 1 &lt;/td&gt;
&lt;td&gt; map(func) &lt;/td&gt;
&lt;td&gt; Return a new distributed dataset formed by passing each element of the source through a function func.  &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; 2 &lt;/td&gt;
&lt;td&gt; filter(func) &lt;/td&gt;
&lt;td&gt; Return a new dataset formed by selecting those elements of the source on which func returns true. &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; 3 &lt;/td&gt;
&lt;td&gt; flatMap(func) &lt;/td&gt;
&lt;td&gt; Similar to map, but each input item can be mapped to 0 or more output items (so func should return a Seq rather than a single item). &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; 4 &lt;/td&gt;
&lt;td&gt; mapPartitions(func) &lt;/td&gt;
&lt;td&gt; Similar to map, but runs separately on each partition (block) of the RDD, so func must be of type Iterator&lt;T&gt; =&gt; Iterator&lt;U&gt; when running on an RDD of type T. &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; 5 &lt;/td&gt;
&lt;td&gt; mapPartitionsWithIndex(func) &lt;/td&gt;
&lt;td&gt; Similar to mapPartitions, but also provides func with an integer value representing the index of the partition, so func must be of type (Int, Iterator&lt;T&gt;) =&gt; Iterator&lt;U&gt; when running on an RDD of type T. &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; 6 &lt;/td&gt;
&lt;td&gt; sample(withReplacement, fraction, seed) &lt;/td&gt;
&lt;td&gt; Sample a fraction fraction of the data, with or without replacement, using a given random number generator seed. &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; 7 &lt;/td&gt;
&lt;td&gt; union(otherDataset) &lt;/td&gt;
&lt;td&gt; Return a new dataset that contains the union of the elements in the source dataset and the argument. &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; 8 &lt;/td&gt;
&lt;td&gt; intersection(otherDataset) &lt;/td&gt;
&lt;td&gt; Return a new RDD that contains the intersection of elements in the source dataset and the argument. &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; 9 &lt;/td&gt;
&lt;td&gt; distinct([numTasks])) &lt;/td&gt;
&lt;td&gt; Return a new dataset that contains the distinct elements of the source dataset. &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; 10 &lt;/td&gt;
&lt;td&gt; groupByKey([numTasks]) &lt;/td&gt;
&lt;td&gt; When called on a dataset of (K, V) pairs, returns a dataset of (K, Iterable&lt;V&gt;) pairs.   &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; 11 &lt;/td&gt;
&lt;td&gt; reduceByKey(func, [numTasks]) &lt;/td&gt;
&lt;td&gt; When called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function func, which must be of type (V,V) =&gt; V. Like in groupByKey, the number of reduce tasks is configurable through an optional second argument. &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; 12 &lt;/td&gt;
&lt;td&gt; aggregateByKey(zeroValue)(seqOp, combOp, [numTasks]) &lt;/td&gt;
&lt;td&gt; When called on a dataset of (K, V) pairs, returns a dataset of (K, U) pairs where the values for each key are aggregated using the given combine functions and a neutral &quot;zero&quot; value. Allows an aggregated value type that is different than the input value type, while avoiding unnecessary allocations. Like in groupByKey, the number of reduce tasks is configurable through an optional second argument. &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; 13 &lt;/td&gt;
&lt;td&gt; sortByKey([ascending], [numTasks]) &lt;/td&gt;
&lt;td&gt; When called on a dataset of (K, V) pairs where K implements Ordered, returns a dataset of (K, V) pairs sorted by keys in ascending or descending order, as specified in the boolean ascending argument. &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt; 14 &lt;/td&gt;
&lt;td&gt; join(otherDataset, [numTasks]) &lt;/td&gt;
&lt;td&gt; When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs with all pairs of elements for each key. Outer joins are also supported through leftOuterJoin and rightOuterJoin. &lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;


&lt;h2&gt;3. RDD transfomation&lt;/h2&gt;

&lt;p&gt;　　RDD 的transformation有几个和scala里的函数组合子一样，其他的我估计也是基于scala的组合子来写的。所以，为了方便起见，能用scala来展现的，我就用scala来写；不可以的，我再用spark来写示例。下面提到的transformation更新到&lt;a href=&quot;http://spark.apache.org/docs/latest/programming-guide.html#transformations&quot;&gt;spark 1.1.1版本&lt;/a&gt;。要想查看最新版本的，可以上&lt;a href=&quot;http://spark.apache.org/&quot;&gt;官网&lt;/a&gt;和&lt;a href=&quot;http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD&quot;&gt;官方API文档&lt;/a&gt;。这里顺带提一下，说到transformation，网上几乎每篇说spark的文章都会用到下面这张图。可是我真的想说，这张图可是Matei这哥们2012年发的&lt;a href=&quot;https://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf&quot;&gt;RDD论文&lt;/a&gt;里的啊，spark更新的速度也许比大多数人换对象的速度还快啊，好多东西已经变了。大家以后要是发这张图，就该说明出处和版本吧，免得大家误解transformation和actions就那几种类型啊。&lt;br/&gt;
　　以下都是基于官方&lt;a href=&quot;http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD&quot;&gt;API DOC&lt;/a&gt;，在我本机上的测试以及网络资料整理而来。所有参考过的资料我都会在最后列出来，供参考。
&lt;img src=&quot;../../images/trans-action-2012.png&quot; alt=&quot;trans-action-2012&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;3.1 RDD transfomation － flatMap&lt;/h2&gt;

&lt;p&gt;　　说明：&lt;br/&gt;
　　flatMap可以理解成map和flat的组合。他处理一个嵌套列表，对其中每个列表中的元素执行map，然后对每个列表执行flat，最后返回一个列表。 &lt;br/&gt;
　　所以，我们也可以先flatten一个列表，再对列表里的每个元素做mapping；当然也可以对嵌套列表里的每个元素做mapping，再对列表做flatten。看下面的例子就明白了：&lt;/p&gt;

&lt;p&gt;　　定义：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def
flatMap[U](f: (T) ⇒ TraversableOnce[U])(implicit arg0: ClassTag[U]): RDD[U]
Return a new RDD by first applying a function to all elements of this RDD, and then flattening the results.   
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　示例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// flatMap operation
scala&amp;gt; val test = List(List(1,2,3,4,5), List(10,20,30,40,50))
test: List[List[Int]] = List(List(1, 2, 3, 4, 5), List(10, 20, 30, 40, 50))

scala&amp;gt; val test1 = test.flatMap( x =&amp;gt; x.map(_ * 2))
test1: List[Int] = List(2, 4, 6, 8, 10, 20, 40, 60, 80, 100)

// flat first, then mapping
scala&amp;gt; val tset2 = test.flatten
tset2: List[Int] = List(1, 2, 3, 4, 5, 10, 20, 30, 40, 50)

scala&amp;gt; val test3 = test.flatten.map(_*2)
test3: List[Int] = List(2, 4, 6, 8, 10, 20, 40, 60, 80, 100)

// mapping first, then flat
scala&amp;gt; val test4 = test.map( x =&amp;gt; x.map(_*2))
test4: List[List[Int]] = List(List(2, 4, 6, 8, 10), List(20, 40, 60, 80, 100))

scala&amp;gt; val test5 = test.map(x =&amp;gt; x.map(_*2)).flatten
test5: List[Int] = List(2, 4, 6, 8, 10, 20, 40, 60, 80, 100)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;3.2 RDD transfomation － mapPartitions&lt;/h2&gt;

&lt;p&gt;　　说明：&lt;br/&gt;
　　mapPartitions是map的一个变种。map的输入函数是应用于RDD中每个元素，而mapPartitions的输入函数是应用于每个分区，也就是把每个分区中的内容作为整体来处理的。&lt;/p&gt;

&lt;p&gt;　　定义：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def  
mapPartitions[U](f: (Iterator[T]) ⇒ Iterator[U], preservesPartitioning: Boolean = false)(implicit arg0: ClassTag[U]): RDD[U]
Return a new RDD by applying a function to each partition of this RDD.

preservesPartitioning indicates whether the input function preserves the partitioner, which should be false unless this is a pair RDD and the input function doesn&#39;t modify the keys.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　f即为输入函数，它处理每个分区里面的内容。每个分区中的内容将以Iterator[T]传递给输入函数f，f的输出结果是Iterator[U]。最终的RDD由所有分区经过输入函数处理后的结果合并起来的。&lt;/p&gt;

&lt;h2&gt;3.3 RDD transfomation － mapPartitionsWithIndex | mapPartitionsWithContext&lt;/h2&gt;

&lt;p&gt;　　说明： &lt;br/&gt;
　　mapPartitions还有些变种，比如mapPartitionsWithContext，它能把处理过程中的一些状态信息传递给用户指定的输入函数。还有mapPartitionsWithIndex，它能把分区的index传递给用户指定的输入函数。&lt;/p&gt;

&lt;p&gt;　　定义：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def
mapPartitionsWithContext[U](f: (TaskContext, Iterator[T]) ⇒ Iterator[U], preservesPartitioning: Boolean = false)(implicit arg0: ClassTag[U]): RDD[U]
Return a new RDD by applying a function to each partition of this RDD. This is a variant of mapPartitions that also passes the TaskContext into the closure.

preservesPartitioning indicates whether the input function preserves the partitioner, which should be false unless this is a pair RDD and the input function does not modify the keys.
Annotations

def
mapPartitionsWithIndex[U](f: (Int, Iterator[T]) ⇒ Iterator[U], preservesPartitioning: Boolean = false)(implicit arg0: ClassTag[U]): RDD[U]
Return a new RDD by applying a function to each partition of this RDD, while tracking the index of the original partition.

preservesPartitioning indicates whether the input function preserves the partitioner, which should be false unless this is a pair RDD and the input function doesn&#39;t modify the keys.
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;3.3 RDD transfomation －  sample&lt;/h2&gt;

&lt;p&gt;　　说明： &lt;br/&gt;
　　根据fraction指定的比例，对数据进行采样，可以选择是否用随机数进行替换，seed用于指定随机数生成器种子。这里我一直有一个疑问，当我的数据集里有100个元素是，设置fraction为0.1，按理应该是返回10个随机数的，可是就返回了6个。似乎返回的随机数会少于数据集元素数量*随机数的比例，晚点继续研究。&lt;/p&gt;

&lt;p&gt;　　定义：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def  sample(withReplacement: Boolean, fraction: Double, seed: Long = Utils.random.nextLong): RDD[T]
Return a sampled subset of this RDD.  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　示例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; val a = sc.parallelize(1 to 100)
a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[17] at parallelize at &amp;lt;console&amp;gt;:12

scala&amp;gt; val sample = a.sample(false, 0.1, 0)
sample: org.apache.spark.rdd.RDD[Int] = PartitionwiseSampledRDD[18] at sample at &amp;lt;console&amp;gt;:14

scala&amp;gt; sample.count
.
.
.
14/12/12 10:39:07 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool 
14/12/12 10:39:07 INFO SparkContext: Job finished: count at &amp;lt;console&amp;gt;:17, took 3.491275316 s
res29: Long = 6

scala&amp;gt; sample.collect
.
.
.
14/12/12 10:39:11 INFO DAGScheduler: Stage 24 (collect at &amp;lt;console&amp;gt;:17) finished in 0.142 s
14/12/12 10:39:11 INFO SparkContext: Job finished: collect at &amp;lt;console&amp;gt;:17, took 0.151221638 s
res30: Array[Int] = Array(22, 46, 48, 80, 87, 97)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　后话：&lt;br/&gt;
　　在看API DOC的时候发现有一个和sample很像的transformation，takeSample，但准确的说这并不是一个transformation，应该算是一个action吧。这个函数可以从数据集里返回固定数量的随机数，弥补上上面我提到的sample那个问题。但是需要注意的是，这个函数是直接在RDD上计算，返回计算结果，并不是一个transformation。看下面的例子就明白了。&lt;/p&gt;

&lt;p&gt;　　示例2：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; val takeSample = a.takeSample(false, 10, 0)
.
.
.
14/12/12 10:43:24 INFO DAGScheduler: Stage 26 (takeSample at &amp;lt;console&amp;gt;:14) finished in 0.068 s
14/12/12 10:43:24 INFO SparkContext: Job finished: takeSample at &amp;lt;console&amp;gt;:14, took 0.074973222 s
takeSample: Array[Int] = Array(68, 18, 97, 26, 61, 33, 67, 10, 2, 1)

scala&amp;gt; takeSample
res33: Array[Int] = Array(68, 18, 97, 26, 61, 33, 67, 10, 2, 1)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;3.4 RDD transfomation － union&lt;/h2&gt;

&lt;p&gt;　　说明：&lt;br/&gt;
　　这个把两个RDD合并为一个，很简单，可以理解成计算并集。但值得说明的是，union和++运算符是等价的，至少在函数定义上是完全一致的。先了解下，以后有需要的时候再看是否有细节上的区别。&lt;/p&gt;

&lt;p&gt;　　定义：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def
++(other: RDD[T]): RDD[T]
Return the union of this RDD and another one. Any identical elements will appear multiple times (use .distinct() to eliminate them).
def
union(other: RDD[T]): RDD[T]
Return the union of this RDD and another one. Any identical elements will appear multiple times (use .distinct() to eliminate them).
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　示例：&lt;br/&gt;
    scala&gt; a
    res37: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[17] at parallelize at &lt;console&gt;:12&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scala&amp;gt; val a = sc.parallelize(1 to 10)
a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[20] at parallelize at &amp;lt;console&amp;gt;:12

scala&amp;gt; val b = sc.parallelize(11 to 20)
b: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[21] at parallelize at &amp;lt;console&amp;gt;:12

scala&amp;gt; val c1 = a++b
c1: org.apache.spark.rdd.RDD[Int] = UnionRDD[22] at $plus$plus at &amp;lt;console&amp;gt;:16

scala&amp;gt; val c2 = a.union(b)

scala&amp;gt; c1.collect
.
.
.
14/12/12 11:01:13 INFO SparkContext: Job finished: collect at &amp;lt;console&amp;gt;:19, took 0.3475174 s
res38: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20)

scala&amp;gt; c2.collect
.
.
.
14/12/12 11:01:17 INFO SparkContext: Job finished: collect at &amp;lt;console&amp;gt;:19, took 0.122094635 s
res39: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;3.5 RDD transfomation － intersection&lt;/h2&gt;

&lt;p&gt;　　说明： &lt;br/&gt;
　　对比上面的union就很好理解了，上面是计算并集，这里是计算交集。这个transformation也有需要注意的地方，就是其有三种形式，根据是否提供第二个参数以及第二个参数的类型。具体可以参考函数定义。c++里这叫多态，现在对scala才是初学阶段，我想既然scala也是OO，那应该也有多态类似的概念吧，这里晚点继续补上。&lt;/p&gt;

&lt;p&gt;　　定义：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def
intersection(other: RDD[T], numPartitions: Int): RDD[T]
Return the intersection of this RDD and another one. The output will not contain any duplicate elements, even if the input RDDs did. Performs a hash partition across the cluster

Note that this method performs a shuffle internally.
numPartitions
How many partitions to use in the resulting RDD

def
intersection(other: RDD[T], partitioner: Partitioner)(implicit ord: Ordering[T] = null): RDD[T]
Return the intersection of this RDD and another one. The output will not contain any duplicate elements, even if the input RDDs did.

Note that this method performs a shuffle internally.
partitioner
Partitioner to use for the resulting RDD

def
intersection(other: RDD[T]): RDD[T]
Return the intersection of this RDD and another one. The output will not contain any duplicate elements, even if the input RDDs did.

Note that this method performs a shuffle internally.  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　示例：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;val x = sc.parallelize(1 to 20)
val y = sc.parallelize(10 to 30)
val z = x.intersection(y)

z.collect
res74: Array[Int] = Array(16, 12, 20, 13, 17, 14, 18, 10, 19, 15, 11)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;3.6 RDD transfomation － distinct&lt;/h2&gt;

&lt;p&gt;　　说明： &lt;br/&gt;
　　即去重，相当于python里面的set。&lt;/p&gt;

&lt;p&gt;　　定义：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def
distinct(): RDD[T]
Return a new RDD containing the distinct elements in this RDD.
def
distinct(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T]
Return a new RDD containing the distinct elements in this RDD.
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;3.7 RDD transfomation － groupByKey&lt;/h2&gt;

&lt;p&gt;　　说明： &lt;br/&gt;
　　这是一个针对键值对结构来进行操作的转换方法，即你的RDD的结构是(key, value)类型，而其中key不是唯一性的。此时我们可以对这个RDD进行groupByKey的转换得到新的RDD，新的RDD的结构同样也是键值对，只是值改变了，并且键是唯一性的，即(key, iterator(value))。&lt;/p&gt;

&lt;p&gt;　　定义：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def
groupByKey(): RDD[(K, Iterable[V])]
Group the values for each key in the RDD into a single sequence. Hash-partitions the resulting RDD with the existing partitioner/parallelism level.

Note: This operation may be very expensive. If you are grouping in order to perform an aggregation (such as a sum or average) over each key, using PairRDDFunctions.aggregateByKey or PairRDDFunctions.reduceByKey will provide much better performance.
def
groupByKey(numPartitions: Int): RDD[(K, Iterable[V])]
Group the values for each key in the RDD into a single sequence. Hash-partitions the resulting RDD with into numPartitions partitions.

Note: This operation may be very expensive. If you are grouping in order to perform an aggregation (such as a sum or average) over each key, using PairRDDFunctions.aggregateByKey or PairRDDFunctions.reduceByKey will provide much better performance.
def
groupByKey(partitioner: Partitioner): RDD[(K, Iterable[V])]
Group the values for each key in the RDD into a single sequence. Allows controlling the partitioning of the resulting key-value pair RDD by passing a Partitioner.

Note: This operation may be very expensive. If you are grouping in order to perform an aggregation (such as a sum or average) over each key, using PairRDDFunctions.aggregateByKey or PairRDDFunctions.reduceByKey will provide much better performance.
&lt;/code&gt;&lt;/pre&gt;

&lt;h2&gt;3.8 RDD transfomation － reduceByKey&lt;/h2&gt;

&lt;p&gt;　　说明：  &lt;br/&gt;
　　和上面的groupByKey一样，这也是一个针对(key, value)型结构的RDD的转换函数。只是上面的groupByKey是将相同key对应的value组合成一个可迭代的对象；而reduceByKey是将相同key对应的value通过传入的函数func计算成一个新的value。&lt;/p&gt;

&lt;p&gt;　　定义：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def
reduceByKey(func: (V, V) ⇒ V): RDD[(K, V)]
Merge the values for each key using an associative reduce function. This will also perform the merging locally on each mapper before sending results to a reducer, similarly to a &quot;combiner&quot; in MapReduce. Output will be hash-partitioned with the existing partitioner/ parallelism level.
def
reduceByKey(func: (V, V) ⇒ V, numPartitions: Int): RDD[(K, V)]
Merge the values for each key using an associative reduce function. This will also perform the merging locally on each mapper before sending results to a reducer, similarly to a &quot;combiner&quot; in MapReduce. Output will be hash-partitioned with numPartitions partitions.
def
reduceByKey(partitioner: Partitioner, func: (V, V) ⇒ V): RDD[(K, V)]
Merge the values for each key using an associative reduce function. This will also perform the merging locally on each mapper before sending results to a reducer, similarly to a &quot;combiner&quot; in MapReduce.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　示例：　&lt;/p&gt;

&lt;h2&gt;3.9 RDD transfomation － aggregateByKey&lt;/h2&gt;

&lt;p&gt;　　说明：&lt;/p&gt;

&lt;p&gt;　　定义：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def
aggregateByKey[U](zeroValue: U)(seqOp: (U, V) ⇒ U, combOp: (U, U) ⇒ U)(implicit arg0: ClassTag[U]): RDD[(K, U)]
Aggregate the values of each key, using given combine functions and a neutral &quot;zero value&quot;. This function can return a different result type, U, than the type of the values in this RDD, V. Thus, we need one operation for merging a V into a U and one operation for merging two U&#39;s, as in scala.TraversableOnce. The former operation is used for merging values within a partition, and the latter is used for merging values between partitions. To avoid memory allocation, both of these functions are allowed to modify and return their first argument instead of creating a new U.
def
aggregateByKey[U](zeroValue: U, numPartitions: Int)(seqOp: (U, V) ⇒ U, combOp: (U, U) ⇒ U)(implicit arg0: ClassTag[U]): RDD[(K, U)]
Aggregate the values of each key, using given combine functions and a neutral &quot;zero value&quot;. This function can return a different result type, U, than the type of the values in this RDD, V. Thus, we need one operation for merging a V into a U and one operation for merging two U&#39;s, as in scala.TraversableOnce. The former operation is used for merging values within a partition, and the latter is used for merging values between partitions. To avoid memory allocation, both of these functions are allowed to modify and return their first argument instead of creating a new U.
def
aggregateByKey[U](zeroValue: U, partitioner: Partitioner)(seqOp: (U, V) ⇒ U, combOp: (U, U) ⇒ U)(implicit arg0: ClassTag[U]): RDD[(K, U)]
Aggregate the values of each key, using given combine functions and a neutral &quot;zero value&quot;. This function can return a different result type, U, than the type of the values in this RDD, V. Thus, we need one operation for merging a V into a U and one operation for merging two U&#39;s, as in scala.TraversableOnce. The former operation is used for merging values within a partition, and the latter is used for merging values between partitions. To avoid memory allocation, both of these functions are allowed to modify and return their first argument instead of creating a new U.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　示例：　&lt;/p&gt;

&lt;h2&gt;3.10 RDD transfomation － sortByKey&lt;/h2&gt;

&lt;p&gt;　　说明：  &lt;br/&gt;
　　按key来排序。但是在一些情况下，当RDD是(key, value)类型时，如果想对value来排序应该怎么处理呢？很简单，就是先把原来的(key, value)转换成(value, key)结构，然后对(value, key)进行sortByKey操作，最后再把已经排序了的(value, key)转换回(key, value)形式。&lt;/p&gt;

&lt;p&gt;　　定义：   &lt;br/&gt;
　　奇怪了，我再官方API DOC里没有找到这个转换函数的定义，晚一点再看一下。&lt;/p&gt;

&lt;p&gt;　　示例：　&lt;/p&gt;

&lt;h2&gt;3.11 RDD transfomation － join&lt;/h2&gt;

&lt;p&gt;　　说明：  &lt;br/&gt;
　　把两个(key, value)结构的RDD合成一个新的(key, value)结构的RDD。即(k1, v1).join((k1, v2)) =&gt; (k1, (v1, v2))，需要注意的是，为了让join操作成功，必须保证key是可以比较的。&lt;/p&gt;

&lt;p&gt;　　定义：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def
join[W](other: RDD[(K, W)], numPartitions: Int): RDD[(K, (V, W))]
Return an RDD containing all pairs of elements with matching keys in this and other. Each pair of elements will be returned as a (k, (v1, v2)) tuple, where (k, v1) is in this and (k, v2) is in other. Performs a hash join across the cluster.
def
join[W](other: RDD[(K, W)]): RDD[(K, (V, W))]
Return an RDD containing all pairs of elements with matching keys in this and other. Each pair of elements will be returned as a (k, (v1, v2)) tuple, where (k, v1) is in this and (k, v2) is in other. Performs a hash join across the cluster.
def
join[W](other: RDD[(K, W)], partitioner: Partitioner): RDD[(K, (V, W))]
Return an RDD containing all pairs of elements with matching keys in this and other. Each pair of elements will be returned as a (k, (v1, v2)) tuple, where (k, v1) is in this and (k, v2) is in other. Uses the given Partitioner to partition the output RDD.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;　　示例：　&lt;br/&gt;
　　&lt;/p&gt;

&lt;h2&gt;4，一些资源&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://blog.csdn.net/anzhsoft/article/details/39851421&quot;&gt;RDD&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://twitter.github.io/scala_school/zh_cn/collections.html#flatMap&quot;&gt;scala school from twitter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.zybuluo.com/jewes/note/35032&quot;&gt;RDD Reference 1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://homepage.cs.latrobe.edu.au/zhe/ZhenHeSparkRDDAPIExamples.html&quot;&gt;RDD API Examples&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD&quot;&gt;RDD API Docs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://spark.apache.org/docs/1.1.1/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions&quot;&gt;pairRDD API Docs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</content>
   </entry>
   

</feed>


</body>
</html>
